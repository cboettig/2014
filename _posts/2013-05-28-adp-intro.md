---
published: false
layout: post

---


## Three curses of dimensionality

1. State space
1. Action space
1. Outcome space


### Step 0

Initialization

#### 0a. 
  Initialize $\bar V_t^0(S_t) $ for all states $S_t$

#### 0b. 
  Choose an initial state $S_0^1$

#### 0c. 
  Set n = 1

### Step 1

choose a sample path $\omega^n$

### Step 2

For $t = 0, 1, 2 ... , T$ do:

#### 2a. 
  Solve: 
 
$$V_t(S_t) = \max_{x_t \in \chi_t} \left(C(S_t, x_t) + \gamma \sum_{s^{\prime} \in \mathcal{S}} \mathbb{P}(s^{\prime} | S_t^n, x_t) V_{t+1}^{n-1} s^{\prime} \right)$$

  and let $x^n_t$ be the value of $x_t$ that solves the maximization (argmax).  

####  2b. 
  
  Update $\bar V_t^{n-1}(S_t)$ using 


$$V_t^n(S_t) = \begin{cases} 
\hat v_t^n & S_t = S_t^n \\
\bar V_t^{n-1}(S_t) & \textrm{otherwise} 
\end{cases}$$

#### 2c. 
  
  Compute  $S^n_{t+1} = S^M(S_t^n, x^n_t, W_{t+1}(\omega^n))$

### Step 3

Let $n = n+1$.  If $n < N$ go to step 1.  


## Why ADP

* ADP for the curse(s) of dimensionality: State space (value function), expectation, and the action space

* An optimizing simulator: Industry applications have shown that it is often better to have accurate detailed dynamics and an approximate solution, then optimal solution to approximate dynamics...


## Decreasing the state space

* Agregation
* Continuous value function approximation

## Initialization problem 

* We do not explore if we are too pessimistic about value of visiting other states.  Start optimisitc: AI's A* alogrithm (synchronous)
* Asynchronous updating -- randomly sampling starting variables
* RTDP (Real Time Dynamic Programming -- not necessarily what it sounds like) external rule determines which states we visit

## Learning



