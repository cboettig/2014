---
layout: post
published: false

---

```{r}
knit_options
```

## Parsing semantic HTML

```{r}
library(RCurl)
library(XML)
library(tm)
library(wordcloud)
library(RColorBrewer)
```

We can get a list of pages to download from the sitemap

```{r}
pagelist <- readLines("http://carlboettiger.info/sitemap.txt")
pagelist <- pagelist[grep("/201\\d/", pagelist)]  # drop non-posts)
pages <- lapply(pagelist, getURLContent, followlocation=TRUE)
```

Or, if an archive is available locally, (e.g. from figshare cache), we can read the files in directly.  


```{r}
pages <- system("ls ~/Documents/labnotebook/_site/2***/*/*/*.html", intern=TRUE)
```

We parse each of the pages as HTML so that we can manipulate them with XML tools


```{r}
html <- lapply(pages, htmlParse)
```

For instance, we can easily get the content of all entries:

```{r}
text <- sapply(html, xpathSApply, "//article", xmlValue) 
```

We can also extract metadata based on the semantic markup.  

```{r}
titles <- sapply(html, xpathSApply, "//title", xmlValue)
categories <- sapply(html, xpathSApply, "//node()[@class='category']", xmlValue)
tags <- sapply(html, xpathSApply, "//node()[@class='tag']", xmlValue)
```

R makes it easy to summarize this data, e.g. by generating a table of the number of entries in each category, or a wordcloud of the tags.  

```{r}
table(unlist(categories))
wordcloud(Corpus(VectorSource(tags)))
```


We can perform more direct text mining as well. For instance, we extract all DOIs found in the text: 

```{r}
doi_pattern = "\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?![\"&\'<>])\\S)+)\\b"
require(gsubfn)
dois <- strapply(text, doi_pattern, perl=TRUE)    #text[-462]
head(sort(table(unlist(dois))))
```

Or generate a wordcloud of the full text 

```{r}

carl <- Corpus(VectorSource(text))
carl <- tm_map(carl, removePunctuation)
carl <- tm_map(carl, tolower)
carl <- tm_map(carl, function(x) removeWords(x, stopwords()))       

carl.tdm <- TermDocumentMatrix(carl)
carl.m <- as.matrix(carl.tdm)
carl.v <- sort(rowSums(carl.m), decreasing=TRUE)
carl.d <- data.frame(word=names(carl.v), freq=carl.v)


wordcloud(carl.d$word,carl.d$freq, scale=c(8,.4),min.freq=3,
          max.words=100, random.order=FALSE, rot.per=.15, 
          colors = brewer.pal(8,"Dark2"))
```

## RDFa parsing


Unfortunately the namespaces of the RDFa itself are less useful in the XML parsing, since they do not correspond to nodes or attributes, but appear only in the values of attributes.  As such, they do not act as XML namespaces.  We would need to first convert the RDFa to RDF to parse them as XML. We can perform this using the [Any23](http://any23.org) API

```{r}
download.file(paste("http://any23.org/rdfxml", "http://carlboettiger.info", sep="/"), "temp.xml")
doc <- xmlParse("temp.xml")
```

Which creates a beautiful RDF XML file of all linked data found in the entry.   

```{r, comment=NA}
doc
```

## XML parsing 

Some entries have content that is not valid XML. 

```{r warning=FALSE, message=FALSE} 
site <- lapply(pages, function(s){ message(s); try(xmlParse(s)) })
parse_fails <- sapply(site, is, "try-error") 
parsed_pages <- site[!parse_fails] 
```



Not all namespaces have been declared in header in RDFa. Should probably be remedied:
```{r}
xmlNamespaces(parsed_pages[[1]])
```


### XML parsing a single page example 

```{r message=FALSE, warning=FALSE, error=FALSE}
doc <- xmlParse(getURLContent("http://www.carlboettiger.info/2012/12/15/nonparametric-bayes-comparisons.html", followlocation=TRUE))
namespaces = c(ns=xmlNamespaces(doc)[[1]]$uri)
```

```{r}
set <- getNodeSet(doc, "//ns:meta[@property]", namespaces=namespaces)
metadata <- cbind(sapply(set, xmlGetAttr, "content"), sapply(set, xmlGetAttr, "property"))
metadata
```

We can extract explicit information of nodes and attributes using the xhtml namespace as well:

```{r}
xpathSApply(doc, "//ns:aside//ns:time", namespaces=namespaces, xmlValue)
```

