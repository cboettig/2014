---
published: false
category: open-science

---



If papers were easier to build upon, I think we would see that 1% (or whatever it is) increase.  Currently it is quite difficult to build on a paper in a meaningful way -- As Mike Fowler says, a typical citation just references the idea (the kind of thing you could take away by skimming the abstract), because digging down further for every paper is unworkable.  

That's notably not true of methods papers, particularly when the method is implemented in software.  When lots of scientists apply the method, rather than just citing the paper to support some claim, errors or bugs in the implementation can easily surface.  These don't always take the form of what you're calling post-publication peer review -- the software is just updated, or sometimes/eventually replaced by a better method and science progresses.  Importantly, this isn't a binary process of "trust-worthy" and "not trust-worthy", since every paper has limitations and many have something valuable to build on, provided it's easy enough to do so.  The harder it is, the fewer papers go in that 1% category.  

Making more elements of the paper easy to build upon would I believe increase both the potential impact of a paper and (thus) the post-publication peer review.  Publishing data is another avenue for that to happen.  Like building on the methods, this is twofold more robust than simply citing a claim -- First, because trusting the claims a paper makes means trusting the data, the analysis, etc, instead of just trusting the data; and second, because it tempts more eyeballs to go beyond the abstract. If the only value of your paper to me is some claim (because it's the only part I can build upon without undo effort), I have little incentive to do more than skim your papers, and only the 1% I don't skim can be part of a real post-publication review.  

Notably, we trust methods that have actually been widely reused, not just any method that's passed peer review.  Such papers make up the current 1% of papers that have really been (implicitly) post-publication peer reviewed, so it's little surprise that methods papers are the generally the most cited publications of the past few decades.  If other parts of the research process were likewise easy to build upon, the whole system might be a bit more robust, a bit more scalable, and a bit, well, more useful to read beyond the abstract?  
