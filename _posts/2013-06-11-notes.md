---
published: false

---



#### What constitutes dat quality?

**Statistical quality** 
Measurement error, the degree to which potential covariates are either controlled for or co-measured, and the sample size.  

**Computational quality**
Open and standardized format (csv, xml in defined schema of the field, etc) Thorough metadata conforming to tightly defined and verifiable schema. Terms defined as URIs / linked data.  Openly accessible under permissive licensing.  

#### How might data quality be assessed generally?

Statistical quality depends on what the researcher wants to do with it, and is generally easy for the researcher to assess. All the elements listed under statistical quality should simply be included in metadata, and researchers can draw their own conclusions about sample size, covariates, etc.

Computational quality measurement should reflect data management practices more broadly.  Something like Tim Berners Lee's five-star mechanism for linked data, or badges that indicate if data includes verification tools, etc. 

#### DataCite's Role?

Good quality data should be incentivised the same way good publications are. If good quality data can make a career, researchers will rise to the challenge with a little help for proofreading and formatting.  Ease of depositing data in a useful format and sharing that data, along with metrics to reflect the impact it has, can go a long way in this regard.  
