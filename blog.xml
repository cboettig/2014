<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Lab Notebook - blog</title>
 <link href="/2014/blog.xml" rel="self"/>
 <link href="/"/>
 <updated>2015-02-25T23:05:31+00:00</updated>
 <id>http://www.carlboettiger.info/2014</id>
 <author>
   <name>Carl Boettiger</name>
   <email>cboettig@gmail.com</email>
 </author>

 
 <entry>
   <title>Three Interfaces For Docker</title>
	 <link href="/2014/11/03/three-interfaces-for-Docker.html"/>
   <updated>2014-11-03T00:00:00+00:00</updated>
   <id>/11/03/three-interfaces-for-Docker</id>
   <content type="html">&lt;p&gt;Here I outline three broad, different strategies for incorporating Docker into a user’s workflow, particularly from the perspective of an instructor getting a group of students up and running in a containerized environment, but also in the context of more generic collaborations. The options require progressively more setup and result in a progressively more ‘native’ feel to running Docker. My emphasis is on running Dockerized R applications and RStudio, though much the same thing can be accomplished with iPython notebooks and many other web apps.&lt;/p&gt;
&lt;p&gt;Of course the great strength of Docker is the relative ease with which one can move between these three strategies while using the identical container, maintaining a consistent computational environment in each case.&lt;/p&gt;
&lt;h2 id=&quot;web-hosted-docker&quot;&gt;Web-hosted Docker&lt;/h2&gt;
&lt;p&gt;In this approach, RStudio-server is deployed on a web server and accessed through the browser. The use of Docker containers makes it easier for an instructor to deploy a consistent environment quickly with the desired software pre-installed and pre-configured.&lt;/p&gt;
&lt;h3 id=&quot;advantages&quot;&gt;Advantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A user just needs a web browser and the URL of the server.&lt;/li&gt;
&lt;li&gt;No need to install any local software.&lt;/li&gt;
&lt;li&gt;No need to download big files.&lt;/li&gt;
&lt;li&gt;Should work with any device that supports a modern browser, including most tablets.&lt;/li&gt;
&lt;li&gt;Convenient to temporarily scale computation onto a larger system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages&quot;&gt;Disadvantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;requires a network connection (at all times)&lt;/li&gt;
&lt;li&gt;requires access to a server with sufficient computational power for the task.&lt;/li&gt;
&lt;li&gt;Someone has to manage user &amp;amp; network security (as with any web server).&lt;/li&gt;
&lt;li&gt;Need additional mechanisms for moving files on and off the server, such as git.&lt;/li&gt;
&lt;li&gt;No native interfaces available, must manage files, edit text etc. through the RStudio IDE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;setup&quot;&gt;Setup:&lt;/h3&gt;
&lt;p&gt;A Docker container running RStudio can be deployed with a single command, see &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image&quot;&gt;rocker wiki instructions on RStudio&lt;/a&gt; for details. The instructor or team-member responsible for the setup would simply need to install docker on server. If multiple students will be accessing a single RStudio-server instance, it must be configured for multiple users. Alternately multiple containers can be run on different ports of the same server. (See wiki).&lt;/p&gt;
&lt;p&gt;Hint: Users can also take advantage of the new R package &lt;a href=&quot;https://github.com/sckott/analogsea&quot;&gt;analogsea&lt;/a&gt; to quickly launch and manage an RStudio Server instance on the Digital Ocean cloud platform. &lt;code&gt;analogsea&lt;/code&gt; can also facilitate transfers of code and other files onto and off of the server.&lt;/p&gt;
&lt;h2 id=&quot;self-hosted-docker&quot;&gt;Self-hosted Docker&lt;/h2&gt;
&lt;p&gt;In this approach, the user installs docker (via &lt;code&gt;boot2docker&lt;/code&gt;, if necessary) on their local machine, but still interacts with the container using the same web-based interface (e.g. &lt;code&gt;rstudio-server&lt;/code&gt;, &lt;code&gt;ipython-notebook&lt;/code&gt;) that one would use in the cloud-hosted model.&lt;/p&gt;
&lt;h3 id=&quot;advantages-1&quot;&gt;Advantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;No need for a network connection (at least once the container image is downloaded / transfered)&lt;/li&gt;
&lt;li&gt;No need to have a server available (with the associated cost and security overhead)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages-1&quot;&gt;Disadvantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;More initial setup: install &lt;code&gt;docker&lt;/code&gt; locally, or install &lt;code&gt;boot2docker&lt;/code&gt; for Mac/Windows users.&lt;/li&gt;
&lt;li&gt;Need to use &lt;code&gt;git&lt;/code&gt; or &lt;code&gt;docker copy&lt;/code&gt; to move files from the container to the host or vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hint: Users might also check out the R package &lt;a href=&quot;https://github.com/wch/harbor&quot;&gt;harbor&lt;/a&gt; for interacting with Docker locally from R.&lt;/p&gt;
&lt;h3 id=&quot;setup-1&quot;&gt;Setup:&lt;/h3&gt;
&lt;p&gt;Setup is much the same as on a remote server, though there is no need to set custom usernames or passwords since the instance will be accessible only to local users. See &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image&quot;&gt;rocker wiki instructions on RStudio&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2 id=&quot;integrated-docker&quot;&gt;Integrated Docker&lt;/h2&gt;
&lt;p&gt;This approach is the same as the self-hosted approach, except that we link shared volumes with the host. At minimum this makes it easier to move files on and off the container without learning git.&lt;/p&gt;
&lt;p&gt;An intriguing advantage of this approach is that it does not restrict the user to the RStudio IDE as a way of editing text, managing files and versions, etc. Most users do not rely exclusively on RStudio for these tasks, and may find that restriction limiting. The integrated approach may be more suited for experienced users who are set in their ways and do not need a pixel-identical work environment of RStudio useful for following directions in a classroom. In the integrated approach, a user can continue to rely on whatever their preferred native tools are, while ensuring that code execution occurs (invisibly) on a Dockerized container.&lt;/p&gt;
&lt;h3 id=&quot;advantages-2&quot;&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Can use native OS tools (text editors, file browsers, version control front ends, etc) for all interactions&lt;/li&gt;
&lt;li&gt;No network required (once image is downloaded / transfered).&lt;/li&gt;
&lt;li&gt;No servers required&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages-2&quot;&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Additional setup beyond self-hosting: mapping shared volumes, managing user permissions.&lt;/li&gt;
&lt;li&gt;Potentially less well suited for classroom use, which may benefit from everyone using the identical RStudio interface rather than a range of different text editors, etc. (Of course one can still share volumes while using RStudio as the IDE).&lt;/li&gt;
&lt;li&gt;Cannot open external windows (e.g. if running R in terminal instead of RStudio, the container running R cannot open an X11 window to display plots. Instead, a user must do something like &lt;code&gt;ggsave()&lt;/code&gt; after plotting interactively to view the resulting graphic in the native file browser. (This is more tedious in base graphics that need &lt;code&gt;dev.off()&lt;/code&gt; etc.). Of course this is not an issue when using RStudio with linked volumes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;setup-2&quot;&gt;Setup&lt;/h3&gt;
&lt;p&gt;The key here is simply to link the working directory on the host to the file system on the container. That way any changes made to the host copy using the host OS tools are immediately available to the container, and vice versa. Setup requires a bit more effort on Windows at this time, though is natively supported for Mac in Docker 1.3. Some care may also be necessary not to change the permissions of the file. See details in the &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Shared-files-with-host-machine&quot;&gt;rocker wiki on shared files&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;aliases&quot;&gt;aliases&lt;/h4&gt;
&lt;p&gt;The most aggressive form of the integrated approach is to literally alias common commands like &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;rstudio&lt;/code&gt; as the corresponding docker calls in &lt;code&gt;.bashrc&lt;/code&gt;, e.g.&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;alias&lt;/span&gt; R=&lt;span class=&quot;st&quot;&gt;&amp;#39;docker run --rm -it --user docker -v $(pwd):/home/docker/`basename $PWD` -w /home/docker/`basename $PWD` rocker/hadleyverse R&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;makes the command &lt;code&gt;R&lt;/code&gt; launch an instance of the &lt;code&gt;rocker/hadleyverse&lt;/code&gt; container sharing the current working directory. Clearly different containers could be substituted in place of &lt;code&gt;rocker/hadleyverse&lt;/code&gt;, including custom extensions. This helps ensure that R is always run in the portable, Dockerized environment. Other than the lack of X11 display for plots, this works and feels identical to an interactive R terminal session.&lt;/p&gt;
&lt;h4 id=&quot;other-tweaks&quot;&gt;Other tweaks&lt;/h4&gt;
&lt;p&gt;Mac/Windows users might also want to customize &lt;code&gt;boot2docker&lt;/code&gt;’s resources to make more of the host computer’s memory and processors available to Docker.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Response To Software Discovery Index Report</title>
	 <link href="/2014/10/08/response-to-software-discovery-index-report.html"/>
   <updated>2014-10-08T00:00:00+00:00</updated>
   <id>/10/08/response-to-software-discovery-index-report</id>
   <content type="html">&lt;p&gt;The NIH has recently announced the &lt;a href=&quot;http://softwarediscoveryindex.org/report&quot;&gt;report&lt;/a&gt; of a landmark meeting which presents a vision for a &lt;em&gt;Software Discovery Index&lt;/em&gt; (SDI). The report is both timely and focused on the key issues of locating, citing, reusing software:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Software developers face challenges disseminating their software and measuring its adoption. Software users have difficulty identifying the most appropriate software for their work. Journal publishers lack a consistent way to handle software citations or to ensure reproducibility of published findings. Funding agencies struggle to make informed funding decisions about which software projects to support, while reviewers have a hard time understanding the relevancy and effectiveness of proposed software in the context of data management plans and proposed analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To address this, they propose an Index which would do three things:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;to assign standard and unambiguous identifiers to reference all software,&lt;/li&gt;
&lt;li&gt;to track specific metadata features that describe that software, and&lt;/li&gt;
&lt;li&gt;to enable robust querying of all relevant information for users.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The report is both timely and focused on key issues confronting our community, including the challenges of identifying, citing, and reusing software. The appendices do an excellent job in outlining key metadata, metrics, and use cases which help frame the discussion. The proposal does well to focus on the importance of identifiers and the creation of a query-able metadata index for research software, but leaves out an essential element necessary to make this work.&lt;/p&gt;
&lt;p&gt;This proposal sounds very much like the CrossRef and DataCite infrastructure already in place for academic literature and data, respectively; and indeed this is an excellent model to follow. However, a key piece of that infrastructure is missing from the present proposal – the social contract between repository or publisher and the index itself.&lt;/p&gt;
&lt;p&gt;CrossRef provides unique identifiers for the academic literature (CrossRef DOIs), but it also defines specific metadata that describe that literature (as well as metrics of its use), and embed that information into a robust, query-able, machine-readable format. DataCite does the same for scientific data. These are exactly the features that the authors of the report seek to emulate.&lt;/p&gt;
&lt;p&gt;Just as CrossRef itself does not host academic papers but only the metadata records, the SDI does not propose to host software itself. This introduces a substantial challenge in &lt;em&gt;maintaining the link&lt;/em&gt; between the metadata and the software itself. The authors have simply proposed that the metadata include “Links to the code repository.” If CrossRef or DataCite DOIs worked in this way, we would soon loose all ability to recover many of the papers or the data itself, and we would be left with only access to the metadata record and a broken link. DOIs were created explicitly to solve this problem, not through technology, but through a &lt;em&gt;social contract&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The scientific publishers who host the actual publications are responsible for ensuring that this link is always maintained when they change names, etc. Should the publisher go out of business, these links may be adjusted to point to a new home, such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;. This guarantees that the DOI always resolves to the resource in question, regardless of where it moves. Should a publisher fail to maintain these links, CrossRef may refuse to provide the publisher any additional DOIs, cutting it off from this key index. This is the social contract. Data repositories work in exactly the same way, purchasing their DOIs from DataCite. (While financial transaction isn’t strictly necessary for the financial contract, it provides a clear business model for maintaining the key organization responsible for the index).&lt;/p&gt;
&lt;p&gt;Without such a mechanism, links in the SDI would surely rot away, all the more rapidly in the fast-moving world of software. Without links to the software itself, the function of the index would be purely academic. Yet such a mechanism requires that the software repositories, not the individual software authors, would be willing to accept the same social contract, receiving (and possibly paying for) identifiers on the condition that they assume the responsibility of maintaining the links. It is unclear that the primary software repositories in use to day (Sourceforge, Github, Bitbucket, etc) would be willing to accept this.&lt;/p&gt;
&lt;p&gt;Data repositories already offer many of the compelling features of this proposal. Many data repositories accept a wide array of file formats including software packages, and would provide such software with a permanent unique identifier in the form of a DataCite DOI, as well as collecting much of the essential metadata listed in report’s Appendix 1, which would then already be accessible through the DataCite API in a nice machine-readable format. This strategy finds several aspects wanting.&lt;/p&gt;
&lt;p&gt;The primary barrier to using data repositories indexed by DataCite arises from the dynamic nature of software relative to data. Data repositories are designed to serve relatively static content with few versions. Software repositories, by contrast, are usually built upon explicit version control platforms such as Git or Subversion designed explicitly for handling continual changes, including branches and mergers, of software code. The report discusses the challenges of software versions as a reason for that citing a software paper as a proxy for citing software is not ideal: the citation to the paper does not convey what version was used. Rapid versioning creates other problems though, both in the number of identifiers that might be created (is each commit a new identifier?) and defining the relationship between different versions of the same software. Branches and merges exacerbate this problem. Existing approaches that provide the user a one-time way to import software from a software repository to a data repository such as those cited in the report (“One significant initiative is a collaboration between Mozilla, figshare, GitHub, and Zenodo”) do nothing to address this issues.&lt;/p&gt;
&lt;p&gt;Less challenging issues involve resolving differences between DataCite metadata and the proposed metadata records for software. Most obviously, the metadata would need a way to declare the object involved software instead of data per se, which would thus allow queries to restrict results to ‘software’ objects to avoid cluttering searches. Ideally, one would also create tools that can import such metadata from the format in which it is usually already defined in software, into the desired format of the index, rather than requiring manual double-entry of this information. These are important but more straight-forward problems which the report already seeks to address.&lt;/p&gt;
&lt;hr /&gt;
&lt;!-- comments
Ilya raises the question: Why not just use Github? I think it is important to note that:

a) Github isn&#39;t forever, repositories come and go all the time, or move to new links, etc

b) Re-creating and running an NIH-Github would be both expensive (Gitlab notwithstanding) and redundant -- researchers would continue to use Github etc.

c) Github provides somewhat limited query-able metadata, that doesn&#39;t capture even the minimal list of fields suggested by the report.
Leveraging existing scientific data repositories by linking them to versioned releases on software repositories addresses each of these problems.

--&gt;
&lt;!--
A. FRAMEWORK SUPPORTING THE SOFTWARE DISCOVERY INDEX
Unique identifiers
Connections to publishers
Use cases
Complementarity with the Data Discovery Index
B. CHALLENGES AND REMAINING QUESTIONS
Defining relevant software
Integrating with other repositories
Evaluating progress and distinguishing this from other efforts
C. IMPLEMENTATION ROADMAP
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts On Harte Interview</title>
	 <link href="/2014/09/05/thoughts-on-Harte-interview.html"/>
   <updated>2014-09-05T00:00:00+00:00</updated>
   <id>/09/05/thoughts-on-Harte-interview</id>
   <content type="html">&lt;p&gt;Read this delightful &lt;a href=&quot;http://www.biodiverseperspectives.com/2014/08/31/diverse-introspectives-a-conversation-with-john-harte/&quot;&gt;John Harte Interview&lt;/a&gt; (ht &lt;span class=&quot;citation&quot; data-cites=&quot;DynamicEcology&quot;&gt;@DynamicEcology&lt;/span&gt;) which resonates rather well with me (no surprise given our common background). Nevertheless, I would have to push back on a few pieces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From general laws flow absolutely bullet-proof insights and this is what we most need&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I must disagree. Bullet-proof insights are mathematical theorems. Their generality is an empirical question about their assumptions (not their predictions). (And lastly, I’m pretty much in support of the “what we most need” part, but it deserves to be substantiated. Certain objectives could be met just fine with a black box/crystal ball predictor of the future that provided no general laws or insights. It remains to be demonstrated that such predictions are (a) not all that is needed or (b) are impossible without general laws).&lt;/p&gt;
&lt;p&gt;The critique against &lt;em&gt;fitting&lt;/em&gt; mechanistic models is a very different argument than a critique against writing down a proposed mechanism merely to study it in the absence of data. Such are the role of theorems in ecology. I’ve no doubt John appreciates the importance of such work, though perhaps too rarely do we acknowledge that such contributions are more fundamental and robust, not less, precisely because they do not involve fitting parameters to data. Theoretical ecology has a rich contribution independent of any observation. Conditions under which populations can oscillate without being driven periodically. The necessary conditions for coexistence of &lt;span class=&quot;math&quot;&gt;\(n\)&lt;/span&gt; species, and role of spatial/temporal heterogeneity therein. Threshold dynamics/&lt;span class=&quot;math&quot;&gt;\(r_0\)&lt;/span&gt;. The evolution of dispersal (e.g. why the existence of spatial heterogeneity alone is not sufficient), of demonstrating that connecting two “sink” patches in which a resident population cannot persist can enable persistence. This kind of work has the bullet-proof status of theorem without making any claim on generality. These are laws in a mathematical sense. Whether they are laws in an ecological sense and whether they are general or not is a question of how often their assumptions are met. This is why, as Tony Ives says, we must test &lt;em&gt;assumptions&lt;/em&gt;, not (merely) &lt;em&gt;predictions&lt;/em&gt; (or worse, “post-dictions” of model fit). How we testing assumption is not as statistically well-posed as how we fit models, but it need not be hard. The assumption that space and time are not everywhere homogeneous is relatively easy to establish across a wide range of scales. Not all assumptions are so easy: That dynamics should be largely restricted to low-dimensional manifolds is far from obvious (at almost any scale).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One of them is the ease with which we can simulate numerically and handle massive data sets. There is a risk that this will divorce people from what really matters, which is the natural world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we can draw meaningful conclusions from the exercises in mathematical logic above (which John no doubt appreciates we have, and would no doubt agree could guide and clarify a lot of muddled thinking), then the same can be said of numerics. Like anything else, of course it can be done poorly, but numerical studies are no more inherently divorced from the natural world than good analytic theory.&lt;/p&gt;
&lt;p&gt;It is unfortunate that ‘massive data sets’ fall in the same sentence, as the term is not synonymous with ‘numerical simulation’. Again, it is perhaps easy to misuse such data in ignorance of the natural world, just like anything else can be done poorly. Yet John has eloquently argued the danger of the ‘mechanistic approach’ allowing intuition to select a few arbitrary features we consider to be important, and the same might be said of the observations themselves. No one would argue that all that is interesting happens to be things we can observe and manipulate on the temporal and spatial scales of an individual human being (e.g. in the domain of field work). Aggregating data across larger temporal and spatial scales, being able to take advantage of rich information about environment, climate, genetics, phylogeny, and so forth in our understanding of patterns and process is important.&lt;/p&gt;
&lt;p&gt;John has an excellent bit in the interview about science advancing by failures rather than successes; that we learn the most when we can observe deviations from the model. In this I see his views and Tony’s views as two sides of the same coin: it is in discovering and then understanding the mechanisms behind the deviations from general theory that we most advance. John’s example from statistical mechanics resonates strongly with me here – deviations from the ideal gas law not only expose interesting new science (forget dipole moments, this is the whole molecular, atomized world view replacing a continuous one), while also recovering the general law under the appropriate limits (of temperature and pressure but also number of atoms).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Docker Notes</title>
	 <link href="/2014/08/14/docker-notes.html"/>
   <updated>2014-08-14T00:00:00+00:00</updated>
   <id>/08/14/docker-notes</id>
   <content type="html">&lt;p&gt;Ticking through a few more of the challenges I raised in my &lt;a href=&quot;http://www.carlboettiger.info/2014/08/07/too-much-fun-with-docker.html&quot;&gt;first post on docker&lt;/a&gt;; here I explore some of the issues about facilitating interaction with a docker container so that a user’s experience is more similar to working in their own environment and less like working on a remote terminal over ssh. While technically minor, these issues are probably the first stumbling blocks in making this a valid platform for new users.&lt;/p&gt;
&lt;h2 id=&quot;sharing-a-local-directory&quot;&gt;Sharing a local directory&lt;/h2&gt;
&lt;p&gt;Launch a bash shell on the container that shares the current working directory of the host machine (from &lt;code&gt;pwd&lt;/code&gt;) with the &lt;code&gt;/host&lt;/code&gt; directory on the container (thanks to Dirk for this solution):&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;:/host cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows a user to move files on and off the container, use a familiar editor and even handle things like git commits / pulls / pushes in their working directory as before. Then the code can be executed in the containerized environment which handles all the dependencies. From the terminal docker opens, we just &lt;code&gt;cd /host&lt;/code&gt; where we find our working directory files, and can launch R and run the scripts. A rather clean way of maintaining the local development experience but containerizing execution.&lt;/p&gt;
&lt;p&gt;In particular, this frees us from having to pass our git credentials etc to the container, though is not so useful if we’re wanting to interact with the container via the RStudio server instead of R running in the terminal. (More on getting around this below).&lt;/p&gt;
&lt;p&gt;Unfortunately, Mac and Windows users have to run Docker inside an already-virualized environment such as provided by &lt;code&gt;boot2docker&lt;/code&gt; or &lt;code&gt;vagrant&lt;/code&gt;. This means that it is only the directories on the virtualized environment, not those on the native OS, can be shared in this way. While one could presumably keep a directory synced between this virtual environment and the native OS, (standard in in &lt;code&gt;vagrant&lt;/code&gt;), this is a problem for the easier-to-use &lt;code&gt;boot2docker&lt;/code&gt; at this time: (&lt;a href=&quot;https://github.com/docker/docker/issues/7249&quot;&gt;docker/issues/7249&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;a-docker-desktop&quot;&gt;A Docker Desktop&lt;/h2&gt;
&lt;p&gt;Dirk brought this &lt;a href=&quot;http://blog.docker.com/2013/07/docker-desktop-your-desktop-over-ssh-running-inside-of-a-docker-container&quot;&gt;docker-desktop&lt;/a&gt; to my attention; which uses Xpra (in place of X11 forwarding) to provide a window with fluxbox running on Ubuntu along with common applications like libreoffce, firefox, and rox file manager. Pretty clever, and worked just fine for me, but needs Xpra on the client machine and requires some extra steps (run the container, query for passwords and ports, run ssh to connect, then run Xpra to launch the window). The result is reasonably responsive but still slower than virtualbox, and probably too slow for real work.&lt;/p&gt;
&lt;h2 id=&quot;base-images&quot;&gt;Base images?&lt;/h2&gt;
&lt;p&gt;The basic Ubuntu:14.04 seems like a good lightweight base image (at 192 MB), but other images try to give more useful building blocks, like &lt;a href=&quot;https://github.com/phusion/baseimage-docker#contents&quot;&gt;phusion/baseimage&lt;/a&gt; (423 MB). Their &lt;code&gt;docker-bash&lt;/code&gt; script and other utilities provide some handy features for managing / debugging containers.&lt;/p&gt;
&lt;h2 id=&quot;other-ways-to-share-files&quot;&gt;Other ways to share files?&lt;/h2&gt;
&lt;p&gt;Took a quick look at this &lt;a href=&quot;https://github.com/gfjardim/docker-dropbox/blob/master/Dockerfile&quot;&gt;Dockerfile for running dropbox&lt;/a&gt;, which works rather well (at least on a linux machine, since it requires local directory sharing). Could probably be done without explicit linking to local directories to faciliate moving files on and off the container. Of course one can always scp/rsync files on and off containers if ssh is set up, but that is unlikely to be a popular solution for students.&lt;/p&gt;
&lt;p&gt;While we have rstudio server running nicely in a Docker container for local or cloud use, it’s still an issue getting Github ssh keys set up to be able to push changes to a repo. We can get around this by linking to our keys directory with the same &lt;code&gt;-v&lt;/code&gt; option shown above. We still need a few more steps: setting the Git username and email, and running &lt;code&gt;ssh-add&lt;/code&gt; for the key. Presumably we could do this with environmental variables and some adjustment to the Dockerfile:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v /path/to/keys:/home/rstudio/.ssh/ -e &lt;span class=&quot;st&quot;&gt;&amp;quot;USERNAME=Carl Boettiger&amp;quot;&lt;/span&gt; -e &lt;span class=&quot;st&quot;&gt;&amp;quot;EMAIL=cboettig@example.org&amp;quot;&lt;/span&gt; cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would prevent storing these secure values on the image itself.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An appropriate amount of fun with docker?</title>
	 <link href="/2014/08/08/an-appropriate-amount-of-fun-with-docker.html"/>
   <updated>2014-08-08T00:00:00+00:00</updated>
   <id>/08/08/an-appropriate-amount-of-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;em&gt;An update on my exploration with Docker. Title courtesy of &lt;a href=&quot;https://twitter.com/DistribEcology/status/497523435371638784&quot;&gt;Ted&lt;/a&gt;, with my hopes that this really does move us in a direction where we can spend less time thinking about the tools and computational environments. Not there yet though&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I’ve gotten RStudio Server working in the &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;ropensci-docker&lt;/a&gt; image (Issues/pull requests welcome!).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 8787:8787 cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will make an RStudio server instance available to you in your browser at localhost:8787. (Change the first number after the -p to have a different address). You can log in with username:pw rstudio:rstudio and have fun.&lt;/p&gt;
&lt;p&gt;One thing I like about this is the ease with which I can now get an RStudio server up and running in the cloud (e.g. I took this for sail on DigitalOcean.com today). This means in few minutes and 1 penny you have a URL that you and any collaborators could use to interact with R using the familiar RStudio interface, already provisioned with your data and dependencies in place.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For me this is a pretty key development. It replaces a lot of command-line only interaction with probably the most familiar R environment out there, online or off. For more widespread use or teaching this probably needs to get simpler still. I’m still skeptical that this will make it out beyond the crazies, but I’m less skeptical than I was when starting this out.&lt;/p&gt;
&lt;p&gt;The ropensci-docker image could no doubt be more modular (and better documented). I’d be curious to hear if anyone has had success or problems running docker on windows / mac platforms. Issues or pull requests on the repo would be welcome! https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile (maybe the repo needs to be renamed from it’s original fork now too…)&lt;/p&gt;
&lt;p&gt;Rich et al highlighted several “remaining challenges” in their original post. Here’s my take on where those stand in the Docker framework, though I’d welcome other impressions:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;dependencies could still be missed by incompletely documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think this one is largely addressed, at least assuming a user loads the Docker image. I’m still concerned that later builds of the docker image could simply break the build (though earlier images may still be available). Does anyone know how to roll back to earlier images in docker?&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The set of scripts for managing reproducibility are at least as complex as the analysis itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think a lot of that size is due to the lack of an R image for Travis and the need to install many common tools from scratch. Because docker is both modular and easily shared via docker hub, it’s much easier to write a really small script that builds on existing images, (as I show in cboettig/rnexml)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Travis.org CI constraints: public/open github repository with analyses that run in under 50 minutes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Docker has two advantages and also some weaknesses here: (1) it should be easy to run locally, while accomplishing much of the same thing as running on travis (though clearly that’s not as nice as running automatically &amp;amp; in the cloud on every push). (2) It’s easier to take advantage of caching – for instance, cboettig/rnexml provides the knitr cache files in the image so that a user can start exploring without waiting for all the data to download and code to run.&lt;/p&gt;
&lt;p&gt;It seems that Travis CI doesn’t currently support docker since the linux kernel they use is too old. (Presumably they’ll update one day. Anyone try Shippable CI? (which supports docker))&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The learning curve is still prohibitive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think that’s still true. But what surprised me is that I’m not sure that it’s gotten any worse by adding docker than it was to begin with using Travis CI. Because the approach can be used both locally and for scaling up in the cloud, I think it offers some more immediate payoffs to users than learning a Github+CI approach does. (Notably it doesn’t require any git just to deploy something ‘reproducible’, though of course it works nicely with git.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Too Much Fun With Docker</title>
	 <link href="/2014/08/07/too-much-fun-with-docker.html"/>
   <updated>2014-08-07T00:00:00+00:00</updated>
   <id>/08/07/too-much-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This post was originally drafted as a set of questions to the revived &lt;a href=&quot;https://groups.google.com/forum/#!forum/ropensci-discuss&quot;&gt;ropensci-discuss list&lt;/a&gt;, hopefully readers might join the discussion from there.&lt;/p&gt;
&lt;p&gt;Been thinking about Docker and the discussion about reproducible research in the comments of Rich et al’s recent post on the &lt;a href=&quot;ropensci.org/blog/2014/06/09/reproducibility/&quot;&gt;rOpenSci blog&lt;/a&gt; where quite a few of people mentioned the potential for Docker as a way to facilitate this.&lt;/p&gt;
&lt;p&gt;I’ve only just started playing around with Docker, and though I’m quite impressed, I’m still rather skeptical that non-crazies would ever use it productively. Nevertheless, I’ve worked up some Dockerfiles to explore how one might use this approach to transparently document and manage a computational environment, and I was hoping to get some feedback from all of you.&lt;/p&gt;
&lt;p&gt;For those of you who are already much more familiar with Docker than me (or are looking for an excuse to explore!), I’d love to get your feedback on some of the particulars. For everyone, I’d be curious what you think about the general concept.&lt;/p&gt;
&lt;p&gt;So far I’ve created a &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;dockerfile&lt;/a&gt; and &lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/ropensci-docker/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have docker up and running, perhaps you can give it a test drive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should find R installed with some common packages. This image builds on Dirk Eddelbuettel’s R docker images and serves as a starting point to test individual R packages or projects.&lt;/p&gt;
&lt;p&gt;For instance, my RNeXML manuscript draft is a bit more of a bear then usual to run, since it needs &lt;code&gt;rJava&lt;/code&gt; (requires external libs), &lt;code&gt;Sxslt&lt;/code&gt; (only available on Omegahat and requires extra libs) and latest &lt;code&gt;phytools&lt;/code&gt; (a tar.gz file from Liam’s website), along with the usual mess of pandoc/latex environment to compile the manuscript itself. By building on ropensci-docker, we need a &lt;a href=&quot;https://github.com/ropensci/RNeXML/tree/master/manuscripts/Dockerfile&quot;&gt;pretty minimal docker file&lt;/a&gt; to compile this environment:&lt;/p&gt;
&lt;p&gt;You can test drive it (&lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/rnexml&quot;&gt;docker image here&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/rnexml /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once in bash, launch R and run &lt;code&gt;rmarkdown::render(&amp;quot;manuscript.Rmd&amp;quot;)&lt;/code&gt;. This will recompile the manuscript from cache and leave you to interactively explore any of the R code shown.&lt;/p&gt;
&lt;h2 id=&quot;advantages-goals&quot;&gt;Advantages / Goals&lt;/h2&gt;
&lt;p&gt;Being able to download a pre-compiled image means a user can run the code without dependency hell (often not as much an R problem as it is in Python, but nevertheless one that I hit frequently, particularly as my projects age), and also without altering their personal R environment. Third (in principle) this makes it easy to run the code on a cloud server, scaling the computing resources appropriately.&lt;/p&gt;
&lt;p&gt;I think the real acid test for this is not merely that it recreates the results, but that others can build and extend on the work (with fewer rather than more barriers than usual). I believe most of that has nothing to do with this whole software image thing – providing the methods you use as general-purpose functions in an R package, or publishing the raw (&amp;amp; processed) data to Dryad with good documentation will always make work more modular and easier to re-use than cracking open someone’s virtual machine. But that is really a separate issue.&lt;/p&gt;
&lt;p&gt;In this context, we look for an easy way to package up whatever a researcher or group is already doing into something portable and extensible. So, is this really portable and extensible?&lt;/p&gt;
&lt;h2 id=&quot;concerns&quot;&gt;Concerns:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;This presupposes someone can run docker on their OS – and from the command line at that. Perhaps that’s the biggest barrier to entry right now, (though given docker’s virulent popularity, maybe something smart people with big money might soon solve).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The only way to interact with thing is through a bash shell running on the container. An RStudio server might be much nicer, but I haven’t been able to get that running. &lt;em&gt;Anyone know how to run RStudio server from docker?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(I tried &amp;amp; &lt;a href=&quot;https://github.com/mingfang/docker-druid/issues/2&quot;&gt;failed&lt;/a&gt;)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;I don’t see how users can move local files on and off the docker container. In some ways this is a great virtue – forcing all code to use fully resolved paths like pulling data from Dryad instead of their hard-drive, and pushing results to a (possibly private) online site to view them. But obviously a barrier to entry. &lt;em&gt;Is there a better way to do this?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;alternative-strategies&quot;&gt;Alternative strategies&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Docker is just one of many ways to do this (particularly if you’re not concerned about maximum performance speed), and quite probably not the easiest. Our friends at Berkeley D-Lab opted for a GUI-driven virtual machine instead, built with Packer and run in Virtualbox, after their experience proved that students were much more comfortable with the mouse-driven installation and a pixel-identical environment to the instructor’s (see their excellent &lt;a href=&quot;https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl&quot;&gt;paper&lt;/a&gt; on this).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Will/should researchers be willing to work and develop in virtual environments? In some cases, the virtual environment can be closely coupled to the native one – you use your own editors etc to do all the writing, and then execute in the virtual environment (seems this is easier in docker/vagrant approach than in the BCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--

I would like a way for a collaborator who knows a little R to be able to open my .Rmd manuscript on his/her own computer, edit some parameters, recompile and view the pdf. RStudio w/ rmarkdown has gone a long way to making that happen.
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Is statistical software harmful?</title>
	 <link href="/2014/06/04/is-statistical-software-harmful.html"/>
   <updated>2014-06-04T00:00:00+00:00</updated>
   <id>/06/04/is-statistical-software-harmful</id>
   <content type="html">&lt;p&gt;Ben Bolker has an excellent post on this complex issue over &lt;a href=&quot;http://dynamicecology.wordpress.com/2014/06/04/guest-post-is-statistical-software-harmful&quot;&gt;at Dynamic Ecology&lt;/a&gt;, which got me thinking about writing my own thoughts on the topic in reply.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Google recently announced that it will be making it’s own self-driving cars, rather than modifying those of others. &lt;a href=&quot;http://www.automotive.com/news/1405-google-envisions-self-driving-cars-with-no-steering-wheel/&quot;&gt;Cars that won’t have steering wheels and pedals&lt;/a&gt;. Just a button that says “stop.” What does this tell us about the future of user-friendly complex statistical software?&lt;/p&gt;
&lt;p&gt;Ben quotes prominent statisticians voicing fears that echo common concerns about self-driving cars:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Andrew Gelman attributes to Brad Efron the idea that “recommending that scientists use Bayes’ theorem is like giving the neighbourhood kids the key to your F-16″.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think it is particularly interesting and instructive that the quote Gelman attributes to Efron is about a mathematical theorem rather than about software (e.g. Bayes Theorem, not WinBUGS). Even relatively simple statistical concepts like &lt;span class=&quot;math&quot;&gt;\(p\)&lt;/span&gt; values can cause plenty of confusion, statistical package or no. The concerns are not unique to software, so the solutions cannot come through limiting access to software.&lt;/p&gt;
&lt;p&gt;I am very wary of the suggestion that we should address concerns of appropriate application by raising barriers to access. Those arguments have been made about knowledge of all forms, from access to publications, to raw data, to things as basic as education and democratic voting.&lt;/p&gt;
&lt;p&gt;There are many good reasons for not creating a statistical software implementation of a new method, but I argue here that fear of misuse just is not one of them.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;em&gt;The barriers created by not having a convenient software implementation are not an appropriate filter to keep out people who can miss-interpret or miss-use the software. As you know, a fundamentally different skillset is required to program a published algorithm (say, MCMC), than to correctly interpret the statistical consequences.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We must be wary of a different kind of statistical machismo, in which we use the ability to implement a method by one’s self as a proxy for interpreting it correctly.&lt;/p&gt;
&lt;p&gt;1a) One immediate corollary of (1) is that: &lt;em&gt;Like it or not, someone is going to build a method that is “easy to use”, e.g. remove the programming barriers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1b) The second corollary is that: &lt;em&gt;individuals with excellent understanding of the proper interpretation / statistics will frequently make mistakes in the computational implementation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Both mistakes will happen. And both are much more formidable problems in the complex methodology of today than when computer was a job description.&lt;/p&gt;
&lt;p&gt;So, what do we do? I think we should abandon the &lt;a href=&quot;http://www.r-bloggers.com/what-is-correctness-for-statistical-software/&quot;&gt;false dichotomy between “usability” and “correctness.”&lt;/a&gt;. Just because software that is easy to use is easy to misuse, does not imply that decreasing usability increases correctness. I think that is a dangerous fallacy.&lt;/p&gt;
&lt;p&gt;A software implementation should aim first to remove the programming barriers rather than statistical knowledge barriers. Best practices such as modularity and documentation should make it easy for users and developers to understand and build upon it. I agree with Ben that software error messages are poor teachers. I agree that a tool cannot be foolproof, no tool ever has been.&lt;/p&gt;
&lt;p&gt;Someone does not misuse a piece of software merely because they do not understand it. Misuse comes from mistakenly thinking you understand it. The premise that most researchers will use something they do not understand just because it is easy to use is distasteful.&lt;/p&gt;
&lt;p&gt;Kevin Slavin gives &lt;a href=&quot;http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world&quot;&gt;a fantastic Ted talk&lt;/a&gt; on the ubiquitous role of algorithms in today’s world. His conclusion is neither one of panacea or doom, but rather that we seek to understand and characterize them, learn their strengths and weaknesses like a naturalist studies a new species.&lt;/p&gt;
&lt;p&gt;More widespread adoption of software such as BUGS &amp;amp; relatives has indeed increased the amount of misuse and false conclusions. But it has also dramatically increased awareness of issues ranging from computational aspects peculiar to particular implementations to general understanding and discourse about Bayesian methods. Like Kevin, I don’t think we can escape the algorithms, but I do think we can learn to understand and live with them.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Plos Data Sharing Policy Reflections</title>
	 <link href="/2014/05/30/PLoS-data-sharing-policy-reflections.html"/>
   <updated>2014-05-30T00:00:00+00:00</updated>
   <id>/05/30/PLoS-data-sharing-policy-reflections</id>
   <content type="html">&lt;p&gt;PLOS has posted an &lt;a href=&quot;http://blogs.plos.org/biologue/2014/05/30/plos-data-policy-update/&quot;&gt;excellent update&lt;/a&gt; reflecting on their experiences a few months in to their new data sharing policy, which requires authors to include a statement of where the data can be obtained rather than providing it upon request. They do a rather excellent job of highlighting common concerns and offering well justified and explained replies where appropriate.&lt;/p&gt;
&lt;p&gt;At the end of the piece they pose several excellent questions, which I reflect on here (mostly as a way of figuring out my own thoughts on these issues).&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;When should an author choose Supplementary Files vs a repository vs figures and tables?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To me, repositories should always be the default. Academic repositories provide robust permanent archiving (such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt; backup), independent DOIs to content, often tracking of use metrics, enhanced discoverability, clear and appropriate licenses, richer metadata, as well as frequently providing things like API access and easy-to-use interfaces. They are the Silicon Valley of publishing innovation today.&lt;/p&gt;
&lt;p&gt;Today I think it is much more likely that some material is not appropriate for a ‘journal supplement’ rather than not being able to find an appropriate repository (enough are free, subject agnostic and accept almost any file types). In my opinion the primary challenge is for publishers to tightly integrate the repository contents with their own website, something that the repositories themselves can support with good APIs and embedding tools (many do, PLOS’s coordination with figshare for individual figures being a great example).&lt;/p&gt;
&lt;p&gt;I’m not clear on “vs figures and tables”, as this seems like a content question of “What” should be archived rather than “Where” (unless it is referring to separately archiving the figures and tables of the main text, which sounds like a great idea to me).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Should software/code be treated any differently from ‘data’? How should materials-sharing differ?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the highest level I think it is possible to see software as a ‘type’ of data. Like other data, it is in need of appropriate licensing, a management plan, documentation/metadata, and conforming to appropriate standards and placed in appropriate repositories. Of course what is meant by “appropriate” differs, but that is also true between other types of data. The same motivations for requiring data sharing (understanding and replicating the work, facilitating future work, increasing impact) apply.&lt;/p&gt;
&lt;p&gt;I think we as a scientific community (or rather, many loosely federated communities) are still working out just how best to share scientific code and the unique challenges that it raises. Traditional scientific data repositories are well ahead in establishing best practices for other data, but are rapidly working out approaches to code. The &lt;a href=&quot;http://openresearchsoftware.metajnl.com/about/editorialPolicies&quot;&gt;guidelines&lt;/a&gt; from the Journal of Open Research Software from the UK Software Sustainability Institute are a great example. (I’ve written on this topic before, such as &lt;a href=&quot;http://www.carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;what I look for in software papers&lt;/a&gt; and on the topic of the &lt;a href=&quot;www.carlboettiger.info/2013/09/25/mozilla-software-review.html&quot;&gt;Mozilla Science Code review pilot&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I’m not informed enough to speak to sharing of non-digital material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What does peer review of data mean, and should reviewers and editors be paying more attention to data than they did previously, now that they can do so?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In as much as we are satisfied with the current definition of peer review for journal articles I think this is a false dichotomy. Landmark papers, at least in my field, five or six decades ago (e.g. about as old as the current peer review system) frequently contained all the data in the paper (papers were longer and data was smaller). Somehow the data outgrew the paper and it just became okay to omit it, just as methods have gotten more complex and papers today frequently gloss over methodological details. The problem, then, is not one of type but one of scale: how do you review data when it takes up more than half a page of printed text.&lt;/p&gt;
&lt;p&gt;The problem of scale is of course not limited to data. Papers frequently have many more authors than reviewers, often representing disparate and highly specialized expertise over possibly years of work, depend upon more than 100 citations and be accompanied by as many pages of supplemental material. To the extent that we’re satisfied with how reviewers and editors have coped with these trends, we can hope for the same for data.&lt;/p&gt;
&lt;p&gt;Meanwhile, data transparency and data reuse may be more effective safe guards. Yes, errors in the data may cause trouble before they can be brought to light, just like bugs in software. But in this way they do eventually come to light, and that is somewhat less worrying if we view data the way we currently build publications (e.g. as fundamental building blocks of research) and publications as we currently view data (e.g. as a means to an ends, illustrated in the idea that it is okay to have mistakes in the data as long as they don’t change the conclusions). Jonathan Eisen has some &lt;a href=&quot;http://www.slideshare.net/phylogenomics/jonathan-eisen-talk-on-open-science-at-bosc2012-ismb&quot; title=&quot;see slide 13&quot;&gt;excellent&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=oWZzUe3Kxeo&quot;&gt;examples&lt;/a&gt; in which openly sharing the data led to rapid discovery and correction of errors that might have been difficult to detect otherwise.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And getting at the reason why we encourage data sharing: how much data, metadata, and explanation is necessary for replication?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I agree that the “What” question is a crux issue, and one we are still figuring out by community. There are really two issues here: what data to include, and what metadata (which to me includes any explanation or other documentation of the data) to provide for whatever data is included.&lt;/p&gt;
&lt;p&gt;On appropriate metadata, we’ll never have a one-size-fits-all answer, but I think the key is to at least uphold current community best-practices (best != mode), whatever they may be. Parts of this are easy: scholarly archives everywhere include basic &lt;a href=&quot;http://en.wikipedia.org/wiki/Dublin_Core&quot;&gt;Dublin Core Elements&lt;/a&gt; metadata like title, author, date, subject and unique identifier, and most data repositories will attach this information in a machine-readable metadata format with minimal burden on the author (e.g. &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt;, or to lesser extent, &lt;a href=&quot;http://figshare.org&quot;&gt;figshare&lt;/a&gt;). Many fields already have well-established and tested standards for data documentation, such as the [Ecological Metadata Langauge], which helps ecologists document things like column names and units in an appropriate and consistent way without constraining how the data is collected or structured.&lt;/p&gt;
&lt;p&gt;What data we include in the first place is more challenging, particularly as there is no good definition of ‘raw data’ (one person’s raw data being another person’s highly processed data). I think a useful minimum might be to provide any data shown in a figure or used in a statistical test that appears in the paper.&lt;/p&gt;
&lt;p&gt;Journal policies can help most in each of these cases by pointing authors to the policies of repositories and to subject-specific publications on these best practices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A crucial issue that is much wider than PLOS is how to cite data and give academic credit for data reuse, to encourage researchers to make data sharing part of their everyday routine.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again I agree that credit for data reuse is an important and largely cultural issue. Certainly editors can play there part as they already do in encouraging authors to cite the corresponding papers on the methods used, etc.&lt;/p&gt;
&lt;p&gt;I think the cultural challenge is much greater for the “long tail” content than it is for the most impactful data. I think most of the top-cited papers over the last two decades have been methods papers (or are cited for the use of a method that has become the standard of a field; often as software). As with any citation, there’s a positive feedback as more people are aware of it. I suspect that the papers announcing the first full genomes of commonly studied organisms (essentially data papers, though published by the most exclusive journals) did not lack citations. For data (or methods for that matter) that do not anticipate that level of reuse, the concern of appropriate credit is more real. Even if a researcher can assume they will be cited by future reuse of their data, they may not feel that sufficient compensation if it means one less paper to their name.&lt;/p&gt;
&lt;p&gt;Unfortunately I think these are not issues unique to data publication but germane to academic credit in general. Citations, journal names, and so forth are not meaningless metrics, but very noisy ones. I think it is too easy to fool ourselves by looking only at cases where statistical averages are large enough to see the signal – datasets like the human genome and algorithms like BLAST we know are impactful, and the citation record bears this out. Really well cited papers or well-cited journals tend to coincide with our notions of impact, so it is easy to overestimate the fidelity of citation statistics when the sample size is much smaller. Besides, academic work is a high-dimensional creature not easily reduced to a few scalar metrics. &lt;!--(I think that is why, at least in the US, we tend
to place more trust in the opinions of people over current metrics.)--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And for long-term preservation, we must ask who funds the costs of data sharing? What file formats should be acceptable and what will happen in the future with data in obsolete file formats? Is there likely to be universal agreement on how long researchers should store data, given the different current requirements of institutions and funders?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think these are questions for the scientific data repositories and the fields they serve, rather than the journals, and for the most part they are handling them well.&lt;/p&gt;
&lt;p&gt;Repositories like &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt; have clear pricing schemes closely integrated with other publication costs, and standing at least an order of magnitude less than most journal publication fees look like a bargain. (Not so if you publish in subscription journals I hear you say. Well, I would not be surprised if we start seeing such repositories negotiate institutional subscriptions to cover the costs of their authors).&lt;/p&gt;
&lt;p&gt;I think the question of data formats is closely tied to that of metadata, as they are all topics of best-practices in archiving. Many scientific data repositories have usually put a lot of thought into these issues and also weigh them against the needs and ease-of-use of the communities they serve. Journal data archiving policies can play their part by encouraging best practices by pointing authors to repository guidelines as well as published articles from their community (such as the &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4608&quot;&gt;Nine Simple Ways&lt;/a&gt; paper by White et al.)&lt;/p&gt;
&lt;p&gt;I feel the almost rhetorical question about ‘universal agreement’ is unnecessarily pessimistic. I suspect that much of the variance in recommendations for the duration a researcher should archive their own work predates the widespread emergence of data repositories, which have vastly simplified the issue from when it was left up to each individual lab. Do we ask this question of the scientific literature? No, largely because many major journals have already provided robust long term archiving with &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;/LOCKSS backup agreements. Likewise scientific data repositories seem to have settled for indefinite archiving. It seems both reasonable and practical that data archiving can be held to the same standard as the journal article itself. (Sure there are lots of challenging issues to be worked out here, the key is only to leave it in the hands of those already leading the way and not re-invent the wheel).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Integrating Github Project Repos Into The Notebook</title>
	 <link href="/2014/05/07/integrating-github-project-repos-into-the-notebook.html"/>
   <updated>2014-05-07T00:00:00+00:00</updated>
   <id>/05/07/integrating-github-project-repos-into-the-notebook</id>
   <content type="html">&lt;p&gt;For a while now most of my active research is developed through &lt;code&gt;.Rmd&lt;/code&gt; scripts connected to a particular project repository (something I discuss at length in &lt;a href=&quot;http://www.carlboettiger.info/2014/05/05/knitr-workflow-challenges.html&quot;&gt;deep challenges with knitr workflows&lt;/a&gt;). In the &lt;a href=&quot;http://www.carlboettiger.info/2014/05/06/steps-to-a-more-portable-workflow.html&quot;&gt;previous post&lt;/a&gt; I discuss creating a &lt;code&gt;template&lt;/code&gt; package with a more transparent organization of files, such as moving manuscripts from &lt;code&gt;inst/doc/&lt;/code&gt; to simply &lt;code&gt;manuscripts/&lt;/code&gt;. This left these exploratory analysis scripts in &lt;code&gt;inst/examples&lt;/code&gt; in a similarly unintuitive place. Though I like having these scripts as part of the repository (which keeps everything for a project in one place, as it were), like the manuscript they aren’t really part of the R package, particularly as I have gotten better at creating proper unit tests in place of just rerunning dynamic scripts occasionally.&lt;/p&gt;
&lt;p&gt;I’ve also been nagged by the idea of having to always just link to these nice dynamic documents from my lab notebook. Sure Github renders the markdown so that it’s easy enough to see highlighted code and figures etc., but it still makes them seem rather external. Occasionally I would copy the complete &lt;code&gt;.md&lt;/code&gt; file into a notebook post, but this divorced it of it’s original version history and associated &lt;code&gt;.Rmd&lt;/code&gt; source.&lt;/p&gt;
&lt;p&gt;One option would be to move them all directly into my lab notebook, &lt;code&gt;.Rmd&lt;/code&gt; files and all. This would integrate the scripts more nicely than Github’s own rendering, matching the URL and look and feel of my notebook. It would also allow for javascript elements such as MathJax equations, Google Analytics, and Disqus that are not possible when only linking to an &lt;code&gt;.md&lt;/code&gt; file on Github.&lt;/p&gt;
&lt;p&gt;In the recent &lt;a href=&quot;https://github.com/ropensci/docs&quot;&gt;ropensci/docs&lt;/a&gt; project we are exploring a way to have Jekyll automatically compile (potentially with caching) a site that uses &lt;code&gt;.Rmd&lt;/code&gt; posts and deploy to Github all using &lt;code&gt;travis&lt;/code&gt;, but we’re not quite finished and this is potentially fragile particularly with the hundreds of posts in this notebook. Besides this, the notebook structure is rather temporally oriented, (posts are chronological and reflected in my URL structure) while these scripts are largely project-oriented. (Consistent use of categories and tags would ameliorate this).&lt;/p&gt;
&lt;h3 id=&quot;embedding-images-in-.rmd-outputs&quot;&gt;Embedding images in &lt;code&gt;.Rmd&lt;/code&gt; outputs&lt;/h3&gt;
&lt;p&gt;A persistent challenge has been how best to deal with images created by these scripts, some of which I may run many times. By default &lt;code&gt;knitr&lt;/code&gt; creates &lt;code&gt;png&lt;/code&gt; images, which as binary files are ill suited for committing to Github, and which could bloat a repository rather quickly. For a long while I have used custom hooks to push these images to &lt;code&gt;flickr&lt;/code&gt;, (see &lt;a href=&quot;http://flickr.com/cboettig&quot;&gt;flickr.com/cboettig&lt;/a&gt;), inserting the permanent flickr URL into the output markdown.&lt;/p&gt;
&lt;p&gt;Recently Martin Fenner convinced me that &lt;code&gt;svg&lt;/code&gt; files would both render more nicely across a range of devices (being vector graphics), and could be easily committed to Github as they are text-based (XML) files, so that reproducing the same image in repeated runs wouldn’t take up any more space. We can then browse a nice version history of the any particular figure, and this also keeps all the output material together, making it easier to archive permanently (certainly nicer than my old archiving solution using data URIs.). Lastly, &lt;code&gt;svg&lt;/code&gt; is both web native, being a standard namespace of HTML5, and potentially interactive, as the &lt;a href=&quot;http://www.omegahat.org/SVGAnnotation/&quot;&gt;SVGAnnotation&lt;/a&gt; R package illustrates. So, lots of advantages in using &lt;code&gt;svg&lt;/code&gt; graphics.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;svg&lt;/code&gt; files also bring some unique challenges. Unlike when &lt;code&gt;png&lt;/code&gt; files are added to Github, webpages cannot directly link them since Github enforces rendering them as text instead of an image through its choice of HTML header, for security reasons. This means the only way to link to an &lt;code&gt;svg&lt;/code&gt; file on Github is to have that file on a &lt;code&gt;gh-pages&lt;/code&gt; branch, where it can be rendered as a website. A distinct disadvantage of this approach is that while we can link to a specific version of any file on Github, we see only the most recent version rendered on the website created by a &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;On the other hand, having the &lt;code&gt;svg&lt;/code&gt; files on the &lt;code&gt;gh-pages&lt;/code&gt; branch further keeps down the footprint of the project &lt;code&gt;master&lt;/code&gt; branch. This leads rather naturally to the idea that the &lt;code&gt;.Rmd&lt;/code&gt; files and their &lt;code&gt;.md&lt;/code&gt; outputs should also appear on the &lt;code&gt;gh-pages&lt;/code&gt; branch. This removes them from their awkward home in &lt;code&gt;inst/examples/&lt;/code&gt;, and enables all the benefits of custom CSS, custom javascript, and custom URLs that we don’t have on Github’s rendering.&lt;/p&gt;
&lt;p&gt;To provide a consistent look and feel, I merely copied over the &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; from my lab notebook, tweaking them slightly to use the assets already hosted there. I add custom domain name for the all my &lt;code&gt;gh-pages&lt;/code&gt; as a sub-domain, &lt;code&gt;io.carlboettiger.info&lt;/code&gt; &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, and now instead of having script output appear like so:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/7dd8fc444cb9d20d839286eac8068b3099ea9b6a/inst/examples/gaussian-process-basics.md&quot;&gt;nonparametric-bayes/inst/examples/gaussian-process-basics.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have the same page rendered on my &lt;code&gt;io&lt;/code&gt; sub-domain:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&quot;&gt;io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;with its mathjax, disqus, matching css, URL and nav elements.&lt;/p&gt;
&lt;h2 id=&quot;landing-pages&quot;&gt;Landing pages&lt;/h2&gt;
&lt;p&gt;An obvious extension of this approach is to grab a copy of the repository README and rename it &lt;code&gt;index.md&lt;/code&gt; and add a yaml header such that it serves as a landing page for the repository. A few lines of Liquid code can then generate the links to the other output scripts, as in this example:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/&quot;&gt;io.carlboettiger.info/nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;template&quot;&gt;Template&lt;/h2&gt;
&lt;p&gt;I have added a &lt;code&gt;gh-pages&lt;/code&gt; branch with this set up to my new &lt;code&gt;template&lt;/code&gt; repository, with some more &lt;a href=&quot;http://io.carlboettiger.info/template/README&quot;&gt;basic documentation and examples&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;There’s no need to use a different sub-domain than the rest of my website, other than that it would require my notebook be hosted on the &lt;a href=&quot;https://github.com/cboettig/cboettig.github.com&quot;&gt;cboettig.github.com&lt;/a&gt; repo instead of &lt;a href=&quot;https://github.com/cboettig/labnotebook&quot;&gt;labnotebook&lt;/a&gt;. However I prefer keeping my hosting on the repository I already have, and it also seems a bit unorthodox to host all my repositories on my main domain. In particular, it increases the chance for URL collisions if I create a repository with the same name as a page or directory on my website. Having gh-pages on the &lt;code&gt;io&lt;/code&gt; sub-domain feels like just the right amount of separation to me.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Steps To A More Portable Workflow</title>
	 <link href="/2014/05/06/steps-to-a-more-portable-workflow.html"/>
   <updated>2014-05-06T00:00:00+00:00</updated>
   <id>/05/06/steps-to-a-more-portable-workflow</id>
   <content type="html">&lt;p&gt;While I have made &lt;a href=&quot;http://www.carlboettiger.info/2012/05/06/research-workflow.html&quot;&gt;my workflow&lt;/a&gt; for most of my ongoing projects available on Github for some time, this does not mean that it has been particularly easy to follow. Further, as I move from project to project I have slowly improved how I handle projects. For instance, I have since added unit tests (with &lt;code&gt;testthat&lt;/code&gt;) and continuous integration (with &lt;a href=&quot;http://travis-ci.org&quot;&gt;travis-ci&lt;/a&gt;) to my repositories, and my handling of manuscripts has gotten more automated, with richer latex templates, yaml metadata, and simpler and more powerful makefiles.&lt;/p&gt;
&lt;p&gt;Though I have typically used my most recent project as a template for my next one (not so trivial as I work on several at a time), I realized it would make sense to just maintain a general template repo with all the latest goodies. I have now launched my &lt;a href=&quot;https://github.com/cboettig/template&quot;&gt;template&lt;/a&gt; on Github.&lt;/p&gt;
&lt;p&gt;I toyed with the idea of just treating the manuscript as a standard vignette, but this would make &lt;code&gt;pandoc&lt;/code&gt; an external dependency for the package, putting an unecessary burden on &lt;code&gt;travis&lt;/code&gt; and users. I settled on creating a &lt;code&gt;manuscripts&lt;/code&gt; directory in the project root folder as the most semantically obvious place. This is added to &lt;code&gt;.Rbuildignore&lt;/code&gt; as it doesn’t fit the standard structure of an R package, but since it is not a vignette and cannot be built with the package dependencies anyhow, this seems to make sense to me.&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The manuscript itslef is written in &lt;code&gt;.Rmd&lt;/code&gt;, with a &lt;code&gt;yaml&lt;/code&gt; header for the usual metadata of authors, affiliations, and so forth. Pandoc’s recent support for &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/manuscript.Rmd#L1-27&quot;&gt;yaml metadata&lt;/a&gt; makes it much easier to use &lt;code&gt;.Rmd&lt;/code&gt; with a LaTeX template, making &lt;code&gt;.Rnw&lt;/code&gt; rather unnecessary. &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/components/elsarticle.latex&quot;&gt;My template&lt;/a&gt; includes a custom &lt;code&gt;LaTeX&lt;/code&gt; template that includes pandoc’s macros for inserting authors, affiliations, and so forth in the correct LaTeX elements, though pandoc’s &lt;a href=&quot;https://github.com/jgm/pandoc-templates/blob/master/default.latex&quot;&gt;default template&lt;/a&gt; is rather good and already has macros for most things in place (meaning you can merely declare the layout or font in the yaml header and magically see the tex interpret it).&lt;/p&gt;
&lt;p&gt;I have tried to keep the &lt;code&gt;manuscripts&lt;/code&gt; directory relatively clean, placing &lt;code&gt;csl&lt;/code&gt;, &lt;code&gt;bibtex&lt;/code&gt;, &lt;code&gt;figures/&lt;/code&gt;, &lt;code&gt;cache/&lt;/code&gt; and other such files in a &lt;code&gt;components/&lt;/code&gt; sub-directory. I have also tried to keep the &lt;code&gt;Makefile&lt;/code&gt; as platform-independent as possible by having it call little Rscripts (also housed in &lt;code&gt;components/&lt;/code&gt;) rather than commandline utilities like &lt;code&gt;sed -i&lt;/code&gt; and &lt;code&gt;wget&lt;/code&gt; that may not behave the same way on all platforms.&lt;/p&gt;
&lt;p&gt;Lastly, Ryan Batts recently convinced me that providing binary cache files of results was an important way to allow a reader to quickly engage in exploring an analysis without having to first let potentially long-running code execute. &lt;code&gt;knitr&lt;/code&gt; provides an excellent way to create and manage this caching on a code chunk by chunk level, which is also crucial when editing a dynamic document with intensive code (no one wants to rerun your MCMC just to rebuild the pdf). Since git/Github seems like a poor option for distributing binaries, I have for the moment just archived the cache on a (university) web server and added a Make/Rscript line to that can restore it from that location. Upon publication this cache could be permanently archived (along with plain text tables of the graphs) and then installed from that archive instead.&lt;/p&gt;
&lt;p&gt;I have also added a separate &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/README.md&quot;&gt;README&lt;/a&gt; in the manuscripts directory to provide some guidance to a user seeking to build the manuscript.&lt;/p&gt;
&lt;p&gt;Examples of an active projects currently using this layout for manuscripts, etc include &lt;a href=&quot;https://github.com/ropensci/RNeXML&quot;&gt;RNeXML&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/&quot;&gt;nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Perhaps I should not have the manuscript on the master branch at all, but putting it on another branch would defeat the purpose of having it in an obviously-named directory of the repository home page where it is most easy to discover.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Deep challenges to dynamic documentation in daily workflows</title>
	 <link href="/2014/05/05/knitr-workflow-challenges.html"/>
   <updated>2014-05-05T00:00:00+00:00</updated>
   <id>/05/05/knitr-workflow-challenges</id>
   <content type="html">&lt;p&gt;We often discuss dynamic documents such as &lt;code&gt;Sweave&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt; in reference to final products such as publications or software package vignettes. In this case, all the elements involved are already fixed: external functions, code, text, and so forth. The dynamic documentation engine is really just a tool to combine them (knit them together). Using dynamic documentation on a day-to-day basis on ongoing research presents a compelling opportunity but a rather more complex challenge as well. The code base grows, some of it gets turned into external custom functions where it continues to change. One analysis script branches into multiple that vary this or that. The text and figures are likewise subject to the same revision as the code, expanding and contracting, or being removed or shunted off into an appendix.&lt;/p&gt;
&lt;p&gt;Structuring a dynamic document when all the parts are morphing and moving is one of the major opportunities for the dynamic approach, but also the most challenging. Here I describe some of those challenges along with various tricks I have adopted to deal with them, mostly in hopes that someone with a better strategy might be inspired to fill me in.&lt;/p&gt;
&lt;h2 id=&quot;the-old-way&quot;&gt;The old way&lt;/h2&gt;
&lt;p&gt;For a while now I have been using the &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; dynamic documentation/reproducible research software for my project workflow. Most discussion of dynamic documentation focuses on ‘finished’ products such as journal articles or reports. Over the past year, I have found the dynamic documentation framework to be particularly useful as I develop ideas, and remarkably more challenging to then integrate into a final paper in a way that really takes advantage of its features. I explain both in some detail here.&lt;/p&gt;
&lt;p&gt;My former workflow followed a pattern no doubt familiar to many:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash away in an R terminal, paste useful bits into an R script…&lt;/li&gt;
&lt;li&gt;Write manuscript separately, pasting in figures, tables, and in-line values returned from R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This doesn’t leave much of a record of what I did or why, which is particularly frustrating when some discussion reminds me of an earlier idea.&lt;/p&gt;
&lt;h2 id=&quot;dynamic-docs-.rmd-files&quot;&gt;Dynamic docs: &lt;code&gt;.Rmd&lt;/code&gt; files&lt;/h2&gt;
&lt;p&gt;When I begin a new project, I now start off writing a &lt;code&gt;.Rmd&lt;/code&gt; file, intermixing notes to myself and code chunks. Chunks break up the code into conceptual elements, markdown gives me a more expressive way to write notes than comment lines do. Output figures, tables, and in-line values inserted. So far so good. I version manage this creature in git/Github. Great, now I have a trackable history of what is going on, and all is well:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Document my thinking and code as I go along on a single file scratch-pad&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Version-stamped history of what I put in and what I got out on each step of the way&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rich markup with equations, figures, tables, embedded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching of script chunks, allowing me to tweak and rerun an analysis without having to execute the whole script. While we can of course duplicate that behavior with careful save and load commands in a script, in knitr this comes for free.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;limitations-to-.rmd-alone&quot;&gt;Limitations to .Rmd alone&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;As I go along, the &lt;code&gt;.Rmd&lt;/code&gt; files starts getting too big and cluttered to easily follow the big picture of what I’m trying to do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Before long, my investigation branches. Having followed one &lt;code&gt;.Rmd&lt;/code&gt; script to some interesting results, I start a new &lt;code&gt;.Rmd&lt;/code&gt; script representing a new line of investigation. This new direction will nevertheless want to re-use large amounts of code from the first file.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;a-solution-the-r-package-research-compendium-approach&quot;&gt;A solution? The R package “research compendium” approach&lt;/h2&gt;
&lt;p&gt;I start abstracting tasks performed in chunks into functions, so I can re-use these things elsewhere, loop over them, and document them carefully somewhere I can reference that won’t be in the way of what I’m thinking. I start to move these functions into &lt;code&gt;R/&lt;/code&gt; directory of an R package structure, documenting with &lt;code&gt;Roxygen&lt;/code&gt;. I write unit tests for these functions (in &lt;code&gt;inst/tests&lt;/code&gt;) to have quick tests to check their sanity without running my big scripts (recent habit). The package structure helps me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reuse the same code between two analyses without copy-paste or getting our of sync&lt;/li&gt;
&lt;li&gt;Document complicated algorithms outside of my working scripts&lt;/li&gt;
&lt;li&gt;Test complicated algorithms outside of my working scripts (&lt;code&gt;devtools::check&lt;/code&gt; and/or unit tests)&lt;/li&gt;
&lt;li&gt;Manage dependencies on other packages (DESCRIPTION, NAMESPACE), including other projects of mine&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This runs into trouble in several ways.&lt;/p&gt;
&lt;h2 id=&quot;problem-1-reuse-of-code-chunks&quot;&gt;Problem 1: Reuse of code chunks&lt;/h2&gt;
&lt;p&gt;What to do with code I want to reuse across blocks but do not want to write as a function, document, or test?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Perhaps this category of problem doesn’t exist, except in my laziness.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This situation arises all the time, usually through the following mechanism: almost any script performs several steps that are best represented as chunks calling different functions, such as &lt;code&gt;load_data&lt;/code&gt;, &lt;code&gt;set_fixed_parameters&lt;/code&gt;, &lt;code&gt;fit_model&lt;/code&gt;, &lt;code&gt;plot_fits&lt;/code&gt;, etc. I then want to re-run almost the same script, but with a slightly different configuration (such as a different data set or extra iterations in the fixed parameters). For just a few such cases, it doesn’t make sense to write these into a single function,&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; instead, I copy this script to a new file and make the changes there.&lt;/p&gt;
&lt;p&gt;This is great until I want to change something in about the way both scripts behave that cannot be handled just by changing the &lt;code&gt;R/&lt;/code&gt; functions they share. Plotting options are a good example of this (I tend to avoid wrapping &lt;code&gt;ggplot&lt;/code&gt; calls as separate functions, as it seems to obfuscate what is otherwise a rather semantic and widely recognized, if sometimes verbose, function call).&lt;/p&gt;
&lt;p&gt;I have explored using &lt;code&gt;knitr&lt;/code&gt;’s support for external chunk inclusion, which allows me to maintain a single R script with all commonly used chunks, and then import these chunks into multiple &lt;code&gt;.Rmd&lt;/code&gt; files. An example of this can be seen in my &lt;code&gt;nonparametric-bayes&lt;/code&gt; repo, where several files (in the same directory) draw most of their code from &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/9232dfd814c40e3c48c5a837be110a870d8639da/inst/examples/BUGS/external-chunks.R&quot;&gt;external-chunks.R&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;problem-2-package-level-reproducibility&quot;&gt;Problem 2: package-level reproducibility&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Minor/relatively easy to fix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separate files can frustrate reproducibility of a given commit. As I change the functions in &lt;code&gt;R/&lt;/code&gt;, the &lt;code&gt;.Rmd&lt;/code&gt; file can give different results despite being unchanged. (Or fail to reflect changes because it is caching chunks and does not recognize the function definitions have changed underneath it). Git provides a solution to this: since the &lt;code&gt;.Rmd&lt;/code&gt; file lives in the same git repository (&lt;code&gt;inst/examples&lt;/code&gt;) as the package, I can make sure the whole repository matches the hash of the &lt;code&gt;.Rmd&lt;/code&gt; file: &lt;code&gt;install_github(&amp;quot;packagename&amp;quot;, &amp;quot;cboettig&amp;quot;, &amp;quot;hash&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This solution is not fail-safe: the installed version, the potentially uncommitted (but possibly installed) version of the R functions in the working directory, and the R functions present at the commit of the &lt;code&gt;.Rmd&lt;/code&gt; file (and thus matching the hash) could all be different. If we commit and install before every &lt;code&gt;knit&lt;/code&gt;, we can avoid these potential errors (at the cost of some computational overhead), restoring reproducibility to the chain.&lt;/p&gt;
&lt;h2 id=&quot;problem-3-synthesizing-results-into-a-manuscript&quot;&gt;Problem 3: Synthesizing results into a manuscript&lt;/h2&gt;
&lt;p&gt;In some ways this is the easiest part, since the code-base is relatively static and it is just a matter of selecting which results and figures to include and what code is necessary to generate it. A few organizational challenges remain:&lt;/p&gt;
&lt;p&gt;While we generally want &lt;code&gt;knitr&lt;/code&gt; code chunks for the figures and tables that will appear, we usually aren’t interested in displaying much, if any, of the actual code in the document text (unlike the examples until this point, where this was a major advantage of the knitr approach). In principle, this is as simple as setting &lt;code&gt;echo=FALSE&lt;/code&gt; in the global chunk options. In practice, it means there is little benefit to having the chunks interwoven in the document. What I tend to want is having all the chunks run at the beginning, such that any variables or results can easily be added (and their appearance tweaked by editing the code) as figure chunks or in-line expressions. The only purpose of maintaining chunks instead of a simple script is the piecewise caching of chunk dependencies which can help debugging.&lt;/p&gt;
&lt;p&gt;Since displaying the code is suppressed, we are then left with the somewhat ironic challenge of how best to present code as a supplement. One option is simply to point to the source &lt;code&gt;.Rmd&lt;/code&gt;, another is to use the &lt;code&gt;tangle()&lt;/code&gt; option to extract all the code as a separate &lt;code&gt;.R&lt;/code&gt; file. In either case, the user must also identify the correct version of the R package itself for the external &lt;code&gt;R/&lt;/code&gt; functions.&lt;/p&gt;
&lt;h2 id=&quot;problem-4-branching-into-other-projects&quot;&gt;Problem 4: Branching into other projects&lt;/h2&gt;
&lt;p&gt;Things get most complicated when projects begin to branch into other projects. In an ideal world this is simple: a new idea can be explored on a new branch of the version control system and merged back in when necessary, and an entirely new project can be built as a new R package in a different repo that depends on the existing project. After several examples of each, I have learned that it is not so simple. Despite the nice tools, I’ve learned I still need to be careful in managing my workflows in order to leave behind material that is understandable, reproducible, and reflects clear provenance. So far, I’ve learned this the hard way. I use this last section of the post to reflect on two of my own examples, as writing this helps me work through what I should have done differently.&lt;/p&gt;
&lt;h3 id=&quot;example-warning-signals-project&quot;&gt;example: warning-signals project&lt;/h3&gt;
&lt;p&gt;For instance, my work on early warning signals dates back to the start of my &lt;a href=&quot;http://openwetware.org/wiki/User:Carl_Boettiger/Notebook/Stochastic_Population_Dynamics/2010/02/09&quot;&gt;open notebook on openwetware&lt;/a&gt;, when my code lived on a Google code page which seems to have disappeared. (At the time it was part of my ‘stochastic population dynamics’ project). When I moved to Github, this project got it’s own repository, &lt;a href=&quot;https://github.com/cboettig/warningsignals&quot;&gt;warningsignals&lt;/a&gt;, though after a major re-factorization of the code I moved to a new repository, &lt;a href=&quot;https://github.com/cboettig/earlywarning&quot;&gt;earlywarning&lt;/a&gt;. Okay, so far that was due to me not really knowing what I was doing.&lt;/p&gt;
&lt;p&gt;My first paper on this topic was based on the master branch of that repository, which still contains the code required. When one of the R dependencies was moved from CRAN I was able to update the codebase to reflect the replacement package (see issue &lt;a href=&quot;https://github.com/cboettig/earlywarning/issues/10&quot;&gt;#10&lt;/a&gt;). Even before that paper appeared I started exploring other issues on different &lt;a href=&quot;https://github.com/cboettig/earlywarning/network&quot;&gt;branches&lt;/a&gt;, with the &lt;code&gt;prosecutor&lt;/code&gt; branch eventually becoming it’s own paper, and then it’s &lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/&quot;&gt;own repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That paper sparked a comment letter in response to it, and the analysis involved in my reply piece was just developed on the same master branch of the prosecutor-fallacy repository. This leaves me with a total of three repositories across four branches, with one repo that corresponds more-or-less directly to a paper, one to two papers, and one to no papers.&lt;/p&gt;
&lt;p&gt;All four branches have diverged and unmerge-able code. Despite sharing and reusing functions across these projects, I often found it better to simply change the function on the new branch or new repo as I desired for the new work. These changes could not be easily merged back as they broke the original function calls of the earlier work.&lt;/p&gt;
&lt;p&gt;Hindsight being 20-20, it would have been preferable that I had maintained one repository, perhaps developed each paper on a different branch and clearly tagged the commit corresponding to the submission of each publication. Ideally these could be merged back where possible to a master branch. Tagged commits provide a more natural solution than unmerged branches to deal with changes to the package that would break methods from earlier publications.&lt;/p&gt;
&lt;h3 id=&quot;example-optimal-control-projects&quot;&gt;example: optimal control projects&lt;/h3&gt;
&lt;p&gt;A different line of research began through a NIMBioS working group called “Pretty Darn Good Control”, beginning it’s digital life in my &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt; repository. Working in different break-out groups as well as further investigation on my own soon created several different projects. Some of these have continue running towards publication, others terminating in dead ends, and still others becoming completely separate lines of work. Later work I have done in optimal control, such &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes&quot;&gt;nonparametric-bayes&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty&quot;&gt;multiple_uncertainty&lt;/a&gt; depend on this package for certain basic functions, though both also contain their own diverged versions of functions that first appeared in &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because the topics are rather different and the shared code footprint is quite small, separate repositories probably makes more sense here. Still, managing the code dependencies in separate repositories requires extra care, as checking out the right version of the focal repository does not guarantee that one will also have the right version of the [pdg_control] repository. Ideally I should note the hash of [pdg_control] on which I depend, and preferably install that package at that hash (easy enough thanks to &lt;code&gt;devtools&lt;/code&gt;), since depending on a separate project that is also still changing can be troublesome. Alternatively it might make more sense to just duplicate the original code and remove this potentially frail dependency. After all, documenting the provenance need not rely on the dependency, and it is more natural to think of these separate repos as divergent forks.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;If I have a lot of different configurations, it may make sense to wrap up all these steps into a single function that takes input data and/or parameters as it’s argument and outputs a data frame with the results and inputs.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>why I sign my reviews</title>
	 <link href="/2014/05/04/why-I-sign-my-reviews.html"/>
   <updated>2014-05-04T00:00:00+00:00</updated>
   <id>/05/04/why-I-sign-my-reviews</id>
   <content type="html">&lt;p&gt;For the past four years I have made an effort to sign all my reviews (which I try to keep to about one a month). It isn’t because I believe in radical openness or something crazy like that. Its really just my self interest involved – at least mostly. Writing a review is an incredibly time consuming, and largely thankless task. Supposedly anonymous peer review is supposed to protect the reviewer, particularly the scenario of the less established scientist critiquing the work of the more established. I am sure it occasionally serves that purpose. On the other hand, that very scenario can be the &lt;em&gt;most&lt;/em&gt; profitable time to sign a review. Really, when are you more likely to get an esteemed colleague to closely read your every argument than when you’re holding up their publication?&lt;/p&gt;
&lt;p&gt;While the possibility of a vindictive and powerful author sounds daunting, but rather inconsistent with my impression of most scientists, who are more apt to be impressed by an intelligent even if flawed critique than by simple praise. I find it hardest to sign a review that I have found very little constructive criticism to offer, though after a decade of being trained to critique science one can always find something. (Of course signing can be hard on the occasional terrible paper for which it is hard to offer much constructive criticism, but fortunately that has been very rare). Both authors and other reviewers (who are sometimes sent the other reviews, a practice I find very educational as a reviewer) have on occasion commented or complemented me on reviews or acknowledged me in the papers, suggesting that the practice does indeed provide for some simple recognition. At times, it may sow seeds for future collaboration.&lt;/p&gt;
&lt;p&gt;Signing my reviews has on occasion given the author a chance to follow up with me directly. While I’m not certain about journal policies in this regard, I suspect we can assume that we’re all adults capable of civil discussion. In any event, a phone call or even a few back-and-forth emails can be immensely more efficient in allowing an author to clarify elements that I have sometimes misunderstood or been unable to follow from the text, as well as making it easier to communicate my difficulties with the paper. In my experience this has resulted in both a faster and more satisfactory resolution to issues that have led to see some papers published more quickly and without as many tedious multiple rounds of revision. Given that many competitive journals simply cut off papers that might otherwise be successful with a bit more dialog between reviewer and author, because multiple “Revise and resubmits” put too much demand on editors, this seems like a desirable outcome for all involved. I’m not suggesting that such direct dialog is always desirable, but that no doubt many of us have been in the position in which a little dialog might have resolved issues more satisfactorily.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Software Sustainability Issues In R</title>
	 <link href="/2014/03/20/software-sustainability-issues-in-R.html"/>
   <updated>2014-03-20T00:00:00+00:00</updated>
   <id>/03/20/software-sustainability-issues-in-R</id>
   <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Editorial note:&lt;/strong&gt; The following is slightly edited text from my post to R-devel discussing this issue, which I first drafted here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There seems to be some question of how frequently changes to software packages result in irreproducible results. I am sure that research using functions like &lt;code&gt;glm&lt;/code&gt; and other functions that are shipped with base R are quite reliable; and after all they already benefit from being versioned with R releases (as Jeroen has argued).&lt;/p&gt;
&lt;p&gt;In my field of ecology and evolution, the situation is quite different. Packages are frequently developed by scientists without any background in programming and become widely used, such as &lt;a href=&quot;http://cran.r-project.org/web/packages/geiger/&quot;&gt;geiger&lt;/a&gt;, with 463 papers citing it and probably many more using it that do not cite it (both because it is sometimes used only as a dependency of another package or just because our community isn’t great at citing packages). The package has changed substantially over the time it has been on CRAN and many functions that would once run based on older versions could no longer run on newer ones. It’s dependencies, notably the phylogenetics package ape, has changed continually over that interval with both bug fixes and substantial changes to the basic data structure. The ape package has 1,276 citations (again a lower bound). I suspect that correctly identifying the right version of the software used in any of these thousands of papers would prove difficult and for a large fraction the results would simply not execute successfully. It would be much harder to track down cases where the bug fixes would have any impact on the result. I have certainly seen both problems in the hundreds of Sweave/knitr files I have produced over the years that use these packages.&lt;/p&gt;
&lt;p&gt;Even work that simply relies on a package that has been archived becomes a substantial challenge to reproducibility by other scientists even when an expert familiar with the packages (e.g. the original author) would not have a problem, as the informatics team at the Evolutionary Synthesis center recently concluded in an exercise trying to reproduce several papers including my own that used a package that had been archived (odesolve, whose replacement, deSolve, does not use quite the same function call for the same &lt;code&gt;lsoda&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;New methods are being published all the time, and I think it is excellent that in ecology and evolution it is increasingly standard to publish R packages implementing those methods, as a scan of any table of contents in “methods in Ecology and Evolution”, for instance, will quickly show. But unlike &lt;code&gt;glm&lt;/code&gt;, these methods have a long way to go before they are fully tested and debugged, and reproducing any work based on them requires a close eye to the versions (particularly when unit tests and even detailed changelogs are not common). The methods are invariably built by “user-developers”, researchers developing the code for their own needs, and thus these packages can themselves fall afoul of changes as they depend and build upon work of other nascent ecology and evolution packages.&lt;/p&gt;
&lt;p&gt;Detailed reproducibility studies of published work in this area are still hard to come by, not least because the actual code used by the researchers is seldom published (other than when it is published as it’s own R package). But incompatibilities between successive versions of the 100s of packages in our domain, along with the interdependencies of those packages might provide some window into the difficulties of computational reproducibility. I suspect changes in these fast-moving packages are far more culprit than differences in compilers and operating systems.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Startup Culture And Platforms For The Academy</title>
	 <link href="/2014/01/23/Startup-Culture-and-Platforms-for-the-Academy.html"/>
   <updated>2014-01-23T00:00:00+00:00</updated>
   <id>/01/23/Startup-Culture-and-Platforms-for-the-Academy</id>
   <content type="html">&lt;h2 id=&quot;scientific-research-moving-beyond-bubbles-to-platforms.&quot;&gt;Scientific Research: Moving beyond bubbles to platforms.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.economist.com/news/special-report/21593580-cheap-and-ubiquitous-building-blocks-digital-products-and-services-have-caused&quot;&gt;The Economist&lt;/a&gt; contrasts the current start-up culture to that of the dotcom bubble. Their special report argues that while the dotcom bubble was characterized by heavy investment in a single idea with a “Build it and they will come” attitude, the current generation of startups is characterized by tinkering and rapid experimentation. They argue that this is possible thanks to the cheaply available &lt;em&gt;“platforms”&lt;/em&gt; of open source software, cloud computing, APIs, and social media dissemination. And they argue that this time, it is no bubble. That it will eat the world. That these lessons apply far beyond tech startups, to businesses in general and even governments. The special report is well worth reading in full.&lt;/p&gt;
&lt;h2 id=&quot;so-how-about-science&quot;&gt;So how about science?&lt;/h2&gt;
&lt;p&gt;After all, experimentation is what science is all about. Yet the platforms that run that world are largely absent from scientific research. Research looks more like the dotcom model, where each research team must make an immense investment in infrastructure and data gathering up front. Must hire expertise in-house for each step of the process, from experimental design to data collection to analysis to writing. Such large teams resemble more the companies of previous decades with their own development, marketing, and research departments, then the agile start-ups currently building the future. The resulting products are monolithic, “build it and they will come”.&lt;/p&gt;
&lt;p&gt;I believe that this is the cornerstone of the open science and reproducible research movement.&lt;/p&gt;
&lt;h2 id=&quot;update-2014-07-24&quot;&gt;Update (2014-07-24)&lt;/h2&gt;
&lt;p&gt;This seems to be exactly what Martin Fenner is talking about in “&lt;a href=&quot;http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/&quot;&gt;Roads, not Stagecoaches&lt;/a&gt;” – the value of transformative infrastructure over individual software contributions.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
