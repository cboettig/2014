<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Lab Notebook</title>
 <link href="/2014/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2015-02-25T23:05:31+00:00</updated>
 <id>http://www.carlboettiger.info/2014</id>
 <author>
   <name>Carl Boettiger</name>
   <email>cboettig@gmail.com</email>
 </author>

 
 <entry>
   <title>Rrhack Notes</title>
	 <link href="/2014/12/11/rrhack-notes.html"/>
   <updated>2014-12-11T00:00:00+00:00</updated>
   <id>/12/11/rrhack-notes</id>
   <content type="html">&lt;p&gt;Misc notes from #rrhack. Much more detail under our &lt;a href=&quot;https://github.com/Reproducible-Science-Curriculum&quot;&gt;Github organization&lt;/a&gt;, including the issues tracker &amp;amp; wiki for the meeting and the start of the repositories for the various teaching modules.&lt;/p&gt;
&lt;p&gt;Also see rrhack slack forum and twitter hashtag.&lt;/p&gt;
&lt;h2 id=&quot;key-questions&quot;&gt;Key questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Audience and motivation. Reproducibility isn’t a goal, it’s a means to an ends. Accelerating science is the goal. Motivation should be to accelerate &amp;amp; scale your own science.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reproducible by whom? Starting point: expert in domain, expert in language can reproduce without the need to rediscover / recreate? Better: agonstic to language?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Complexity. How frequently does routine, non-computational research involve writing over 1000 lines of code? Can 1000 lines of code be managed successfully without a software development approach?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;knitr &amp;amp; scaling issues&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The download file problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;mine-on-using-docker&quot;&gt;Mine on using Docker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Docker installed on cluster&lt;/li&gt;
&lt;li&gt;Students use RStudio export function to download &amp;amp; submit (into a standard course management platform). import data via direct calls from R (e.g. &lt;code&gt;download.file()&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;jenny-bryan-on-on-using-github-to-teach&quot;&gt;Jenny Bryan on on using Github to teach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Free organization account (gold) with 50 private repos&lt;/li&gt;
&lt;li&gt;Prof / TA team has write access&lt;/li&gt;
&lt;li&gt;Each student needs to be their own team with write access to own repo&lt;/li&gt;
&lt;li&gt;Student team has read access to everything, so they can see each-others work&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Homework submission: open an issue in their repo, tag the owners group, include the hash.&lt;/p&gt;
&lt;p&gt;Need programmatic control of Github to set this up.&lt;br /&gt; Need alterego to test that this works&lt;/p&gt;
&lt;p&gt;Users avoid command line for most work to start, just using Github interface and RStudio interface.&lt;/p&gt;
&lt;p&gt;Not happy with Windows version of Github’s git client (hard to connect to RStudio). Not a problem with the mac.&lt;/p&gt;
&lt;h2 id=&quot;dockerized-rstudio-as-pointy-clicky-application&quot;&gt;Dockerized RStudio as pointy-clicky application?&lt;/h2&gt;
&lt;p&gt;From Jenny: Desktop launch for docker. from Dan: rewrap boot2docker to launch RStudio (hadleyverse) with a click. (installs virtualbox,docker, &amp;amp; boot2docker first if necessary. 129 MB. Not self contained though?).&lt;/p&gt;
&lt;h2 id=&quot;new-tricks&quot;&gt;New tricks&lt;/h2&gt;
&lt;p&gt;Working with multiple branches with different content (e.g. gh-pages): Add a &lt;code&gt;web/&lt;/code&gt; subdirectory to repo and .gitignore, check out the gh-pages branch there. Add indicator of branch to user’s prompt.&lt;/p&gt;
&lt;p&gt;Using the wiki pages: see [richfitz/ghwiki]. Rather than publish &lt;code&gt;.Rmd&lt;/code&gt; files&lt;/p&gt;
&lt;p&gt;Organization: just stick with package compendium format?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Self Destroy Droplet On Completion</title>
	 <link href="/2014/12/07/self-destroy-droplet-on-completion.html"/>
   <updated>2014-12-07T00:00:00+00:00</updated>
   <id>/12/07/self-destroy-droplet-on-completion</id>
   <content type="html">&lt;p&gt;Scott’s analogsea package provides a great way to script commands for cloud instances on the digitalocean platform. for instance, we can use analogsea to automatically start an instance of the desired size, submit a computationally intensive task, and then terminate the instance when the task completes successfully. This last step is particularly convenient since it makes it easier to use a very powerful (and thus expensive) instance for a short time, knowing it will terminate and avoid extra charges while idle.&lt;/p&gt;
&lt;p&gt;To avoid first having to install the necessary software environment on the newly created digitalocean instance, we will simply pull a docker image that has already been provisioned. This is particularly useful in both keeping what we information we need to send to the cloud machine consise (no need to list all dependencies) and fast (particularly in the case of installing any packages from source, such as R packages from CRAN. Thee complete installation process to generate the image we use here can take over an hour itself).&lt;/p&gt;
&lt;p&gt;The analogsea package provides nice functions for working with docker as well, as we will illustrate here.&lt;/p&gt;
&lt;p&gt;First, we can define a custom little function that will pull a given Github repo, run the script specified, and push the results back up.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;task &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(REPO, PATH, SCRIPT, &lt;span class=&quot;dt&quot;&gt;USER =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;Sys.getenv&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;USER&amp;quot;&lt;/span&gt;), 
                 &lt;span class=&quot;dt&quot;&gt;GH_TOKEN =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;Sys.getenv&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;GH_TOKEN&amp;quot;&lt;/span&gt;), 
                 &lt;span class=&quot;dt&quot;&gt;EMAIL =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(USER, &lt;span class=&quot;st&quot;&gt;&amp;quot;@&amp;quot;&lt;/span&gt;, USER, &lt;span class=&quot;st&quot;&gt;&amp;quot;.com&amp;quot;&lt;/span&gt;),
                 &lt;span class=&quot;dt&quot;&gt;IMG =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;rocker/hadleyverse&amp;quot;&lt;/span&gt;){
  &lt;span class=&quot;kw&quot;&gt;paste&lt;/span&gt;(
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;-it &amp;quot;&lt;/span&gt;, IMG, &lt;span class=&quot;st&quot;&gt;&amp;quot; bash -c &lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\&amp;quot;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;git config --global user.name &amp;quot;&lt;/span&gt;, USER),
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;git config --global user.email &amp;quot;&lt;/span&gt;, EMAIL),
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;git clone &amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;https://&amp;quot;&lt;/span&gt;, USER, &lt;span class=&quot;st&quot;&gt;&amp;quot;:&amp;quot;&lt;/span&gt;, GH_TOKEN, &lt;span class=&quot;st&quot;&gt;&amp;quot;@github.com/&amp;quot;&lt;/span&gt;, USER, &lt;span class=&quot;st&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;, REPO, &lt;span class=&quot;st&quot;&gt;&amp;quot;.git&amp;quot;&lt;/span&gt;),
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;cd &amp;quot;&lt;/span&gt;, REPO),
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;cd &amp;quot;&lt;/span&gt;, PATH),
  &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Rscript &amp;quot;&lt;/span&gt;, SCRIPT),
  &lt;span class=&quot;st&quot;&gt;&amp;quot;git add -A&amp;quot;&lt;/span&gt;,
  &lt;span class=&quot;st&quot;&gt;&amp;quot;git commit -a -m &amp;#39;runs from droplet&amp;#39;&amp;quot;&lt;/span&gt;,
  &lt;span class=&quot;st&quot;&gt;&amp;quot;git push origin master&amp;quot;&lt;/span&gt;,
  &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\&amp;quot;&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;,
  &lt;span class=&quot;dt&quot;&gt;sep=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;; &amp;quot;&lt;/span&gt;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This could probably be made a bit more elegant, but the idea is simple. Note that we will clone over https, assuming a Github authentication token is available in the environment (e.g. in &lt;code&gt;.Rprofile&lt;/code&gt;) as &lt;code&gt;GH_TOKEN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To use this function to run the script &lt;code&gt;knit.R&lt;/code&gt; in the &lt;code&gt;inst/examples&lt;/code&gt; directory of my &lt;code&gt;template&lt;/code&gt; repo, I do:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;tsk &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;task&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;template&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;inst/examples&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;knit.R&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;IMG=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;cboettig/strata&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which returns the commands we’ll need to run as a string. In this case I have set a custom docker image &lt;code&gt;cboettig/strata&lt;/code&gt; that contains my standard development environment in use on my laptop, strata.&lt;/p&gt;
&lt;p&gt;If we have Docker installed on the local system, we can verify this script works locally first:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;system&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;paste&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;docker run --rm&amp;quot;&lt;/span&gt;, tsk))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready for the analogsea part to submit this job to a digitalocean machine which it will create and destroy on the fly. Note that this assumes we have a digitalocean account and have saved a personal authentication token to our environment as &lt;code&gt;DO_PAT&lt;/code&gt; (otherwise analogsea will simply prompt us to autheticate manually in the browser). This also requires we have an ssh key added to our account already (at least at this time).&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(analogsea)
&lt;span class=&quot;kw&quot;&gt;docklet_create&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;size=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;512mb&amp;#39;&lt;/span&gt;) %&amp;gt;%
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docklet_run&lt;/span&gt;(tsk) %&amp;gt;%
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;droplet_delete&lt;/span&gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;analogsea will first create the droplet of the desired size (analogsea refers to digitalocean droplets which have Docker software installed as “docklets”), then run our command and destroy the droplet.&lt;/p&gt;
&lt;p&gt;Note that the functions will only continue to the next step if the previous is successful. Consequently, if the script fails for some reason, the instance will perist and we can attempt to debug if we so choose. If we want the instance to be destroyed whether the script succeeds or fails, we can simply drop the last &lt;code&gt;%&amp;gt;%&lt;/code&gt; pipe and the destroy command will still run. (Otherwise some error handling would be reuired aroun the &lt;code&gt;docklet_run&lt;/code&gt; code tomake sure it continues.&lt;/p&gt;
&lt;p&gt;Sometimes the droplet login will fail due to having had a previous digitalocean instance with the same ip (causing ssh to warn that the host identity has changed) or to allow the token to be approved. In this case, it may help to create the the droplet in a separate step, ssh into the ip returned vy &lt;code&gt;droplets()&lt;/code&gt; manually outside of R, and then return to R to launch the task:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;droplets&lt;/span&gt;()
d[[&lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;]] %&amp;gt;%&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docklet_run&lt;/span&gt;(tsk) %&amp;gt;%
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;droplet_delete&lt;/span&gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This assumes our desired droplet is the second in the list (hence &lt;code&gt;d[[2]]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Of course our R instance needs to persist long enough for the job to complete, so we need to be sure to run this from a machine that will itslef remain up, such as a desktop or even another server.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Lsn Nimble</title>
	 <link href="/2014/12/04/lsn-nimble.html"/>
   <updated>2014-12-04T00:00:00+00:00</updated>
   <id>/12/04/lsn-nimble</id>
   <content type="html">&lt;p&gt;some sample data&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(sde)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(nimble)
&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;123&lt;/span&gt;)
d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt; *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;-x))
s &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;) 
data &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sde.sim&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;X0=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;,&lt;span class=&quot;dt&quot;&gt;drift=&lt;/span&gt;d, &lt;span class=&quot;dt&quot;&gt;sigma=&lt;/span&gt;s, &lt;span class=&quot;dt&quot;&gt;T=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;20&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;N=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;sigma.x not provided, attempting symbolic derivation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(data)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-1-1.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;lsn-version&quot;&gt;LSN version&lt;/h2&gt;
&lt;p&gt;Test case: Set prior for &lt;code&gt;m&lt;/code&gt; &lt;span class=&quot;math&quot;&gt;\(\approx 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;lsn &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;modelCode&lt;/span&gt;({
   theta ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;100.0&lt;/span&gt;)
   sigma_x ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;100.0&lt;/span&gt;)
   sigma_y ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;100.0&lt;/span&gt;)
       m ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(-&lt;span class=&quot;fl&quot;&gt;1e2&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;1e2&lt;/span&gt;)
    x[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
    y[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;) 

  for(i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:(N&lt;span class=&quot;dv&quot;&gt;-1&lt;/span&gt;)){
    mu_x[i] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;x[i] +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;y[i] *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(theta -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;x[i]) 
    x[i&lt;span class=&quot;dv&quot;&gt;+1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dnorm&lt;/span&gt;(mu_x[i], &lt;span class=&quot;dt&quot;&gt;sd =&lt;/span&gt; sigma_x) 
    mu_y[i] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;y[i] +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;m *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;t[i]
    y[i&lt;span class=&quot;dv&quot;&gt;+1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dnorm&lt;/span&gt;(mu_y[i], &lt;span class=&quot;dt&quot;&gt;sd =&lt;/span&gt; sigma_y) 
  }
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Constants in the model definition are the length of the dataset, &lt;span class=&quot;math&quot;&gt;\(N\)&lt;/span&gt; and the time points of the sample. Note we’ve made time explicit, we’ll assume uniform spacing here.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;constants &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;N =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(data$x), &lt;span class=&quot;dt&quot;&gt;t =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(data$x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initial values for the parameters&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;inits &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;theta =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;m =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sigma_x =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sigma_y =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;y =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;,constants$N))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and here we go as before:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;Rmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nimbleModel&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;code =&lt;/span&gt; lsn, 
                      &lt;span class=&quot;dt&quot;&gt;constants =&lt;/span&gt; constants, 
                      &lt;span class=&quot;dt&quot;&gt;data =&lt;/span&gt; data, 
                      &lt;span class=&quot;dt&quot;&gt;inits =&lt;/span&gt; inits)
Cmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(Rmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;mcmcspec &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;MCMCspec&lt;/span&gt;(Rmodel, &lt;span class=&quot;dt&quot;&gt;print=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;,&lt;span class=&quot;dt&quot;&gt;thin=&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;1e2&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] RW sampler;   targetNode: theta,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
[2] RW sampler;   targetNode: sigma_x,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
[3] RW sampler;   targetNode: sigma_y,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
[4] RW sampler;   targetNode: m,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
[5] RW sampler;   targetNode: y[1],  adaptive: TRUE,  adaptInterval: 200,  scale: 1
[6] conjugate_dnorm sampler;   targetNode: y[2],  dependents_dnorm: x[3], y[3]
...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;Rmcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;buildMCMC&lt;/span&gt;(mcmcspec)
Cmcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(Rmcmc, &lt;span class=&quot;dt&quot;&gt;project =&lt;/span&gt; Cmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;Cmcmc&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e4&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and examine results&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nfVar&lt;/span&gt;(Cmcmc, &lt;span class=&quot;st&quot;&gt;&amp;#39;mvSamples&amp;#39;&lt;/span&gt;)))
&lt;span class=&quot;kw&quot;&gt;dim&lt;/span&gt;(samples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 100 206&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;samples[,&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10.11174&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -1.88765e-05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$sigma_x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.385018&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;m&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;m&amp;#39;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma_x&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma[x]))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma_y&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma[y]))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(theta))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-10-1.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-10-2.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-10-3.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-10-4.svg&quot; /&gt;&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples[, &lt;span class=&quot;st&quot;&gt;&amp;#39;m&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;m&amp;#39;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples[, &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma_x&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma[x]))
&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples[, &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma_y&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma[y]))
&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples[, &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(theta))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-11-1.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-11-2.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-11-3.svg&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-04-lsn-nimble/unnamed-chunk-11-4.svg&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>OU model in Nimble</title>
	 <link href="/2014/12/03/ou-model-in-nimble.html"/>
   <updated>2014-12-03T00:00:00+00:00</updated>
   <id>/12/03/ou-model-in-nimble</id>
   <content type="html">&lt;p&gt;Sanity test with a simple model, Start with some sample data from an OU process:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;sde&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;nimble&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;123&lt;/span&gt;)
d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt; *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;-x))
s &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;) 
data &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sde.sim&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;X0=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;,&lt;span class=&quot;dt&quot;&gt;drift=&lt;/span&gt;d, &lt;span class=&quot;dt&quot;&gt;sigma=&lt;/span&gt;s, &lt;span class=&quot;dt&quot;&gt;T=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;20&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;N=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;sigma.x not provided, attempting symbolic derivation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(data)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-2-1.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Specify this model in Nimble BUGS code&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ou &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;modelCode&lt;/span&gt;({
   theta ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;100.0&lt;/span&gt;)
       r ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;20.0&lt;/span&gt;)
   sigma ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-10&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)
    x[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)

  for(t in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:(N&lt;span class=&quot;dv&quot;&gt;-1&lt;/span&gt;)){
    mu[t] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;x[t] +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;r *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(theta -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;x[t]) 
    x[t&lt;span class=&quot;dv&quot;&gt;+1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dnorm&lt;/span&gt;(mu[t], &lt;span class=&quot;dt&quot;&gt;sd =&lt;/span&gt; sigma) 
  }
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nimble parameters&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;const &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;N =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(data$x))
ou_inits &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;theta =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;r =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sigma =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create, spec, build, &amp;amp; compile&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ou_Rmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nimbleModel&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;code =&lt;/span&gt; ou, &lt;span class=&quot;dt&quot;&gt;constants =&lt;/span&gt; const, &lt;span class=&quot;dt&quot;&gt;data =&lt;/span&gt; data, &lt;span class=&quot;dt&quot;&gt;inits =&lt;/span&gt; ou_inits)
ou_spec &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;MCMCspec&lt;/span&gt;(ou_Rmodel, &lt;span class=&quot;dt&quot;&gt;thin=&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;1e2&lt;/span&gt;)
ou_Rmcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;buildMCMC&lt;/span&gt;(ou_spec)
ou_Cmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(ou_Rmodel)
ou_mcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(ou_Rmcmc, &lt;span class=&quot;dt&quot;&gt;project =&lt;/span&gt; ou_Cmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the MCMC&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ou_mcmc&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e4&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and examine the results&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nfVar&lt;/span&gt;(ou_mcmc, &lt;span class=&quot;st&quot;&gt;&amp;#39;mvSamples&amp;#39;&lt;/span&gt;)))
&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10.47953&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.392594&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;means&lt;/span&gt;(samples$r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in eval(expr, envir, enclos): could not find function &amp;quot;means&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(r))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(theta))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;], samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(r), &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(simga))
&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples[, &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-8-1.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-8-2.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-8-3.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-8-4.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-8-5.png&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;block-sampler&quot;&gt;Block sampler&lt;/h3&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ou_spec$&lt;span class=&quot;kw&quot;&gt;addSampler&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;RW_block&amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;targetNodes=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;,&lt;span class=&quot;st&quot;&gt;&amp;#39;sigma&amp;#39;&lt;/span&gt;,&lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;adaptInterval=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[4] RW_block sampler;   targetNodes: r, sigma, theta,  adaptive: TRUE,  adaptInterval: 100,  scale: 1,  propCov: identity&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ou_Rmcmc2 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;buildMCMC&lt;/span&gt;(ou_spec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;ou_mcmc2 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(ou_Rmcmc2, &lt;span class=&quot;dt&quot;&gt;project=&lt;/span&gt;ou_Rmodel, &lt;span class=&quot;dt&quot;&gt;resetFunctions=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(not clear why we use the old project here; but seems to allow us to inherit from previous settings, e.g. the monitors from &lt;code&gt;mcmcSpec()&lt;/code&gt; initialization)&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ou_mcmc2&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e4&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples2 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nfVar&lt;/span&gt;(ou_mcmc2, &lt;span class=&quot;st&quot;&gt;&amp;#39;mvSamples&amp;#39;&lt;/span&gt;)))
&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples2$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10.46894&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples2[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(r))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples2[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(sigma))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples2[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(theta))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples2[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;], samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;sigma&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(r), &lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(simga))
&lt;span class=&quot;kw&quot;&gt;hist&lt;/span&gt;(samples2[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-13-1.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-13-2.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-13-3.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-13-4.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-ou-model-in-nimble/unnamed-chunk-13-5.png&quot; /&gt;&lt;/p&gt;
&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Nimble Explore</title>
	 <link href="/2014/12/03/nimble-explore.html"/>
   <updated>2014-12-03T00:00:00+00:00</updated>
   <id>/12/03/nimble-explore</id>
   <content type="html">&lt;p&gt;Working through quick-start example in &lt;a href=&quot;http://r-nimble.org/manuals/NimbleUserManual.pdf&quot;&gt;nimble manual&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The manual gives essentially no introduction to what appears to be a classic BUGS example model for stochastically failing pumps.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(nimble)
pumpCode &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;modelCode&lt;/span&gt;({
  for (i in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:N){
    theta[i] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dgamma&lt;/span&gt;(alpha,beta)
    lambda[i] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;theta[i]*t[i]
    x[i] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dpois&lt;/span&gt;(lambda[i])
  }
  alpha ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dexp&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1.0&lt;/span&gt;)
  beta ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dgamma&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;,&lt;span class=&quot;fl&quot;&gt;1.0&lt;/span&gt;)
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pumpConsts &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;N =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;, 
                   &lt;span class=&quot;dt&quot;&gt;t =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;94.3&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;15.7&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;62.9&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;126&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;5.24&lt;/span&gt;,
                         &lt;span class=&quot;fl&quot;&gt;31.4&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;1.05&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;1.05&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;2.1&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;10.5&lt;/span&gt;))
pumpData &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;x =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;14&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;19&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;22&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pumpInits &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;alpha =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, 
                  &lt;span class=&quot;dt&quot;&gt;beta =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;,
                  &lt;span class=&quot;dt&quot;&gt;theta =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;rep&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;, pumpConsts$N))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nimbleModel&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;code =&lt;/span&gt; pumpCode, 
                    &lt;span class=&quot;dt&quot;&gt;name =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;pump&amp;#39;&lt;/span&gt;, 
                    &lt;span class=&quot;dt&quot;&gt;constants =&lt;/span&gt; pumpConsts,
                    &lt;span class=&quot;dt&quot;&gt;data =&lt;/span&gt; pumpData, 
                    &lt;span class=&quot;dt&quot;&gt;inits =&lt;/span&gt; pumpInits)

pump$&lt;span class=&quot;kw&quot;&gt;getNodeNames&lt;/span&gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;alpha&amp;quot;               &amp;quot;beta&amp;quot;                &amp;quot;lifted_d1_over_beta&amp;quot;
 [4] &amp;quot;theta[1]&amp;quot;            &amp;quot;theta[2]&amp;quot;            &amp;quot;theta[3]&amp;quot;           
 [7] &amp;quot;theta[4]&amp;quot;            &amp;quot;theta[5]&amp;quot;            &amp;quot;theta[6]&amp;quot;           
[10] &amp;quot;theta[7]&amp;quot;            &amp;quot;theta[8]&amp;quot;            &amp;quot;theta[9]&amp;quot;           
[13] &amp;quot;theta[10]&amp;quot;           &amp;quot;lambda[1]&amp;quot;           &amp;quot;lambda[2]&amp;quot;          
[16] &amp;quot;lambda[3]&amp;quot;           &amp;quot;lambda[4]&amp;quot;           &amp;quot;lambda[5]&amp;quot;          
[19] &amp;quot;lambda[6]&amp;quot;           &amp;quot;lambda[7]&amp;quot;           &amp;quot;lambda[8]&amp;quot;          
[22] &amp;quot;lambda[9]&amp;quot;           &amp;quot;lambda[10]&amp;quot;          &amp;quot;x[1]&amp;quot;               
[25] &amp;quot;x[2]&amp;quot;                &amp;quot;x[3]&amp;quot;                &amp;quot;x[4]&amp;quot;               
[28] &amp;quot;x[5]&amp;quot;                &amp;quot;x[6]&amp;quot;                &amp;quot;x[7]&amp;quot;               
[31] &amp;quot;x[8]&amp;quot;                &amp;quot;x[9]&amp;quot;                &amp;quot;x[10]&amp;quot;              &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can see &lt;code&gt;theta&lt;/code&gt; has our initial conditions, while &lt;code&gt;lambda&lt;/code&gt; has not yet been initialized:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$lambda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] NA NA NA NA NA NA NA NA NA NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hmm, initially we cannot simulate &lt;code&gt;theta&lt;/code&gt; values though (or rather, we just get NaNs and warnings if we do). At the moment I’m not clear on why, though seems to be due to the lifted node:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;simulate&lt;/span&gt;(pump, &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;)
pump$theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$lifted_d1_over_beta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we calculate the log probability density of the determinstic dependencies of alpha and beta nodes (i.e. the lifted node) then we’re okay:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;) ## This makes the simulations here reproducible
&lt;span class=&quot;kw&quot;&gt;calculate&lt;/span&gt;(pump, pump$&lt;span class=&quot;kw&quot;&gt;getDependencies&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;determOnly =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;simulate&lt;/span&gt;(pump, &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 1.79180692 0.29592523 0.08369014 0.83617765 1.22254365 1.15835525
 [7] 0.99001994 0.30737332 0.09461909 0.15720154&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still need to initialize lambda, e.g. by calculating the probability density on those nodes:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;calculate&lt;/span&gt;(pump, &lt;span class=&quot;st&quot;&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$lambda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 168.9673926   4.6460261   5.2641096 105.3583839   6.4061287
 [6]  36.3723548   1.0395209   0.3227420   0.1987001   1.6506161&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;though not entirely clear to me why the guide prefers to do this as the dependencies of theta (which clearly include lambda, but also other things). Also not clear if these &lt;code&gt;calculate&lt;/code&gt; steps are necessary to proceed with the &lt;code&gt;MCMCspec&lt;/code&gt; and &lt;code&gt;buildMCMC&lt;/code&gt;, or compile steps. Let’s reset the model&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and find out:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nimbleModel&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;code =&lt;/span&gt; pumpCode, 
                    &lt;span class=&quot;dt&quot;&gt;name =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;pump&amp;#39;&lt;/span&gt;, 
                    &lt;span class=&quot;dt&quot;&gt;constants =&lt;/span&gt; pumpConsts,
                    &lt;span class=&quot;dt&quot;&gt;data =&lt;/span&gt; pumpData, 
                    &lt;span class=&quot;dt&quot;&gt;inits =&lt;/span&gt; pumpInits)

pump$theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pump$lambda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] NA NA NA NA NA NA NA NA NA NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good, we’re reset. Now we try:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;Cpump &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(pump)
pumpSpec &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;MCMCspec&lt;/span&gt;(pump)
pumpSpec$&lt;span class=&quot;kw&quot;&gt;addMonitors&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;thin = 1: alpha, beta, theta&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;pumpMCMC &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;buildMCMC&lt;/span&gt;(pumpSpec)
CpumpMCMC &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(pumpMCMC, &lt;span class=&quot;dt&quot;&gt;project =&lt;/span&gt; pump)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;CpumpMCMC&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nfVar&lt;/span&gt;(CpumpMCMC, &lt;span class=&quot;st&quot;&gt;&amp;#39;mvSamples&amp;#39;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;,
&lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(alpha))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;type =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;l&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;iteration&amp;#39;&lt;/span&gt;,
&lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(beta))
&lt;span class=&quot;kw&quot;&gt;plot&lt;/span&gt;(samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;], samples[ , &lt;span class=&quot;st&quot;&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;], &lt;span class=&quot;dt&quot;&gt;xlab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(alpha),
&lt;span class=&quot;dt&quot;&gt;ylab =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(beta))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-nimble-explore/unnamed-chunk-15-1.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-nimble-explore/unnamed-chunk-15-2.png&quot; /&gt; &lt;img src=&quot;/2014/assets/figures/posts/2014-12-03-nimble-explore/unnamed-chunk-15-3.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Note the poor mixing (which is improved by the block sampler, as shown in the manual).&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Not completely certain that this destroys anything connected to the object as C pointers from before, but seems like it should.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Coreos Cluster Gotchas</title>
	 <link href="/2014/11/26/coreos-cluster-gotchas.html"/>
   <updated>2014-11-26T00:00:00+00:00</updated>
   <id>/11/26/coreos-cluster-gotchas</id>
   <content type="html">&lt;p&gt;Overall impression is that CoreOS is a promising way to easily set up a highly available cluster (e.g. when most important thing is that a service stays up when a node goes down) since it can migrate a containerized app to a new machine rather than having to already have the same app running on all machines. Either way a load-balancer needs to handle the addressing, which is do-able but somewhat tricky.&lt;/p&gt;
&lt;p&gt;Less useful in the role of a simple server; as admin on the base system is somewhat more limited (e.g. network stats, NFS sharing, etc), and more pointedly I seem to continually run afoul of stability issues in Fleet when cluster changes size, with no way to recover without destroying and relaunching the entire cluster.&lt;/p&gt;
&lt;p&gt;The most compelling features for me, the automated updating and the restarting containers on system reboot, can be replicated rather straight forwardly on a normal distribution.&lt;/p&gt;
&lt;p&gt;Fleet cannot pick a leader in a cluster of size 2 (no majority) and fails when CoreOS loses a majority. A cluster of size 3 can replace 1 node, but if 2 nodes fail, the cluster is hosed. See &lt;a href=&quot;https://coreos.com/docs/cluster-management/scaling/etcd-optimal-cluster-size/&quot;&gt;optimal cluster size&lt;/a&gt; and &lt;a href=&quot;https://github.com/coreos/etcd/issues/863#issuecomment-60523183&quot;&gt;etcd/issues/863&lt;/a&gt;. Rescaling may assign the new node a new address, and the majority must approve the new peer. If there’s not a majority available (e.g. cluster goes from 3 to 1) you’re stuck.&lt;/p&gt;
&lt;p&gt;In the etcd &amp;gt; 0.5.0 (alpha channel now methinks) some recovery is possible, see: &lt;a href=&quot;https://github.com/coreos/etcd/issues/1242&quot;&gt;etcd/issues/1242&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On Amazon, CoreOS provides ability to launch an AWS auto-scaling group as a CloudFormation configuration, which can set a minimum cluster size and always restart when a node goes down. Setting the minumum below 3 results in an invalid cluster (failed etcd connection due to lacking majority) and needs to be destroyed. Need to destroy the autoscaling group, cannot simply remove instances (since they will be regenerated). Also remember to adjust the security groups to set outside access for the appropriate service ports.&lt;/p&gt;
&lt;p&gt;Persistant URL address is challenging when nodes keep changing. If one node is guarenteed to be up, we can have it run the nginx load balancer to redirect to the other nodes (using toml nginx templates).&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;coreos-docker-part-ways&quot;&gt;CoreOS &amp;amp; Docker part ways?&lt;/h3&gt;
&lt;p&gt;Update: and &lt;a href=&quot;https://coreos.com/blog/rocket/&quot;&gt;now it seems CoreOS isn’t happy with Docker and seeks to invent their own runtime&lt;/a&gt;… time will tell if it gets critical mass to be viable, but doesn’t seem like it is well aligned with my own use cases of quickly deploying basic services (RStudio, Gitlab, Drone, Docker Registry).&lt;/p&gt;
&lt;p&gt;Also, lots of competition/ecosystem for container orchistration alternatives to fleet/CoreOS, though the use case for many of these isn’t entirely clear for my needs. In particular, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://aws.amazon.com/ecs/&quot;&gt;Amazon’s container service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/kubernetes&quot;&gt;Google’s container service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and of course, Docker’s own fig.org.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again all seem to emphasize more the stable, complex service model in the cloud and aren’t really necesary for the portable research software dev model.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Coreos Docker Registries Etc</title>
	 <link href="/2014/11/24/coreos-docker-registries-etc.html"/>
   <updated>2014-11-24T00:00:00+00:00</updated>
   <id>/11/24/coreos-docker-registries-etc</id>
   <content type="html">&lt;h2 id=&quot;a-secure-docker-registry&quot;&gt;A secure docker registry&lt;/h2&gt;
&lt;p&gt;Running one’s own docker registry is far more elegant than moving tarballs between machines (e.g. when migrating between servers, particularly for images that may contain sensitive data such as security credentials). While it’s super convenient to have a containerized version of the Docker registry ready for action, it doesn’t do much good without putting it behind an HTTPS server (otherwise we have to restart our entire docker service with the insecure flag to permit communication with an unauthenticated registry – doesn’t sound like a good idea). So this meant learning how to use &lt;code&gt;nginx&lt;/code&gt; load balancing, which I guess is useful to know more generally.&lt;/p&gt;
&lt;h3 id=&quot;first-pass-nginx-on-ubuntu-server&quot;&gt;First pass: nginx on ubuntu server&lt;/h3&gt;
&lt;p&gt;After a few false starts, decided the &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04&quot;&gt;digitalocean guide&lt;/a&gt; is easily the best (though steps 1-3 can be skipped by using a containerized &lt;code&gt;registry&lt;/code&gt; instead). This runs &lt;code&gt;nginx&lt;/code&gt; directly from the host OS, which is in some ways more straight forward but less portable. A few notes-to-self in working through the tutorial:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note: At first, nginx refuses to run because there’s was &lt;code&gt;default&lt;/code&gt; configuration in &lt;code&gt;cd /etc/nginx/sites-enabled&lt;/code&gt; that tries to create a conflict. Remove this and things go pretty nicely.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note: Running the registry container explicitly on port &lt;code&gt;127.0.0.1&lt;/code&gt; provides an internal-only port that we can then point to from nginx. (Actually this will no longer matter when we use a containerized &lt;code&gt;nginx&lt;/code&gt;, since we will simply not export these ports at all, but only expose the port of the &lt;code&gt;nginx&lt;/code&gt; load balancer). Still, good to finally be aware of the difference between &lt;code&gt;127.0.0.1&lt;/code&gt; and &lt;code&gt;0.0.0.0&lt;/code&gt; (the publicly visible localhost, and the default if we supply only a port) in this context.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note: Running and configuring &lt;code&gt;nginx&lt;/code&gt; Note that keys are specific to the url. This is necessary for the server signing request, but I believe could have been omitted in the root certificate. Here’s how w ego about creating a root key and certificate (&lt;code&gt;crt&lt;/code&gt;), a server key, server signing request (&lt;code&gt;csr&lt;/code&gt;), and then sign the latter with the former to get the server certificate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;openssl&lt;/span&gt; genrsa -out dockerCA.key 2048
&lt;span class=&quot;kw&quot;&gt;openssl&lt;/span&gt; req -x509 -new -nodes -key dockerCA.key -days 10000 -out dockerCA.crt -subj &lt;span class=&quot;st&quot;&gt;&amp;#39;/C=US/ST=Oregon/L=Portland/CN=coreos.carlboettiger.info&amp;#39;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;openssl&lt;/span&gt; genrsa -out docker-registry.key 2048
&lt;span class=&quot;kw&quot;&gt;openssl&lt;/span&gt; req -new -key docker-registry.key -out docker-registry.csr -subj &lt;span class=&quot;st&quot;&gt;&amp;#39;/C=US/ST=Oregon/L=Portland/CN=coreos.carlboettiger.info&amp;#39;&lt;/span&gt;
&lt;span class=&quot;kw&quot;&gt;openssl&lt;/span&gt; x509 -req -in docker-registry.csr -CA dockerCA.crt -CAkey dockerCA.key -CAcreateserial -out docker-registry.crt -days 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we also need the &lt;code&gt;htpasswd&lt;/code&gt; file from above, which needs &lt;code&gt;apache2-utils&lt;/code&gt; and so cannot be generated directly from the CoreOS terminal (though the &lt;code&gt;openssl&lt;/code&gt; certs can):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo htpasswd -bc /etc/nginx/docker-registry.htpasswd $USERNAME $PASSWORD&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having created these ahead of time, I end up just copying my keys into the Dockerfile for my &lt;code&gt;nginx&lt;/code&gt; instance (if we generated them on the container, we’d still need to get &lt;code&gt;dockerCA.crt&lt;/code&gt; off the container to authenticate the client machines. Makes for a simple Dockerfile that we then build locally:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM ubuntu:14.04
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y apache2-utils curl nginx openssl supervisor
COPY docker-registry /etc/nginx/sites-available/docker-registry
RUN ln -s /etc/nginx/sites-available/docker-registry /etc/nginx/sites-enabled/docker-registry

## Copy over certificates ##
COPY docker-registry.crt /etc/ssl/certs/docker-registry 
COPY docker-registry.key /etc/ssl/private/docker-registry 
COPY docker-registry.htpasswd /etc/nginx/docker-registry.htpasswd


EXPOSE 8080

## use supervisord to persist
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
CMD [&amp;quot;/usr/bin/supervisord&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we need to install the &lt;code&gt;dockerCA.crt&lt;/code&gt; certificate on any client that wants to access the private registry. On Ubuntu this looks like:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; mkdir /usr/local/share/ca-certificates/docker-dev-cert
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; cp devdockerCA.crt /usr/local/share/ca-certificates/docker-dev-cert
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; update-ca-certificates 
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; service docker restart&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But on CoreOS we use a different directory (and restarting the docker service doesn’t seem possible or necessary):&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; cp dockerCA.crt /etc/ssl/certs/docker-cert
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; update-ca-certificates  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note: Could not get the official &lt;code&gt;nginx&lt;/code&gt; container to run the &lt;code&gt;docker-registry&lt;/code&gt; config file as &lt;code&gt;/etc/nginx/nginx.conf&lt;/code&gt;, either with or without adding &lt;code&gt;daemon off;&lt;/code&gt; at the top of &lt;code&gt;/etc/nginx/nginx.conf&lt;/code&gt;. With, it complains this is a duplicate, (despite being recommended on the &lt;a href=&quot;https://registry.hub.docker.com/_/nginx&quot;&gt;nginx container documentation&lt;/a&gt;, though admittedly this already appears in the default command &lt;code&gt;[&amp;quot;nginx&amp;quot; &amp;quot;-g&amp;quot; &amp;quot;daemon off;&amp;quot;]&lt;/code&gt;). Without, the error says that &lt;code&gt;upstream&lt;/code&gt; directive is not allowed here. Not sure what to make of these errors, ended up running an ubuntu container and then just installing &lt;code&gt;nginx&lt;/code&gt; etc following the digitalocean guide. Ended up dropping the &lt;code&gt;daemon off;&lt;/code&gt; from the config file and running &lt;code&gt;service nginx start&lt;/code&gt; through &lt;code&gt;supervisord&lt;/code&gt; to ensure that the container stays up. Oh well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note: I got a 502 error when calling &lt;code&gt;curl&lt;/code&gt; against the the &lt;code&gt;nginx&lt;/code&gt; container-provided URL (with or without SSL enabled), since from inside the &lt;code&gt;nginx&lt;/code&gt; container we cannot access the host addresses. The simplest solution is to add &lt;code&gt;--net=&amp;quot;host&amp;quot;&lt;/code&gt; when we &lt;code&gt;docker run&lt;/code&gt; the &lt;code&gt;nginx&lt;/code&gt; container, but this isn’t particularly secure. Instead, we’ll link directly to the ports of the &lt;code&gt;registry&lt;/code&gt; container like this:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run  --name=registry -p 8080:8080 registry
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=nginx --net=container:registry nginx&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we do not need to export the registry port (e.g. &lt;code&gt;-p 5000:5000&lt;/code&gt;) at all, but we do need to export the &lt;code&gt;nginx&lt;/code&gt; load-balancer port &lt;em&gt;from the &lt;code&gt;registry&lt;/code&gt; container&lt;/em&gt; first, since we will simply be linking it’s network with the special &lt;code&gt;--net=container:registry&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that we would probably want to link a local directory to provide persistent storage for the &lt;code&gt;registry&lt;/code&gt;; in the above example images committed to registry are lost when the container is destroyed.&lt;/p&gt;
&lt;p&gt;We can now log in:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; login https://&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;YOUR-DOMAIN&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;:8080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now reference our private registry by using its full address in the namespace of the image in commands to &lt;code&gt;docker pull&lt;/code&gt;, &lt;code&gt;push&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt; etc.&lt;/p&gt;
&lt;h2 id=&quot;migrating-gitlab-between-servers&quot;&gt;Migrating gitlab between servers&lt;/h2&gt;
&lt;p&gt;This migration was my original motivation to configure the private docker registry; ironically it isn’t necessary for this case (though it’s useful for the drone image, for instance).&lt;/p&gt;
&lt;p&gt;Note that there is no need to migrate the redis and postgresql containers manually. Migrating the backup file over to the corresponding location in the linked volume and then running the backup-restore is sufficient. Upgrading is also surprisingly smooth; we can backup (just in case), then stop and remove the container (leaving the &lt;code&gt;redis&lt;/code&gt; and &lt;code&gt;postgresql&lt;/code&gt; containers running), pull and relaunch with otherwise matched option arguments and the upgrade runs automatically.&lt;/p&gt;
&lt;p&gt;When first launching the &lt;code&gt;gitlab&lt;/code&gt; container on a tiny droplet running coreos, my droplet seems invariably to hang. Rebooting from the digitalocean terminal seems to fix this. A nice feature of &lt;code&gt;fleet&lt;/code&gt; is that all the containers are restarted automatically after reboot, unlike when running these directly from &lt;code&gt;docker&lt;/code&gt; on my ubuntu machine.&lt;/p&gt;
&lt;h2 id=&quot;notes-on-fleet-unit-files&quot;&gt;Notes on fleet unit files&lt;/h2&gt;
&lt;p&gt;Fleet unit files are actually pretty handy and straight forward. One trick is that we must quote commands in which we want to make use of environmental variables. For instance, one must write:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Environment=&amp;quot;VERSION=1.0&amp;quot;
ExecStart=/bin/bash -c &amp;quot;/usr/bin/docker run image:${VERSION}&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in a &lt;code&gt;Service&lt;/code&gt; block, rather than &lt;code&gt;ExecStart=/usr/bin/docker run ...&lt;/code&gt; directly, for the substitution to work. It seems if we are using the more standard practice of environment files (which after all is the necessary approach to avoid having to edit the unit file directly one way or another anyway), we can avoid the &lt;code&gt;bin/bash&lt;/code&gt; wrapper and insert the environment reference directly.&lt;/p&gt;
&lt;p&gt;If we’re not doing anything fancy wrt load balancing between different servers, we don’t have that much use for the corresponding “sidekick” unit files that keep our global &lt;code&gt;etcd&lt;/code&gt; registry up to date. Perhaps these will see more use later.&lt;/p&gt;
&lt;h2 id=&quot;cloud-config&quot;&gt;Cloud-config&lt;/h2&gt;
&lt;p&gt;Note that we need to refresh the discovery url pretty-much anytime we completely destroy the cluster.&lt;/p&gt;
&lt;p&gt;A few edits to my cloud-config to handle initiating swap, essential for running most things (gitlab, rstudio) on tiny droplets. Still requires one manual reboot for the allocation to take effect. Adds this to the &lt;code&gt;units&lt;/code&gt; section of &lt;code&gt;#cloud-config&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    ## Configure SWAP as per https://github.com/coreos/docs/issues/52
    - name: swap.service
      command: start
      content: |
        [Unit]
        Description=Turn on swap

        [Service]
        Type=oneshot
        Environment=&amp;quot;SWAPFILE=/1GiB.swap&amp;quot;
        RemainAfterExit=true
        ExecStartPre=/usr/sbin/losetup -f ${SWAPFILE}
        ExecStart=/usr/bin/sh -c &amp;quot;/sbin/swapon $(/usr/sbin/losetup -j ${SWAPFILE} | /usr/bin/cut -d : -f 1)&amp;quot;
        ExecStop=/usr/bin/sh -c &amp;quot;/sbin/swapoff $(/usr/sbin/losetup -j ${SWAPFILE} | /usr/bin/cut -d : -f 1)&amp;quot;
        ExecStopPost=/usr/bin/sh -c &amp;quot;/usr/sbin/losetup -d $(/usr/sbin/losetup -j ${SWAPFILE} | /usr/bin/cut -d : -f 1)&amp;quot;

        [Install]
        WantedBy=local.target

    - name: swapalloc.service
      command: start
      content: |
        [Unit]
        Description=Allocate swap

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c &amp;quot;sudo fallocate -l 1024m /1GiB.swap &amp;amp;&amp;amp; sudo chmod 600 /1GiB.swap &amp;amp;&amp;amp; sudo chattr +C /1GiB.swap &amp;amp;&amp;amp; sudo mkswap /1GiB.swap&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More probably be structured more elegantly, but it works. (Not much luck trying to tweak this into a bunch of &lt;code&gt;ExecStartPre&lt;/code&gt; commands though.&lt;/p&gt;
&lt;h2 id=&quot;nfs-sharing-on-coreos&quot;&gt;NFS sharing on CoreOS?&lt;/h2&gt;
&lt;p&gt;Couldn’t figure this one out. &lt;a href=&quot;http://serverfault.com/questions/647014/share-disks-through-nfs-on-a-coreos-cluster&quot;&gt;My SO Q here&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Coreos And Other Infrastructure Notes</title>
	 <link href="/2014/11/19/coreos-and-other-infrastructure-notes.html"/>
   <updated>2014-11-19T00:00:00+00:00</updated>
   <id>/11/19/coreos-and-other-infrastructure-notes</id>
   <content type="html">&lt;h2 id=&quot;coreos&quot;&gt;CoreOS?&lt;/h2&gt;
&lt;p&gt;Security model looks excellent. Some things not so clear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In a single node setup, what happens with updates? Would containers being run directly come down and not go back up automatically? In general, how effective or troublesome is it to run a single, low-demand app on a single node CoreOS rather than, say, an ubuntu image (e.g. just to benefit from the security updates model)? For instance, would an update cause a running app to exit in this scenario? (Say, if the container is launched directly with &lt;code&gt;docker&lt;/code&gt; and not through &lt;code&gt;fleet&lt;/code&gt;?) (Documentation merely notes that cluster allocation / fleet algorithm is fastest with between 3 &amp;amp; 9 nodes).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If I have a heterogenous cluster with one more powerful compute node, is there a way to direct that certain apps are run on that node and that other apps are not?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Looks like one needs a load-balancer to provide a consistent IP for containers that might be running on any node of the cluster?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/coreos/docs/issues/52&quot;&gt;Enabling swap&lt;/a&gt;. Works, but is there a way to do this completely in &lt;code&gt;cloud-config&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;setting-up-my-domain-names-for-digitalocean&quot;&gt;Setting up my domain names for DigitalOcean&lt;/h2&gt;
&lt;p&gt;In Dreamhost DNS management:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I have my top-level domain registered through Dreamhost, uses dreamhost’s nameservers.&lt;/li&gt;
&lt;li&gt;A-level entry for top level domain points to (the new) Github domain IP address&lt;/li&gt;
&lt;li&gt;Have CNAME entries for &lt;code&gt;www&lt;/code&gt; and &lt;code&gt;io&lt;/code&gt; pointing to &lt;code&gt;cboettig.github.io&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;First step&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add an A-level entry, &lt;code&gt;server.carlboettiger.info&lt;/code&gt;, pointing to DigitalOcean server IP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then go over to DigitalOcean panel.&lt;/p&gt;
&lt;p&gt;From DigitalOcean DNS management:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;add new (A level) DNS entry as &lt;code&gt;server.carlboettiger.info&lt;/code&gt; pointing to DO server IP&lt;/li&gt;
&lt;li&gt;Delete the existing three NS entries &lt;code&gt;ns1.digitalocean.com&lt;/code&gt; etc.&lt;/li&gt;
&lt;li&gt;Add three new NS entries using &lt;code&gt;ns1.dreamhost.com&lt;/code&gt; etc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Things should be good to go!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Wssspe Feedback</title>
	 <link href="/2014/11/17/wssspe-feedback.html"/>
   <updated>2014-11-17T00:00:00+00:00</updated>
   <id>/11/17/wssspe-feedback</id>
   <content type="html">&lt;h3 id=&quot;wssspe-working-groups-reproducibility-reuse-and-sharing-neil-che-hong&quot;&gt;WSSSPE working groups: Reproducibility, Reuse, and Sharing (Neil Che Hong)&lt;/h3&gt;
&lt;p&gt;Our group focused on journal policies regarding software papers. Our objectives were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A Survey of journals that publish software papers. The Software Sustainability Institute already maintains &lt;a href=&quot;http://www.software.ac.uk/resources/guides/which-journals-should-i-publish-my-software&quot;&gt;a list&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A summary of policies each of these journals has in place regarding software papers. (e.g. licensing requirements, repository requirements, required sections in the manuscripts regarding installation or tests, etc).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Developing a five-star rating system for ranking these policies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply our rating system to each of these journals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Solicit feedback &amp;amp; iterate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We got about half way though this for some of the most recognized journals on the list; &lt;a href=&quot;https://docs.google.com/document/d/1Mivadj16_9tPrw8Nxmhp72AtNC0BEVO-f7M84FXTbSU/edit?usp=drive_web&quot;&gt;see Google Doc notes&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;feedback-for-wssspe&quot;&gt;Feedback for WSSSPE:&lt;/h3&gt;
&lt;p&gt;WSSSPE’s conference-proceedings model of submitting a short papers that get five very thorough expert reviews ahead of time is really excellent. This is not common practice in my field, so this was my first time participating in such a model. Not only did I benefit from both the chance to write up our piece ahead of time and get expert feedback from people coming from a broader range of backgrounds than I can usually interact with, but also the ability to read the full papers and not just the abstracts of other attendees in advance of the workshop was an invaluable way to learn more, make the most of the time we had, and keep a record.&lt;/p&gt;
&lt;p&gt;A full-day workshop is a big travel commitment (travel costs, 2 nights lodging, and using up most of the preceding and following day) while simultaneously being not much time to meet people, share ideas, and start working towards any actual products.&lt;/p&gt;
&lt;p&gt;The format proposed at the end of the session that seemed most popular in the show of hands for future WSSSPEs – a two to three day event uncoupled from Supercomputing, based in the US in an easy city to fly into, and with more time to move ideas forward into products using a small group / hackathon model would address most of my criticsm.&lt;/p&gt;
&lt;h3 id=&quot;misc-notesdiscussions&quot;&gt;Misc notes/discussions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Interesting discussion/ideas for tracking usage of software based on updating patterns, from James Horowitz and company: &lt;a href=&quot;http://james.howison.name/pubs/wiggins2009heartbeat-meas0.pdf&quot;&gt;Heartbeat (pdf)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Neil mentioned a similar workshop he had recently taken part in creating a &lt;a href=&quot;http://m.f1000research.com/articles/3-271/v1&quot;&gt;reviewer’s oath&lt;/a&gt;, recently submitted as an opinion piece to F1000. Certainly more of a guideline than most journals give, if a bit pedantic at times (for instance, as much as I believe in signing my own reviews, I would not recommend it to someone else as a blanket policy in the same vein as basic ethics like acknowledging what I don’t know. I think the ‘Oath’ needs to treat this with greater nuiance.) Anyway, food for thought.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(I didn’t manage to catch much with twitter this time, guess too much happening in in-person discussions).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Nimble Explore</title>
	 <link href="/2014/11/14/nimble-explore.html"/>
   <updated>2014-11-14T00:00:00+00:00</updated>
   <id>/11/14/nimble-explore</id>
   <content type="html">&lt;p&gt;A quick first exploration of &lt;a href=&quot;http://r-nimble.org&quot;&gt;NIMBLE&lt;/a&gt; and some questions.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;nimble&amp;quot;&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;sde&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s simulate from a simple OU process: &lt;span class=&quot;math&quot;&gt;\(dX = \alpha (\theta - X) dt + \sigma dB_t\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;set.seed&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;123&lt;/span&gt;)
d &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.5&lt;/span&gt; *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;-x))
s &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;expression&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;) 
data &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sde.sim&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;X0=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;,&lt;span class=&quot;dt&quot;&gt;drift=&lt;/span&gt;d, &lt;span class=&quot;dt&quot;&gt;sigma=&lt;/span&gt;s, &lt;span class=&quot;dt&quot;&gt;T=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;N=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;400&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## sigma.x not provided, attempting symbolic derivation.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;i.e. &lt;span class=&quot;math&quot;&gt;\(\alpha = 0.5\)&lt;/span&gt;, &lt;span class=&quot;math&quot;&gt;\(\theta = 10\)&lt;/span&gt;, &lt;span class=&quot;math&quot;&gt;\(\sigma=1\)&lt;/span&gt;, starting at &lt;span class=&quot;math&quot;&gt;\(X_0 = 6\)&lt;/span&gt; and running for 100 time units with a dense sampling of 400 points.&lt;/p&gt;
&lt;p&gt;Le’t now estimate a Ricker model based upon (set aside closed-form solutions to this estimate for the moment, since we’re investigating MCMC behavior here).&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;code &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;modelCode&lt;/span&gt;({
      K ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;40.0&lt;/span&gt;)
      r ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;, &lt;span class=&quot;fl&quot;&gt;20.0&lt;/span&gt;)
  sigma ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;fl&quot;&gt;1e-6&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;100&lt;/span&gt;)

  iQ &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; /&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(sigma *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;sigma)

  x[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dunif&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)

  for(t in &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:(N&lt;span class=&quot;dv&quot;&gt;-1&lt;/span&gt;)){
    mu[t] &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;log&lt;/span&gt;(x[t]) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;r *&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt; -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;x[t]/K) 
    x[t&lt;span class=&quot;dv&quot;&gt;+1&lt;/span&gt;] ~&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;dlnorm&lt;/span&gt;(mu[t], iQ) 
  }
})

constants &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;N =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;length&lt;/span&gt;(data$x))
inits &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;K =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;r =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;sigma =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)

Rmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;nimbleModel&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;code=&lt;/span&gt;code, &lt;span class=&quot;dt&quot;&gt;constants=&lt;/span&gt;constants, &lt;span class=&quot;dt&quot;&gt;data=&lt;/span&gt;data, &lt;span class=&quot;dt&quot;&gt;inits=&lt;/span&gt;inits)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NIMBLE certainly makes for a nice syntax so far. Here we go now: create MCMC specification and algorithm&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;mcmcspec &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;MCMCspec&lt;/span&gt;(Rmodel)
Rmcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;buildMCMC&lt;/span&gt;(mcmcspec)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can also query some details regarding our specification (set by default)&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;mcmcspec$&lt;span class=&quot;kw&quot;&gt;getSamplers&lt;/span&gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] RW sampler;   targetNode: K,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
## [2] RW sampler;   targetNode: r,  adaptive: TRUE,  adaptInterval: 200,  scale: 1
## [3] RW sampler;   targetNode: sigma,  adaptive: TRUE,  adaptInterval: 200,  scale: 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;mcmcspec$&lt;span class=&quot;kw&quot;&gt;getMonitors&lt;/span&gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## thin = 1: K, r, sigma, x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to compile model and MCMC algorithm&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;Cmodel &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(Rmodel)
Cmcmc &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;compileNimble&lt;/span&gt;(Rmcmc, &lt;span class=&quot;dt&quot;&gt;project =&lt;/span&gt; Cmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note we could have specified the &lt;code&gt;Rmodel&lt;/code&gt; as the “project” (as shown in the example from the Nimble website), but this is more explicit. Rather convenient way to add to an existing model in this manner.&lt;/p&gt;
&lt;p&gt;And Now we can execute the MCMC algorithm in blazing fast C++ and then extract the samples&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;Cmcmc&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;10000&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;samples &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;as.matrix&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;nfVar&lt;/span&gt;(Cmcmc, &lt;span class=&quot;st&quot;&gt;&amp;#39;mvSamples&amp;#39;&lt;/span&gt;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do these estimates compare to theory:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$K)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.05681&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(samples$r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.180207&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;Some quick impressions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Strange that &lt;code&gt;Rmodel&lt;/code&gt; call has to be repeated before we can set up a custom MCMC (&lt;a href=&quot;http://r-nimble.org/examples&quot;&gt;nimble docs&lt;/a&gt;). How/when was this object altered since it was defined in the above code? Seems like this could be problematic for interpreting / reproducing results?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What’s going on with &lt;code&gt;getSamplers()&lt;/code&gt; and &lt;code&gt;getMonitors()&lt;/code&gt;? Guessing these are in there just to show us what the defaults will be for our model?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;why do we assign &lt;code&gt;Cmodel&lt;/code&gt; if it seems we don’t use it? (the compilation needs to be done but isn’t explicitly passed to the next step). Seems we can use &lt;code&gt;Cmodel&lt;/code&gt; instead of &lt;code&gt;Rmodel&lt;/code&gt; in the &lt;code&gt;Cmcmc &amp;lt;- compileNimble(Rmcmc, project = Cmodel)&lt;/code&gt;, which makes the dependency more explicit, at least that notation is more explicit. Seems like it should be possiple to omit the first &lt;code&gt;compileNimble()&lt;/code&gt; and have the second call the &lt;code&gt;compileNimble&lt;/code&gt; automatically if it gets an object whose class is that of &lt;code&gt;Rmodel&lt;/code&gt; instead?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeated calls to &lt;code&gt;Cmcmc&lt;/code&gt; seem not to give the same results. Are we adding additional mcmc steps by doing this?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thinking an &lt;code&gt;as.data.frame&lt;/code&gt; would be nicer than &lt;code&gt;as.matrix&lt;/code&gt; in the &lt;code&gt;nfVar&lt;/code&gt; &lt;code&gt;mvSamples&lt;/code&gt; coercion.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t understand what &lt;code&gt;simulate&lt;/code&gt; does (or why it always returns NULL?).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Draft Wssspe Thoughts</title>
	 <link href="/2014/11/13/draft-wssspe-thoughts.html"/>
   <updated>2014-11-13T00:00:00+00:00</updated>
   <id>/11/13/draft-wssspe-thoughts</id>
   <content type="html">&lt;p&gt;Scratching out notes for lightning talk at WSSSPE…&lt;/p&gt;
&lt;p&gt;(CB from ropensci, Karthik too. )&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Responses to the challenges of academic software can be divided into roughly two categories. Give them better tools! Give them better training!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We do software: over 50 packages for accessing data repositories, integrating and manipulating, annotating and publishing data. We do training: workshops, online tutorials, developing course materials. But most importantly, we aim to build community. At rOpenSci, it’s not about us and them, it’s about the community. Through mostly informal mechanisms – hackathons, workshops, blogging, and Github – we have brought people together to write software that does what they need it to do.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;what does it look like? Engaging domain scientists. reviewing new contributions on Github – from bugs and Pull Requests to completely new packages. Hackathons, bringing people together to code away on whatever itch they’ve always wanted to scratch. Discussions on blogs on social media, conferences, and even here at WSSSPE. We want domain practioners to look up and see other domain practioners learning this process, learning to code, to write tests and rely on version control, and to think how things can be done differently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the hardest part isn’t walking the path to sustainable software. The hardest part is knowing you want to walk it in the first place. At the end of the day, our success isn’t measured in software packages, publications, or grant dollars; but here, in new faces (on Github, blog, or live events) that say “I’m here! I get it!”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;!--
We build tools in a language familiar to the domain scientists we expect
to use them, not just to save them time or make them more reproducible
in the future, but help them right now to do something they cannot
otherwise do.
--&gt;
&lt;!--

## Give them tools! ## 

- Native language tools: 
  Very widespread use of R in our focal disciplines)
- Save users time _today_: 
  Tools that do something users already want to do and can&#39;t (query arbitrary species distributions &amp; plot on map!, yay! automatically publish data/metadata to repository, yay!), are easier to adopt than ones that make you re-learn how to do something you already know. 
- Save them even more time in the future
  all the reproducibility &amp; sustainability goodness baked in

## Give them training! ## 

- Workshops
- self-driven tutorials
- train-the-trainers: modular teaching


## Give them community! ##

- Hackathons
- Real people
- Social media: Github, Website &amp; blog, Twitter


--&gt;
</content>
 </entry>
 
 <entry>
   <title>Discrete Grid Notes</title>
	 <link href="/2014/11/13/discrete-grid-notes.html"/>
   <updated>2014-11-13T00:00:00+00:00</updated>
   <id>/11/13/discrete-grid-notes</id>
   <content type="html">&lt;p&gt;Smoothing post-policy calc?&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;
fig3 &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ggplot&lt;/span&gt;(policies, &lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(stock, stock -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;value, &lt;span class=&quot;dt&quot;&gt;color=&lt;/span&gt;method)) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;stat_smooth&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;lwd=&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;1.2&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;method=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;loess&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;degree=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;span=&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;0.2&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;level=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;n=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;50&lt;/span&gt;) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;facet_wrap&lt;/span&gt;(~method) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;xlab&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;stock size, x(t)&amp;quot;&lt;/span&gt;) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;ylab&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;escapement, S(t)&amp;quot;&lt;/span&gt;)  +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;scale_colour_manual&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;values=&lt;/span&gt;colorkey, &lt;span class=&quot;dt&quot;&gt;guide=&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;)
fig3

&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;dplyr&amp;quot;&lt;/span&gt;)
s_policies &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;select_&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;ggplot_build&lt;/span&gt;(fig3)$data[[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]], &lt;span class=&quot;st&quot;&gt;&amp;quot;x&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;PANEL&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;y&amp;quot;&lt;/span&gt;)
s_policies$PANEL &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mapvalues&lt;/span&gt;(s_policies$PANEL, &lt;span class=&quot;dt&quot;&gt;from =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;span class=&quot;dv&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;to =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;levels&lt;/span&gt;(policies$method))
&lt;span class=&quot;kw&quot;&gt;names&lt;/span&gt;(s_policies) &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;names&lt;/span&gt;(policies)

&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;tidyr&amp;quot;&lt;/span&gt;)
opt_policy &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;s_policies %&amp;gt;%&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;mutate&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;harvest =&lt;/span&gt; stock -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;value) %&amp;gt;%&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;select&lt;/span&gt;(stock, method, &lt;span class=&quot;dt&quot;&gt;value =&lt;/span&gt; harvest) %&amp;gt;%
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;spread&lt;/span&gt;(method, value) %&amp;gt;%
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;select&lt;/span&gt;(-stock)
OPT &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;as.data.frame&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(opt_policy, function(y) &lt;span class=&quot;kw&quot;&gt;sapply&lt;/span&gt;(y, function(x) &lt;span class=&quot;kw&quot;&gt;which.min&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;abs&lt;/span&gt;(x-x_grid)))))&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Dear DockerHub users: please configure your repository links</title>
	 <link href="/2014/11/07/dear-docker-hub-users.html"/>
   <updated>2014-11-07T00:00:00+00:00</updated>
   <id>/11/07/dear-docker-hub-users</id>
   <content type="html">&lt;p&gt;The DockerHub is a great resource for discovering and distributing Dockerfiles. Many users sharing public images take advantage of the Docker Hub’s &lt;em&gt;Automated Build&lt;/em&gt; configuration, which is excellent as this automatically allows the Hub to display the Dockerfile and provides some medium of security above simply downloading and running some untrusted binary black box.&lt;/p&gt;
&lt;p&gt;Unfortunately, far fewer users configure &lt;em&gt;Repository Links&lt;/em&gt; to trigger builds to update even when the resulting Dockerfile is unchanged. As a result, many excellent Docker containers that are not under active development have not been rebuilt in several months, meaning that they still contain widely known dangerous security flaws such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Shellshock_(software_bug)&quot;&gt;Shellshock&lt;/a&gt; (September 2014).&lt;/p&gt;
&lt;p&gt;This problem is easily avoided by configuring the &lt;em&gt;Repository Links&lt;/em&gt; setting to point to the repository being used as a base image in &lt;code&gt;FROM&lt;/code&gt;. The official base images such as &lt;code&gt;debian&lt;/code&gt; and &lt;code&gt;ubuntu&lt;/code&gt; (e.g. the images with no additional namespace) are regularly updated to patch security vulnerabilities as soon as they are discovered, resulting in updates being made every few days on average. Setting the repository link to the &lt;code&gt;FROM&lt;/code&gt; source allows your repository to be rebuilt as soon as its base image has been updated, ensuring that you inherit those updates.&lt;/p&gt;
&lt;p&gt;Naturally this strategy does not help if your &lt;code&gt;FROM&lt;/code&gt; image isn’t an official base image and hasn’t configured &lt;em&gt;Repository Links&lt;/em&gt; (or if such a break in the chain appears anywhere along the &lt;code&gt;FROM&lt;/code&gt; recursion). In such cases, having a &lt;code&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get upgrade -y&lt;/code&gt; command (or equivalent option for your distribution) might be a good idea to make sure that your image at least gets the latest updates, but you’ll still need to set up some automatic or manual &lt;em&gt;Build Triggers&lt;/em&gt; to ensure this is run regularly; or better yet, just &lt;em&gt;avoid building on or using stale images&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you do have a reliable &lt;em&gt;Repository Links&lt;/em&gt; chain to an official image, then &lt;code&gt;apt-get upgrade&lt;/code&gt; is not necessary (and in fact is not advised in Best Practices). Instead, make sure all images in the chain call &lt;code&gt;apt-get update&lt;/code&gt; in the same RUN line as &lt;code&gt;apt-get install -y ...&lt;/code&gt;, which will ensure that cache is broken and the latest versions of the packages are installed. See the official &lt;a href=&quot;https://docs.docker.com/articles/dockerfile_best-practices/&quot;&gt;Dockerfile Best Practices&lt;/a&gt; for more information.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;NB: I’m not a security professional; this just looks like common sense usage&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>linking binaries from other containers</title>
	 <link href="/2014/11/05/notes.html"/>
   <updated>2014-11-05T00:00:00+00:00</updated>
   <id>/11/05/notes</id>
   <content type="html">&lt;p&gt;Been thinking about this for a while, but &lt;span class=&quot;citation&quot; data-cites=&quot;benmarwick&quot;&gt;@benmarwick&lt;/span&gt; ’s examples with &lt;code&gt;--volumes-from&lt;/code&gt; convinced me to give this a try.&lt;/p&gt;
&lt;p&gt;While there’s an obvious level of convenience in having something like LaTeX bundled into the &lt;code&gt;hadleyverse&lt;/code&gt; container so that users can build nice pdfs, if often feels not very docker-esque to me to just throw the kitchen sink into a container. At the risk of some added complexity, we can provide LaTeX from a dedicated TeX container to a container that doesn’t have it built in, like &lt;code&gt;rocker/rstudio&lt;/code&gt;. Check this out:&lt;/p&gt;
&lt;p&gt;First, we run the docker container providing the texlive binaries as linked volume. Note that even after the 4 GB &lt;code&gt;texlive&lt;/code&gt; container has been downloaded that this is slow to execute due to the volume linking flag (not really sure why that is).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --name tex -v /usr/local/texlive leodido/texlive true&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the above task is complete, we can run the &lt;code&gt;rstudio&lt;/code&gt; container, which doesn’t have tex installed by itself, and access tex by linking:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -dP --volumes-from tex \
 -e PATH=$PATH:/usr/local/texlive/2014/bin/x86_64-linux/ \
 rocker/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now log into RStudio, create a new Rnw file and presto, RStudio discovers the tex compilers and builds us a pdf. This does make our Docker execution lines a bit long, but that’s what &lt;a href=&quot;www.fig.sh&quot;&gt;fig&lt;/a&gt; is for. (Or a good ole Makefile).&lt;/p&gt;
&lt;p&gt;Note this requires we build &lt;code&gt;texlive&lt;/code&gt; in a way that isolates it to it’s own path (e.g. &lt;code&gt;/usr/local/texlive&lt;/code&gt;). The default installation with &lt;code&gt;apt-get&lt;/code&gt; installs everything in separate locations that overlap with existing directories (like &lt;code&gt;/usr/bin&lt;/code&gt;), which makes linking clumsy or impossible (we would need separate paths for all the components, e.g. since shared libraries aren’t found under the &lt;code&gt;bin&lt;/code&gt; path, and we cannot link such a volume to another container without destroying everything in it’s &lt;code&gt;/usr/bin&lt;/code&gt;, clearly not a good idea). Instead, if we use the standard &lt;code&gt;texlive&lt;/code&gt; install script from &lt;a href=&quot;https://www.tug.org/texlive/&quot; class=&quot;uri&quot;&gt;https://www.tug.org/texlive/&lt;/a&gt;, this installs everything into &lt;code&gt;/usr/local/texlive&lt;/code&gt; which is much more portable as illustrated above. Not quite sure if it’s actually a good idea to build containers this way or not.&lt;/p&gt;
&lt;p&gt;I’ll keep shipping latex inside the &lt;code&gt;hadleyverse&lt;/code&gt; container (has about 300 MB of texlive that covers most common usecases), but this is certainly an intruging recipe to mix and match.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Three Interfaces For Docker</title>
	 <link href="/2014/11/03/three-interfaces-for-Docker.html"/>
   <updated>2014-11-03T00:00:00+00:00</updated>
   <id>/11/03/three-interfaces-for-Docker</id>
   <content type="html">&lt;p&gt;Here I outline three broad, different strategies for incorporating Docker into a user’s workflow, particularly from the perspective of an instructor getting a group of students up and running in a containerized environment, but also in the context of more generic collaborations. The options require progressively more setup and result in a progressively more ‘native’ feel to running Docker. My emphasis is on running Dockerized R applications and RStudio, though much the same thing can be accomplished with iPython notebooks and many other web apps.&lt;/p&gt;
&lt;p&gt;Of course the great strength of Docker is the relative ease with which one can move between these three strategies while using the identical container, maintaining a consistent computational environment in each case.&lt;/p&gt;
&lt;h2 id=&quot;web-hosted-docker&quot;&gt;Web-hosted Docker&lt;/h2&gt;
&lt;p&gt;In this approach, RStudio-server is deployed on a web server and accessed through the browser. The use of Docker containers makes it easier for an instructor to deploy a consistent environment quickly with the desired software pre-installed and pre-configured.&lt;/p&gt;
&lt;h3 id=&quot;advantages&quot;&gt;Advantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A user just needs a web browser and the URL of the server.&lt;/li&gt;
&lt;li&gt;No need to install any local software.&lt;/li&gt;
&lt;li&gt;No need to download big files.&lt;/li&gt;
&lt;li&gt;Should work with any device that supports a modern browser, including most tablets.&lt;/li&gt;
&lt;li&gt;Convenient to temporarily scale computation onto a larger system.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages&quot;&gt;Disadvantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;requires a network connection (at all times)&lt;/li&gt;
&lt;li&gt;requires access to a server with sufficient computational power for the task.&lt;/li&gt;
&lt;li&gt;Someone has to manage user &amp;amp; network security (as with any web server).&lt;/li&gt;
&lt;li&gt;Need additional mechanisms for moving files on and off the server, such as git.&lt;/li&gt;
&lt;li&gt;No native interfaces available, must manage files, edit text etc. through the RStudio IDE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;setup&quot;&gt;Setup:&lt;/h3&gt;
&lt;p&gt;A Docker container running RStudio can be deployed with a single command, see &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image&quot;&gt;rocker wiki instructions on RStudio&lt;/a&gt; for details. The instructor or team-member responsible for the setup would simply need to install docker on server. If multiple students will be accessing a single RStudio-server instance, it must be configured for multiple users. Alternately multiple containers can be run on different ports of the same server. (See wiki).&lt;/p&gt;
&lt;p&gt;Hint: Users can also take advantage of the new R package &lt;a href=&quot;https://github.com/sckott/analogsea&quot;&gt;analogsea&lt;/a&gt; to quickly launch and manage an RStudio Server instance on the Digital Ocean cloud platform. &lt;code&gt;analogsea&lt;/code&gt; can also facilitate transfers of code and other files onto and off of the server.&lt;/p&gt;
&lt;h2 id=&quot;self-hosted-docker&quot;&gt;Self-hosted Docker&lt;/h2&gt;
&lt;p&gt;In this approach, the user installs docker (via &lt;code&gt;boot2docker&lt;/code&gt;, if necessary) on their local machine, but still interacts with the container using the same web-based interface (e.g. &lt;code&gt;rstudio-server&lt;/code&gt;, &lt;code&gt;ipython-notebook&lt;/code&gt;) that one would use in the cloud-hosted model.&lt;/p&gt;
&lt;h3 id=&quot;advantages-1&quot;&gt;Advantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;No need for a network connection (at least once the container image is downloaded / transfered)&lt;/li&gt;
&lt;li&gt;No need to have a server available (with the associated cost and security overhead)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages-1&quot;&gt;Disadvantages:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;More initial setup: install &lt;code&gt;docker&lt;/code&gt; locally, or install &lt;code&gt;boot2docker&lt;/code&gt; for Mac/Windows users.&lt;/li&gt;
&lt;li&gt;Need to use &lt;code&gt;git&lt;/code&gt; or &lt;code&gt;docker copy&lt;/code&gt; to move files from the container to the host or vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hint: Users might also check out the R package &lt;a href=&quot;https://github.com/wch/harbor&quot;&gt;harbor&lt;/a&gt; for interacting with Docker locally from R.&lt;/p&gt;
&lt;h3 id=&quot;setup-1&quot;&gt;Setup:&lt;/h3&gt;
&lt;p&gt;Setup is much the same as on a remote server, though there is no need to set custom usernames or passwords since the instance will be accessible only to local users. See &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image&quot;&gt;rocker wiki instructions on RStudio&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2 id=&quot;integrated-docker&quot;&gt;Integrated Docker&lt;/h2&gt;
&lt;p&gt;This approach is the same as the self-hosted approach, except that we link shared volumes with the host. At minimum this makes it easier to move files on and off the container without learning git.&lt;/p&gt;
&lt;p&gt;An intriguing advantage of this approach is that it does not restrict the user to the RStudio IDE as a way of editing text, managing files and versions, etc. Most users do not rely exclusively on RStudio for these tasks, and may find that restriction limiting. The integrated approach may be more suited for experienced users who are set in their ways and do not need a pixel-identical work environment of RStudio useful for following directions in a classroom. In the integrated approach, a user can continue to rely on whatever their preferred native tools are, while ensuring that code execution occurs (invisibly) on a Dockerized container.&lt;/p&gt;
&lt;h3 id=&quot;advantages-2&quot;&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Can use native OS tools (text editors, file browsers, version control front ends, etc) for all interactions&lt;/li&gt;
&lt;li&gt;No network required (once image is downloaded / transfered).&lt;/li&gt;
&lt;li&gt;No servers required&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;disadvantages-2&quot;&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Additional setup beyond self-hosting: mapping shared volumes, managing user permissions.&lt;/li&gt;
&lt;li&gt;Potentially less well suited for classroom use, which may benefit from everyone using the identical RStudio interface rather than a range of different text editors, etc. (Of course one can still share volumes while using RStudio as the IDE).&lt;/li&gt;
&lt;li&gt;Cannot open external windows (e.g. if running R in terminal instead of RStudio, the container running R cannot open an X11 window to display plots. Instead, a user must do something like &lt;code&gt;ggsave()&lt;/code&gt; after plotting interactively to view the resulting graphic in the native file browser. (This is more tedious in base graphics that need &lt;code&gt;dev.off()&lt;/code&gt; etc.). Of course this is not an issue when using RStudio with linked volumes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;setup-2&quot;&gt;Setup&lt;/h3&gt;
&lt;p&gt;The key here is simply to link the working directory on the host to the file system on the container. That way any changes made to the host copy using the host OS tools are immediately available to the container, and vice versa. Setup requires a bit more effort on Windows at this time, though is natively supported for Mac in Docker 1.3. Some care may also be necessary not to change the permissions of the file. See details in the &lt;a href=&quot;https://github.com/rocker-org/rocker/wiki/Shared-files-with-host-machine&quot;&gt;rocker wiki on shared files&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;aliases&quot;&gt;aliases&lt;/h4&gt;
&lt;p&gt;The most aggressive form of the integrated approach is to literally alias common commands like &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;rstudio&lt;/code&gt; as the corresponding docker calls in &lt;code&gt;.bashrc&lt;/code&gt;, e.g.&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;alias&lt;/span&gt; R=&lt;span class=&quot;st&quot;&gt;&amp;#39;docker run --rm -it --user docker -v $(pwd):/home/docker/`basename $PWD` -w /home/docker/`basename $PWD` rocker/hadleyverse R&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;makes the command &lt;code&gt;R&lt;/code&gt; launch an instance of the &lt;code&gt;rocker/hadleyverse&lt;/code&gt; container sharing the current working directory. Clearly different containers could be substituted in place of &lt;code&gt;rocker/hadleyverse&lt;/code&gt;, including custom extensions. This helps ensure that R is always run in the portable, Dockerized environment. Other than the lack of X11 display for plots, this works and feels identical to an interactive R terminal session.&lt;/p&gt;
&lt;h4 id=&quot;other-tweaks&quot;&gt;Other tweaks&lt;/h4&gt;
&lt;p&gt;Mac/Windows users might also want to customize &lt;code&gt;boot2docker&lt;/code&gt;’s resources to make more of the host computer’s memory and processors available to Docker.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Goodbye Jekyll?</title>
	 <link href="/2014/10/28/jekyll-free.html"/>
   <updated>2014-10-28T00:00:00+00:00</updated>
   <id>/10/28/jekyll-free</id>
   <content type="html">&lt;p&gt;The great strength of Jekyll is in providing a really convenient HTML templating system through the &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; directories and Liquid variables (including the auto-populated ones like &lt;code&gt;page.previous.url&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;For quickly deploying simple sites though, this is often unnecessary: one or two layout files will suffice, and an &lt;code&gt;_includes&lt;/code&gt; directory is not so useful with only a single layout. The ease of maintenance by having a template divided into modular chunks is somewhat trumped by the greater simplicity of copying a single template or set of templates over into a new directory.&lt;/p&gt;
&lt;p&gt;And deploying Jekyll could be easier, particularly with pandoc as the parser. Despite plugins that nicely let pandoc act just like the built-in parsers and a CI setup with Travis to support automated building of my site on push, setting these components up repeatedly on every new repository is a bit tedious. Occassional updates of Jekyll and related gems have also broken my build pipeline more than once, though this is less of an issue now that I’ve added bundler and a Gemfile to restrict gem versions and provide a Dockerized setup for local deploying. These things keep the overhead low for my main site, but are an overhead to replicate.&lt;/p&gt;
&lt;p&gt;Meanwhile, I’ve found Pandoc’s templating system to be immensely powerful, particularly with the yaml headers now supported. To provide a lightweight way to deploy a website on a gh-pages branch of a new repository, I’ve found this system works quite well. I’ve illustrated this on my &lt;a href=&quot;https://github.com/cboettig/tree/gh-pages&quot;&gt;gh-pages branch of my template&lt;/a&gt; repository. Previously, this used Jekyll with the built-in redcarpet markdown parser to deploy markdown files in a style consistent with my notebook.&lt;/p&gt;
&lt;p&gt;Now, I’ve stripped this down to simply use a pandoc template, pandoc YAML, and a Makefile to accomplish much the same thing.&lt;/p&gt;
&lt;p&gt;I was dissapointed to see that the &lt;code&gt;_output.yaml&lt;/code&gt; used by rmarkdown for building multi-page websites did not leverage the generic &lt;code&gt;metadata.yaml&lt;/code&gt; approach already built into pandoc. This prevents us specifying custom generic metadata the way one does in Jekyll with &lt;code&gt;_config.yaml&lt;/code&gt;, as I describe in &lt;a href=&quot;https://github.com/rstudio/rmarkdown/issues/297&quot;&gt;rmarkdown#297&lt;/a&gt; I can work around this with the Makefile by calling pandoc manually with the additional &lt;code&gt;metadta.yaml&lt;/code&gt; file, as follows:&lt;/p&gt;
&lt;pre class=&quot;make&quot;&gt;&lt;code&gt;%.html: %.Rmd
  R --vanilla --slave -e &amp;quot;knitr::knit(&amp;#39;$&amp;lt;&amp;#39;)&amp;quot;
  pandoc --template _layouts/default.html metadata.yaml -o $@ $(basename $&amp;lt;).md
  rm $(basename $&amp;lt;).md

%.html: %.md
  pandoc --template _layouts/default.html metadata.yaml -o $@ $&amp;lt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;Rmd&lt;/code&gt; building is somewhat more cumbersome since we have to bypass &lt;code&gt;rmarkdown:render&lt;/code&gt; for this to work.&lt;/p&gt;
&lt;p&gt;I had to collapse all my &lt;code&gt;_includes&lt;/code&gt; and nested &lt;code&gt;_layouts&lt;/code&gt; into a single &lt;code&gt;layout&lt;/code&gt;, replace the Jekyll Liquid blocks, &lt;code&gt;{{&lt;/code&gt; with pandoc-template &lt;code&gt;$&lt;/code&gt; ones, and write out a basic &lt;code&gt;metadata.yaml&lt;/code&gt; file, and things are &lt;a href=&quot;http://io.carlboettiger.info/template/&quot;&gt;good to go&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Nonetheless, I sometimes wish I could break templates into more re-usable components; similar to the the way &lt;code&gt;_includes&lt;/code&gt; provides re-usable components for the templates specified in the &lt;code&gt;_layouts&lt;/code&gt; directory of a jekyll site.&lt;/p&gt;
&lt;p&gt;My first thought was to simply add the re-usable elements into a metadata block itself. (This seemed particularly promising since we can already have an external metadata.yaml to provide a metadata block we can use across multiple file). However, it seems that Pandoc always escape the html contents in my yaml metadata. For instance, if I add the block:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
header: |
    &amp;lt;header class=&amp;quot;something&amp;quot;&amp;gt;&amp;lt;h1&amp;gt;$title$&amp;lt;/h1&amp;gt;&amp;lt;/header&amp;gt;
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then in my template add &lt;code&gt;$header$&lt;/code&gt;, I get the above block but with all the angle brackets escaped. I had thought since I have denoted this as a literal block with ‘|’ in the yaml I would get this block unaltered. How can I prevent pandoc from escaping the html? (I realize that still wouldn’t parse the &lt;code&gt;$title$&lt;/code&gt; metadata, but that’s a separate issue).&lt;/p&gt;
&lt;p&gt;The other approach I considered is to exploit the &lt;code&gt;--include-before-body&lt;/code&gt; and &lt;code&gt;--include-after-body arguments&lt;/code&gt;. While more limited since I am restricted to these two variables, this approach does allow me to specify a file with a re-usable component block and avoids the issue of HTML escaping observed above. Other than the limit of two such variables, the other limit to this approach is that metadata elements like &lt;code&gt;$title$&lt;/code&gt; are processed only in templates, not in files.&lt;/p&gt;
&lt;p&gt;It seems like pandoc is thus really close to being able to support templates that are made from re-usable blocks rather than completely specified from scratch, but not quite there. I realize pandoc isn’t trying to be a replacement for static website generation, but still feel that re-usable blocks would make the existing template system a bit more flexible and user-friendly.&lt;/p&gt;
&lt;p&gt;Quite a few of pandoc’s current functions already approximate this behavior in a hard-wired fashion; e.g. &lt;code&gt;$highlight-css$&lt;/code&gt; uses the &lt;code&gt;--highlight-style&lt;/code&gt; option to select among a bunch of pre-defined highlight blocks. Thus I suspect pandoc might be easier to extend in the future if such features could just be added through an include mechanism rather than this hardwired approach.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See this as a query to &lt;a href=&quot;https://groups.google.com/forum/#!topic/pandoc-discuss/pe63zLmNwtk&quot;&gt;pandoc-discuss&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Docker And User Permissions Crazyness</title>
	 <link href="/2014/10/21/docker-and-user-permissions-crazyness.html"/>
   <updated>2014-10-21T00:00:00+00:00</updated>
   <id>/10/21/docker-and-user-permissions-crazyness</id>
   <content type="html">&lt;p&gt;Lots of crazyness getting to the bottom of permissions changes, as discussed in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/rocker-org/rocker/issues/50&quot;&gt;rocker issues tracker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/26500270&quot;&gt;Stackoverflow question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!topic/docker-user/VFdFuZ4Ze_A&quot;&gt;Docker mailing list&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Long story short: docker cares only about UIDs, so we have to explicitly make sure these match. Some very good answers including from Docker core-team members on the discussion list. Overall approach outlined at the end of the rocker issues tracker.&lt;/p&gt;
&lt;p&gt;Here’s the SO version of the question, for my reference:&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Consider the following trivial Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM debian:testing
RUN  adduser --disabled-password --gecos &amp;#39;&amp;#39; docker
RUN  adduser --disabled-password --gecos &amp;#39;&amp;#39; bob &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in a working directory with nothing else. Build the docker image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t test .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then run a bash script on the container, linking the working directory into a new subdir on bob’s home directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v $(pwd):/home/bob/subdir test &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Who owns the contents of &lt;code&gt;subdir&lt;/code&gt; on the container? On the container, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /home/bob/subdir
ls -l&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ad we see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-rw-rw-r-- 1 docker docker 120 Oct 22 03:47 Dockerfile&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Holy smokes! &lt;code&gt;docker&lt;/code&gt; owns the contents! Back on the host machine outside the container, we see that our original user still owns the &lt;code&gt;Dockerfile&lt;/code&gt;. Let’s try and fix the ownership of &lt;code&gt;bob&lt;/code&gt;’s home directory. On the container, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;chown -R bob:bob /home/bob
ls -l &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-rw-rw-r-- 1 bob bob 120 Oct 22 03:47 Dockerfile&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait! outside the container, we now run &lt;code&gt;ls -l&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-rw-rw-r-- 1 1001 1001 120 Oct 21 20:47 Dockerfile&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we no longer own our own file. Terrible news!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;If we had only added one user in the above example, everything would have gone more smoothly. For some reason, Docker seems to be making any home directory owned by the &lt;em&gt;first&lt;/em&gt; non-root user it encounters (even if that user is declared on an earlier image). Likewise, this &lt;em&gt;first&lt;/em&gt; user is the one that corresponds to the same ownership permissions as my home user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt; Is that correct? Can someone point me to documentation of this, I’m just conjecturing based on the above experiment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt;: Perhaps this is just because they both have the same numerical value on the kernel, and if I tested on a system where my home user was not id &lt;code&gt;1000&lt;/code&gt; then permissions would get changed in every case?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt;: The real question is, of course, ‘what do I do about this?’ If &lt;code&gt;bob&lt;/code&gt; is logged in as &lt;code&gt;bob&lt;/code&gt; on the given host machine, he should be able to run the container as &lt;code&gt;bob&lt;/code&gt; and not have file permissions altered under his host account. As it stands, he actually needs to run the container as user &lt;code&gt;docker&lt;/code&gt; to avoid having his account altered.&lt;/p&gt;
&lt;p&gt;I hear you asking &lt;em&gt;Why do I have such a weird Dockerfile anyway?&lt;/em&gt;. I wonder too sometimes. I am writing a container for a webapp (RStudio-server) that permits different users to log in, which simply uses the user names and credentials from the linux machine as the valid user names. This brings me the perhaps unusual motivation of wanting to create multiple users. I can get around this by creating the user only at runtime and everthing is fine. However, I use a base image that has added a single &lt;code&gt;docker&lt;/code&gt; user so that it can be used interactively without running as root (as per best practice). This ruins everything since that user becomes the &lt;em&gt;first&lt;/em&gt; user and ends up owning everything, so attempts to log on as other users fail (the app cannot start because it lacks write permissions). Having the startup script run &lt;code&gt;chown&lt;/code&gt; first solves this issue, but at the cost of linked volumes changing permissions (obviously only a problem if we are linking volumes).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/10/20/notes.html"/>
   <updated>2014-10-20T00:00:00+00:00</updated>
   <id>/10/20/notes</id>
   <content type="html">&lt;p&gt;Keep thinking about this quote from Jeroen Oom’s &lt;a href=&quot;http://arxiv.org/abs/1406.4806&quot;&gt;recent piece on the arxiv&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The role and shape of data is the main characteristic that distinguishes scientific computing. In most general purpose programming languages, data structures are instances of classes with well-defined fields and methods. […] Strictly defined structures make it possible to write code implementing all required operations in advance without knowing the actual content of the data. &lt;em&gt;It also creates a clear separation between developers and users&lt;/em&gt; [emphasis added]. Most applications do not give users direct access to raw data. Developers focus in implementing code and designing data structures, whereas users merely get to execute a limited set of operations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This paradigm does not work for scientific computing. Developers of statistical software have relatively little control over the structure, content, and quality of the data. Data analysis starts with the user supplying a dataset, which is rarely pretty. Real world data come in all shapes and formats. They are messy, have inconsistent structures, and invisible numeric properties. Therefore statistical programming languages define data structures relatively loosely and instead implement a rich lexicon for interactively manipulating and testing the data. Unlike software operating on well-defined data structures, it is nearly impossible to write code that accounts for any scenario and will work for every possible dataset. Many functions are not applicable to every instance of a particular class, or might behave differently based on dynamic properties such as size or dimensionality. &lt;em&gt;For these reasons there is also less clear of a separation between developers and users in scientific computing.&lt;/em&gt; The data analysis process involves simultaneously debugging of code and data where the user iterates back and forth between manipulating and analyzing the data. Implementations of statistical methods tend to be very flexible with many parameters and settings to specify behavior for the broad range of possible data. And still the user might have to go through many steps of cleaning and reshaping to give data the appropriate structure and properties to perform a particular analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Good inspiration for this week’s assignment:&lt;/p&gt;
&lt;h2 id=&quot;notes-for-swc-training-short-motivational-pitch-on-r&quot;&gt;Notes for SWC training: short motivational pitch on R&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;What’s the difference between a novice programmer and a professional programmer?&lt;br /&gt;The novice pauses a moment before doing something stupid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this course, we’ll be learning about the R programming environment. You may already have heard of R or have used it before.&lt;/p&gt;
&lt;p&gt;It’s the number one language for statistical programming. It’s used by most major companies, putting these skills in high demand.&lt;/p&gt;
&lt;p&gt;Recently DICE Magazine showed that programmers with expertise in R topped the tech survey salary charts (at $115,531, above Hadoop, MapReduce, C, or Cloud, mobile or UI/UX design).&lt;/p&gt;
&lt;p&gt;You may not (always) like the R programming environment. The syntax is often challenging, it can do counterintuitive things, and it’s terrible at mind reading.&lt;/p&gt;
&lt;p&gt;Try not to take this out on your hardware. Hardware is expensive.&lt;/p&gt;
&lt;p&gt;R’s strength lies in data.&lt;/p&gt;
&lt;p&gt;No other major language has key statistical concepts like ‘missing data’ baked in at the lowest level.&lt;/p&gt;
&lt;p&gt;We will not be approaching R as a series of recipes to perform predifined tasks.&lt;/p&gt;
&lt;p&gt;In scientific research we can seldom predict just what our data will look like in advance or what our analysis will require. This makes it impossible to always rely on pre-made software and graphical interfaces you may be familar with, and blurs the lines between &lt;em&gt;users&lt;/em&gt; &lt;em&gt;developers&lt;/em&gt;. Unlike other languages such as C, Java or Python that are often used to build ‘end-user’ software in which the underlying language is completely invisible, R does not make this distnction. R gives a lto of power to the end user. And with great power comes great responsibility.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Gitlab And Other Configuration Notes</title>
	 <link href="/2014/10/16/gitlab-and-other-configuration-notes.html"/>
   <updated>2014-10-16T00:00:00+00:00</updated>
   <id>/10/16/gitlab-and-other-configuration-notes</id>
   <content type="html">&lt;p&gt;Updating gitlab setup:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; pull sameersbn/redis:latest
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; pull sameersbn/gitlab:7.3.2-1
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; pull sameersbn/postgresql:latest


&lt;span class=&quot;kw&quot;&gt;mkdir&lt;/span&gt; -p /opt/gitlab/data
&lt;span class=&quot;kw&quot;&gt;mkdir&lt;/span&gt; -p /opt/postgresql/data

&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=postgresql -d \
  -e &lt;span class=&quot;st&quot;&gt;&amp;#39;DB_NAME=gitlabhq_production&amp;#39;&lt;/span&gt; -e &lt;span class=&quot;st&quot;&gt;&amp;#39;DB_USER=gitlab&amp;#39;&lt;/span&gt; -e &lt;span class=&quot;st&quot;&gt;&amp;#39;DB_PASS=password&amp;#39;&lt;/span&gt; \
  -v /opt/postgresql/data:/var/lib/postgresql \
  sameersbn/postgresql:latest
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=redis -d sameersbn/redis:latest

&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=gitlab -d \
  --link postgresql:postgresql \
  --link redis:redisio \
  -p 10080:80 -p 10022:22 \
  -v /opt/gitlab/data:/home/git/data \
    sameersbn/gitlab:7.3.2-1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More consisely, do this with &lt;a href=&quot;http://fig.sh&quot;&gt;fig&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;sourceCode yaml&quot;&gt;&lt;code class=&quot;sourceCode yaml&quot;&gt;&lt;span class=&quot;fu&quot;&gt;gitlab:&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;image:&lt;/span&gt; sameersbn/gitlab:7.3.2-1
  &lt;span class=&quot;fu&quot;&gt;links:&lt;/span&gt;
   &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; postgres
   &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;redis:&lt;/span&gt;redisio
  &lt;span class=&quot;fu&quot;&gt;ports:&lt;/span&gt;
   &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;10080:80&amp;quot;&lt;/span&gt;
   &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;10022:22&amp;quot;&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;volumes:&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;/opt/gitlab/data:&lt;/span&gt;/home/git/data
  &lt;span class=&quot;fu&quot;&gt;environment:&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; SMTP_USER=USER@gmail.com
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; SMTP_PASS=PASSWORD

&lt;span class=&quot;fu&quot;&gt;postgres:&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;image:&lt;/span&gt; postgres:latest
  &lt;span class=&quot;fu&quot;&gt;volumes:&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;/opt/postgresql/data:&lt;/span&gt;/var/lib/postgresql
  &lt;span class=&quot;fu&quot;&gt;environment:&lt;/span&gt;
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; POSTGRESQL_USER=gitlab
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; POSTGRESQL_PASS=
    &lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; POSTGRESQL_DB=gitlabhq_production
&lt;span class=&quot;fu&quot;&gt;redis:&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;image:&lt;/span&gt; redis:2.8.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hmm… memory error using the &lt;code&gt;fig&lt;/code&gt; approach; doesn’t happen when running individual containers as above…&lt;/p&gt;
&lt;p&gt;Looks like we have to run the original version if we want to keep our database. But no go since sql database information is lost:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; pull gitlab:7.2.1-1
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=gitlab -d \
  -p 10080:80 -p 10022:22 \
  -v /opt/gitlab/data:/home/git/data \
    sameersbn/gitlab:7.2.1-1
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; stop gitlab
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --rm -it \
  -p 20080:80 -p 20022:22 \
  -v /opt/gitlab/data:/home/git/data \
    sameersbn/gitlab:7.2.1-1 app:rake gitlab:backup:restore
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; restart gitlab&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;drone&quot;&gt;Drone&lt;/h2&gt;
&lt;p&gt;Ah, Drone now provides their own Dockerfile, which we can grab and build for the latest Drone:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;git&lt;/span&gt; clone https://github.com/drone/drone.git
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; build -t drone/drone drone&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can run, linking volumes appropriately:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name drone -d -p 88:80 \
-v /var/run/docker.sock:/var/run/docker.sock \
-t \
-e DRONE_GITHUB_CLIENT=&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;clientkey&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; \
-e DRONE_GITHUB_SECRET=&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;secretkey&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; \
drone/drone&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this doesn’t work with a &lt;code&gt;drone.toml&lt;/code&gt; file even when linking volumes etc., see &lt;a href=&quot;https://github.com/drone/drone/issues/580&quot;&gt;#580&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also note that this setup shares docker images with the host machine, rather than having a seperate library, which is rather good for saving space. I believe this should be trivial to back-up (just by exporting the container), but have to test that stil.&lt;/p&gt;
&lt;p&gt;These rather verbose docker calls for drone and gitlab make a great use-case for fig. Unfortunately, fig seems to crash out of memory on my tiny DO droplet, but running these commands manually works like a charm.&lt;/p&gt;
&lt;h2 id=&quot;digitalocean&quot;&gt;DigitalOcean&lt;/h2&gt;
&lt;p&gt;Ooh: configure scripts for starting DO droplets. e.g. automate the launch of &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-use-cloud-config-for-your-initial-server-setup&quot;&gt;a more secure configuration&lt;/a&gt;, looks like a more formal way than my shell script. /ht &lt;span class=&quot;citation&quot; data-cites=&quot;hadley&quot;&gt;@hadley&lt;/span&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Rocker Versioning</title>
	 <link href="/2014/10/14/rocker-versioning.html"/>
   <updated>2014-10-14T00:00:00+00:00</updated>
   <id>/10/14/rocker-versioning</id>
   <content type="html">&lt;p&gt;Been looking into building versioned images for previous R releases using Docker, based on somewhat common requests to our recently begun &lt;a href=&quot;http://github.com/rocker-org&quot;&gt;rocker&lt;/a&gt; project. Versioning is under early development and the best way to go about this is not yet clear. Getting the correct version of R installed is not always trivial but is relatively straight forward, and I outline two approaches below.&lt;/p&gt;
&lt;p&gt;Getting the correct version of packages (or even merely any compatible version of the package) to install is a considerably more difficult problem, which I’ll discuss later.&lt;/p&gt;
&lt;p&gt;We’re is considering two different strategies, each with strengths and weaknesses:&lt;/p&gt;
&lt;h2 id=&quot;compiled-builds&quot;&gt;Compiled builds&lt;/h2&gt;
&lt;p&gt;The most straight-forward recipe seems to be to adapt the &lt;code&gt;rocker/r-devel&lt;/code&gt; file to compile the desired version by pulling from the appropriate tag in the R SVN repository, as suggested by &lt;span class=&quot;citation&quot; data-cites=&quot;eddelbuettel&quot;&gt;@eddelbuettel&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As Dirk suggested, we can build on the r-devel recipe, simply by pointing to the appropriate tag. Occasionally this needs a few extra packages added to the list. I was able to get R 2.0.0 to compile, but not R 1.0.0. More recent versions than 2.0.0 don’t seem to pose any difficulty for compiling. Nonetheless, installing additional packages is still an issue.&lt;/p&gt;
&lt;h2 id=&quot;binary-builds&quot;&gt;Binary builds&lt;/h2&gt;
&lt;p&gt;At different approach is to use the binary versions from old Debian images. This approach works rather well when docker images are available for earlier Debian releases (&lt;code&gt;oldstable&lt;/code&gt; and &lt;code&gt;stable&lt;/code&gt;, which currently go back as far as Debian &lt;code&gt;6.0&lt;/code&gt; and &lt;code&gt;7.0&lt;/code&gt;; while the main rocker release builds on Debian &lt;code&gt;testing&lt;/code&gt; which is at &lt;code&gt;8.0&lt;/code&gt;). Merely using the earlier Debian releases, we can jump back to certain versions of R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6.0 : R 2.11.1&lt;/li&gt;
&lt;li&gt;7.0 : R 2.15.1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The advantage of this is that binary forms of many common R packages may also be available from the same repositories.&lt;/p&gt;
&lt;p&gt;Dirk &lt;span class=&quot;citation&quot; data-cites=&quot;eddelbuettel&quot;&gt;@eddelbuettel&lt;/span&gt; also suggests looking at &lt;a href=&quot;http://snapshot.debian.org/&quot;&gt;Debian snapshot archive&lt;/a&gt; binaries. This allows us to install intermediate versions of R in binary form, as well as specific versions of any package for which debian binaries have been built. The brilliant bit about this is that we can add any particular snapshot time-period as a normal repository, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deb     http://snapshot.debian.org/archive/debian/20091004T111800Z/ lenny main
deb-src http://snapshot.debian.org/archive/debian/20091004T111800Z/ lenny main
deb     http://snapshot.debian.org/archive/debian-security/20091004T121501Z/ lenny/updates main
deb-src http://snapshot.debian.org/archive/debian-security/20091004T121501Z/ lenny/updates main&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the package manage can thus handle all the dependencies. As noted, this works only for those packages for which we have debian binaries available in the release.&lt;/p&gt;
&lt;p&gt;This is limited to what we can use as a base image, particularly for old versions of R where the binaries are only available for i386 architectures (while there are some Docker images providing i386 architectures, we’ve so far used only amd64 versions). Given the rapid growth of R however, it is likely that the preponderance of use-cases will focus on relatively recent R versions.&lt;/p&gt;
&lt;p&gt;Unfortunately, I haven’t gotten this working yet (See issue &lt;a href=&quot;https://github.com/rocker-org/rocker-versioned/issues/2&quot;&gt;#2&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;installing-r-packages&quot;&gt;Installing R packages&lt;/h2&gt;
&lt;p&gt;Installing packages directly from CRAN is more dubious. We may be able to install earlier versions of particular packages from the CRAN archives using the CRAN data from &lt;a href=&quot;https://github.com/metacran/crandb&quot;&gt;metacran/crandb&lt;/a&gt; as &lt;span class=&quot;citation&quot; data-cites=&quot;hadley&quot;&gt;@hadley&lt;/span&gt; recommended.&lt;/p&gt;
&lt;p&gt;Hoping that we can do this more generally by building on &lt;span class=&quot;citation&quot; data-cites=&quot;gmbecker&quot;&gt;@gmbecker&lt;/span&gt; ’s work, which does just this using R versions built as Amazon EC2 AMIs. (See issue &lt;a href=&quot;https://github.com/rocker-org/rocker-versioned/issues/1&quot;&gt;#1&lt;/a&gt;).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Lessons Learned In Writing Dockerfiles</title>
	 <link href="/2014/10/09/lessons-learned-in-writing-dockerfiles.html"/>
   <updated>2014-10-09T00:00:00+00:00</updated>
   <id>/10/09/lessons-learned-in-writing-dockerfiles</id>
   <content type="html">&lt;p&gt;Writing dockerfiles is pretty straight forward. Nevertheless, a little extra care goes a long way. Docker’s own &lt;a href=&quot;https://docs.docker.com/articles/dockerfile_best-practices/&quot;&gt;Best Practices&lt;/a&gt; are a great starting point, covering everything from formatting to use of certain commands. In Rocker, We’ve tried to follow all of these suggestions and have found them very helpful. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Minimize the number of layers, but use &lt;code&gt;\&lt;/code&gt; to break commands across multiple lines,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Always run &lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y ...&lt;/code&gt; rather than running updates in one layer and install in a different layer,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;COPY&lt;/code&gt; instead of &lt;code&gt;ADD&lt;/code&gt;, &lt;code&gt;WORKDIR&lt;/code&gt; instead of &lt;code&gt;cd&lt;/code&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a non-root user&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was tricky since we intend (and use) our dockerfiles as base images for other Dockerfiles. Setting the default user with &lt;code&gt;USER&lt;/code&gt; would interfere with this (adding that user to sudoers, having to use &lt;code&gt;sudo apt-get update&lt;/code&gt; or switch back to &lt;code&gt;USER root&lt;/code&gt; and back again. Both &lt;code&gt;sudo&lt;/code&gt; and switching back and forth on &lt;code&gt;USER&lt;/code&gt; are discouraged in best practices)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Response To Software Discovery Index Report</title>
	 <link href="/2014/10/08/response-to-software-discovery-index-report.html"/>
   <updated>2014-10-08T00:00:00+00:00</updated>
   <id>/10/08/response-to-software-discovery-index-report</id>
   <content type="html">&lt;p&gt;The NIH has recently announced the &lt;a href=&quot;http://softwarediscoveryindex.org/report&quot;&gt;report&lt;/a&gt; of a landmark meeting which presents a vision for a &lt;em&gt;Software Discovery Index&lt;/em&gt; (SDI). The report is both timely and focused on the key issues of locating, citing, reusing software:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Software developers face challenges disseminating their software and measuring its adoption. Software users have difficulty identifying the most appropriate software for their work. Journal publishers lack a consistent way to handle software citations or to ensure reproducibility of published findings. Funding agencies struggle to make informed funding decisions about which software projects to support, while reviewers have a hard time understanding the relevancy and effectiveness of proposed software in the context of data management plans and proposed analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To address this, they propose an Index which would do three things:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;to assign standard and unambiguous identifiers to reference all software,&lt;/li&gt;
&lt;li&gt;to track specific metadata features that describe that software, and&lt;/li&gt;
&lt;li&gt;to enable robust querying of all relevant information for users.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The report is both timely and focused on key issues confronting our community, including the challenges of identifying, citing, and reusing software. The appendices do an excellent job in outlining key metadata, metrics, and use cases which help frame the discussion. The proposal does well to focus on the importance of identifiers and the creation of a query-able metadata index for research software, but leaves out an essential element necessary to make this work.&lt;/p&gt;
&lt;p&gt;This proposal sounds very much like the CrossRef and DataCite infrastructure already in place for academic literature and data, respectively; and indeed this is an excellent model to follow. However, a key piece of that infrastructure is missing from the present proposal – the social contract between repository or publisher and the index itself.&lt;/p&gt;
&lt;p&gt;CrossRef provides unique identifiers for the academic literature (CrossRef DOIs), but it also defines specific metadata that describe that literature (as well as metrics of its use), and embed that information into a robust, query-able, machine-readable format. DataCite does the same for scientific data. These are exactly the features that the authors of the report seek to emulate.&lt;/p&gt;
&lt;p&gt;Just as CrossRef itself does not host academic papers but only the metadata records, the SDI does not propose to host software itself. This introduces a substantial challenge in &lt;em&gt;maintaining the link&lt;/em&gt; between the metadata and the software itself. The authors have simply proposed that the metadata include “Links to the code repository.” If CrossRef or DataCite DOIs worked in this way, we would soon loose all ability to recover many of the papers or the data itself, and we would be left with only access to the metadata record and a broken link. DOIs were created explicitly to solve this problem, not through technology, but through a &lt;em&gt;social contract&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The scientific publishers who host the actual publications are responsible for ensuring that this link is always maintained when they change names, etc. Should the publisher go out of business, these links may be adjusted to point to a new home, such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;. This guarantees that the DOI always resolves to the resource in question, regardless of where it moves. Should a publisher fail to maintain these links, CrossRef may refuse to provide the publisher any additional DOIs, cutting it off from this key index. This is the social contract. Data repositories work in exactly the same way, purchasing their DOIs from DataCite. (While financial transaction isn’t strictly necessary for the financial contract, it provides a clear business model for maintaining the key organization responsible for the index).&lt;/p&gt;
&lt;p&gt;Without such a mechanism, links in the SDI would surely rot away, all the more rapidly in the fast-moving world of software. Without links to the software itself, the function of the index would be purely academic. Yet such a mechanism requires that the software repositories, not the individual software authors, would be willing to accept the same social contract, receiving (and possibly paying for) identifiers on the condition that they assume the responsibility of maintaining the links. It is unclear that the primary software repositories in use to day (Sourceforge, Github, Bitbucket, etc) would be willing to accept this.&lt;/p&gt;
&lt;p&gt;Data repositories already offer many of the compelling features of this proposal. Many data repositories accept a wide array of file formats including software packages, and would provide such software with a permanent unique identifier in the form of a DataCite DOI, as well as collecting much of the essential metadata listed in report’s Appendix 1, which would then already be accessible through the DataCite API in a nice machine-readable format. This strategy finds several aspects wanting.&lt;/p&gt;
&lt;p&gt;The primary barrier to using data repositories indexed by DataCite arises from the dynamic nature of software relative to data. Data repositories are designed to serve relatively static content with few versions. Software repositories, by contrast, are usually built upon explicit version control platforms such as Git or Subversion designed explicitly for handling continual changes, including branches and mergers, of software code. The report discusses the challenges of software versions as a reason for that citing a software paper as a proxy for citing software is not ideal: the citation to the paper does not convey what version was used. Rapid versioning creates other problems though, both in the number of identifiers that might be created (is each commit a new identifier?) and defining the relationship between different versions of the same software. Branches and merges exacerbate this problem. Existing approaches that provide the user a one-time way to import software from a software repository to a data repository such as those cited in the report (“One significant initiative is a collaboration between Mozilla, figshare, GitHub, and Zenodo”) do nothing to address this issues.&lt;/p&gt;
&lt;p&gt;Less challenging issues involve resolving differences between DataCite metadata and the proposed metadata records for software. Most obviously, the metadata would need a way to declare the object involved software instead of data per se, which would thus allow queries to restrict results to ‘software’ objects to avoid cluttering searches. Ideally, one would also create tools that can import such metadata from the format in which it is usually already defined in software, into the desired format of the index, rather than requiring manual double-entry of this information. These are important but more straight-forward problems which the report already seeks to address.&lt;/p&gt;
&lt;hr /&gt;
&lt;!-- comments
Ilya raises the question: Why not just use Github? I think it is important to note that:

a) Github isn&#39;t forever, repositories come and go all the time, or move to new links, etc

b) Re-creating and running an NIH-Github would be both expensive (Gitlab notwithstanding) and redundant -- researchers would continue to use Github etc.

c) Github provides somewhat limited query-able metadata, that doesn&#39;t capture even the minimal list of fields suggested by the report.
Leveraging existing scientific data repositories by linking them to versioned releases on software repositories addresses each of these problems.

--&gt;
&lt;!--
A. FRAMEWORK SUPPORTING THE SOFTWARE DISCOVERY INDEX
Unique identifiers
Connections to publishers
Use cases
Complementarity with the Data Discovery Index
B. CHALLENGES AND REMAINING QUESTIONS
Defining relevant software
Integrating with other repositories
Evaluating progress and distinguishing this from other efforts
C. IMPLEMENTATION ROADMAP
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/10/07/notes.html"/>
   <updated>2014-10-07T00:00:00+00:00</updated>
   <id>/10/07/notes</id>
   <content type="html">&lt;h2 id=&quot;misc-docker-notes&quot;&gt;Misc Docker Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;funny business with Locales, see &lt;a href=&quot;https://github.com/eddelbuettel/rocker/issues/19&quot;&gt;#19&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;misc-notes-on-littler&quot;&gt;Misc notes on &lt;code&gt;littler&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;INSTALL&lt;/code&gt; page is outdated. Instead, do:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;
&lt;span class=&quot;kw&quot;&gt;apt-get&lt;/span&gt; update \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;apt-get&lt;/span&gt; build-dep -y littler \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;apt-get&lt;/span&gt; install autoconf git \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;git&lt;/span&gt; clone https://github.com/eddelbuettel/littler.git \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;/littler/./bootstrap&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;May want to symlink too:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ln&lt;/span&gt; -s /littler/examples/install.r /usr/local/bin/install.r \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;ln&lt;/span&gt; -s /littler/examples/install2.r /usr/local/bin/install2.r \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;ln&lt;/span&gt; -s /littler/examples/installGithub.r /usr/local/bin/installGithub.r \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;  &lt;span class=&quot;kw&quot;&gt;ln&lt;/span&gt; -s /littler/examples/testInstalled.r /usr/local/bin/testInstalled.r \
&lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;make&lt;/span&gt; clean&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but if linking is all we need, probably better to just pull those new scripts. This installs a lot of additional things we don’t need (build-deps, git history of littler, etc).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note proper use of &lt;code&gt;autoconf&lt;/code&gt; to generate the &lt;code&gt;configure&lt;/code&gt; script from &lt;code&gt;configure.ac&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/10/02/notes.html"/>
   <updated>2014-10-02T00:00:00+00:00</updated>
   <id>/10/02/notes</id>
   <content type="html">&lt;h2 id=&quot;docker&quot;&gt;docker&lt;/h2&gt;
&lt;p&gt;2014-09-29:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discussion with Dirk on repositories, library paths, versions.&lt;/li&gt;
&lt;li&gt;library paths: apt-get users the &lt;code&gt;usr/lib&lt;/code&gt; path, while user-run install commands (e.g. &lt;code&gt;install.packages&lt;/code&gt;) uses &lt;code&gt;usr/local/lib/&lt;/code&gt;, path. Dirk recommends that &lt;code&gt;/usr/local/lib/R/site-library&lt;/code&gt; is configured to be user-writable for package installation, rather than installing into home.&lt;/li&gt;
&lt;li&gt;building directly from CRAN&lt;/li&gt;
&lt;li&gt;building dependencies: &lt;code&gt;apt-get build-dep&lt;/code&gt;, needs the corresponding &lt;code&gt;deb-src&lt;/code&gt; lines.&lt;/li&gt;
&lt;li&gt;issues and tweaks to littler see &lt;a href=&quot;https://github.com/eddelbuettel/littler/pull/2&quot;&gt;PR #2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2014-10-01:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discussion on &lt;a href=&quot;https://github.com/ropensci/docker/issues/5&quot;&gt;minimal images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Discussion on &lt;a href=&quot;https://github.com/sckott/analogsea/issues/47&quot;&gt;analogsea + docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Blog &lt;a href=&quot;http://www.magesblog.com/2014/09/running-rstudio-via-docker-in-cloud.html&quot;&gt;coverage&lt;/a&gt; of Dirk’s talk on our Docker work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/boot2docker/boot2docker-cli&quot;&gt;boot2docker-cli&lt;/a&gt; includes linux flavors, so I might get a look at what the docker experience feels like for those poor souls who can only live it through full virutalization. No go on the install methods listed, but the binary seems to work. Unfortunately my laptop cannot run 64 bit virtualbox…&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2014-10-02:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.docker.com/articles/dockerfile_best-practices/&quot;&gt;Official Dockerfile Best Practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See &lt;code&gt;rocker&lt;/code&gt; commit log and issues log.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2014-09-29:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Carsten’s suggestion re docker registeries, is this something that scientific repositories might one day support? (Excerpt from my reply post):&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;While I see your point that the Docker Hub might not be ideal for all cases, I think the most important attribute of a repository should be longevity. Certainly Docker Hub won’t be around forever, but at this stage with 60 million in it’s latest VC round it’s likely to be more long-lasting than anything that a small organization like rOpenSci would host. It would be great to see existing scientific repositories show an interest in archiving images in this way though, since organizations like DataONE and Dryad already have recognition in the scientific community and better discoverability / search / metadata features. Building of the docker registry technology would of course make a lot more sense than just archiving static binary docker images, which lack both the space saving features and the ease of download / integration that examples like the docker registry have.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I think it would be interesting to bring that discussion to some of the scientific data repositories and see what they say. Of course there’s the chicken &amp;amp; egg problem in that most researchers have never heard of docker. Would be curious what others think of this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;on &lt;a href=&quot;http://theincidentaleconomist.com/wordpress/how-do-you-do-it-all/&quot;&gt;productivity&lt;/a&gt;; surprisingly resonates with me.&lt;/li&gt;
&lt;li&gt;SWC discussion on teaching material in semester courses. In particular see &lt;a href=&quot;http://www.programmingforbiologists.org&quot;&gt;Ethan White’s course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SWC and MozillaScience mention starting a repo for &lt;a href=&quot;https://github.com/mozillascience/codeReview&quot;&gt;code review best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Katz slides: &lt;a href=&quot;http://www.slideshare.net/danielskatz/valuing-software-and-other-research-outputs&quot;&gt;valuing software and other research outputs&lt;/a&gt; &lt;!-- Query to Dan Katz --&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/09/24/notes.html"/>
   <updated>2014-09-24T00:00:00+00:00</updated>
   <id>/09/24/notes</id>
   <content type="html">&lt;h2 id=&quot;rocker-docker&quot;&gt;rocker / docker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Talking over strategy with Dirk, see summary in &lt;a href=&quot;https://github.com/eddelbuettel/rocker/issues/1&quot;&gt;#1&lt;/a&gt; and follow-up issues, &lt;a href=&quot;https://github.com/eddelbuettel/rocker/issues/3&quot;&gt;#3&lt;/a&gt;, &lt;a href=&quot;https://github.com/eddelbuettel/rocker/issues/4&quot;&gt;#4&lt;/a&gt;, &lt;a href=&quot;https://github.com/eddelbuettel/rocker/issues/5&quot;&gt;#5&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rdataone-eml&quot;&gt;rdataone &amp;amp; EML&lt;/h2&gt;
&lt;p&gt;Trying to fix travis issues. Much craziness.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;travis.sh&lt;/code&gt; doesn’t provide notes on setup for repos where the R package is in a subdirectory. Looks like &lt;code&gt;cd&lt;/code&gt; commands are persistent though throughout a travis file. okay.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;dataone&lt;/code&gt; is imported by EML but suggested by &lt;code&gt;dataone&lt;/code&gt;. Since &lt;code&gt;install_github&lt;/code&gt; likes to install the suggests list, this creates problems:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;travis.sh install_github&lt;/code&gt; has no way to indicate that I don’t want to install suggests list.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;install_github(&amp;quot;DataONEorg/dataone/dataone&amp;quot;, dependencies=NA)&lt;/code&gt; should get around this by not installing the suggested EML package when trying to install &lt;code&gt;dataon&lt;/code&gt;. For reasons inexplicable to me, that doesn’t seem to work(!), at least on travis. I’ve had to remove the package from the suggests list.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The build environment often seems a lot more fragile than the package itself. EML travis builds were rather badly broken with both &lt;code&gt;dataone&lt;/code&gt; and &lt;code&gt;rrdf&lt;/code&gt; disappearing from CRAN. If we were installing them from the ubuntu binaries, of course this wouldn’t be quite as common a problem. Or even better, if our build environment came as a custom docker image. Fixing this is not completely trivial: we now have to install these packages from github, which in the case of rrdf means installing the latex build environment simply to build the rrdf vignette that we don’t need.&lt;/p&gt;
&lt;p&gt;While these issues can no doubt frustrate users as well, I’m not convinced that CI should really be testing build environment problems when I want it to be testing changes I’m making to my package. In the big picture, we need more stable build environments, and of course I’m asking for trouble by depending on lots of packages, particularly new, complex and otherwise fragile packages, so this testing is valuable. But on the other hand, this mostly just gets in the way. Ideally I should be able to point to a stable build environment and just ignore changes to the later packages until I want to deal with them. That’s what most users do with their own systems – not upgrading their personal libraries, distributions, etc, until they are ready to deal with anything that breaks. Being forced onto the bleeding edge all the time forces me to waste considerable time or accept a broken CI state that need not actually be broken.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Containerizing My Development Environment</title>
	 <link href="/2014/09/22/containerizing-my-development-environment.html"/>
   <updated>2014-09-22T00:00:00+00:00</updated>
   <id>/09/22/containerizing-my-development-environment</id>
   <content type="html">&lt;p&gt;A key challenge for reproducible research is developing solutions that integrate easily into a researcher’s existing workflow. Having to move all of one’s development onto remote machines, into a particular piece of workflow software or IDE, or even just constrained to a window running a local virtual machine in an unfamiliar or primitive environment isn’t particularly appealing. In my experience this doesn’t reflect the workflow of even those folks already concerned about reproducibility, and is, I suspect, a major barrier in adoption of such tools.&lt;/p&gt;
&lt;p&gt;One reason I find docker particularly attractive for reproducible research it the idea of containerizing my development environment into something I can transport or recreate identically anywhere, particularly on another Linux machine. This also provides a convenient backup system for my development environment, no need to remember each different program or how I installed or configured it when moving to a new machine.&lt;/p&gt;
&lt;h2 id=&quot;using-aliases&quot;&gt;Using aliases&lt;/h2&gt;
&lt;p&gt;For me, a convenient way to do this involves creating a simple alias for running a container. This allows me to distinguish between running any software and the container, while managing my files and data through my native operating system tools. I’ve set the following alias in my &lt;code&gt;bashrc&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alias c=&amp;#39;docker run --rm -it -v $(pwd):/home/$USER/`basename $PWD` -w /home/$USER/`basename $PWD` -e HOME=$HOME -e USER=$USER --user=$USER strata&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can then just do &lt;code&gt;c R&lt;/code&gt; (think &lt;code&gt;c&lt;/code&gt; for container) to get R running in a container, &lt;code&gt;c bash&lt;/code&gt; to drop into a bash shell on the container, &lt;code&gt;c pandoc --version&lt;/code&gt; echoes the version of pandoc available on our container (or otherwise execute the container version of pandoc), and so forth.&lt;/p&gt;
&lt;h3 id=&quot;explanation-a-non-root-container&quot;&gt;explanation: a non-root container&lt;/h3&gt;
&lt;p&gt;The trick here is primarily to handle permissions appropriately. Docker is run as a root user by default, which results in any files created or modified become owned by root instead of the user, which is clearly not desirable. Getting around this requires quite a bit of trickery. The break down of each of these arguments is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--rm&lt;/code&gt; remove this container when we quit, we don’t need to let it persist as a stopped container we could later return to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-it&lt;/code&gt; Interactive terminal&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-v&lt;/code&gt; binds a host volume to the container. Files on the host working directory (&lt;code&gt;pwd&lt;/code&gt;) will be available on the container, and changes made on the container are immediately written to the host directory:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;-v $(pwd):/home/$USER/`basename $PWD`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The path after the colon specifies where this directory should live on the container: we specify in a directory that has the same name as the current working directory &lt;code&gt;basename $PWD&lt;/code&gt;, located in the home directory of the user (e.g. where the user has write permissions).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-w&lt;/code&gt; specifies the working directory we should drop into when our session on the container starts. We set this to match the path where we have just mounted our current working directory:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;-w /home/$USER/`basename $PWD`&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;-e HOME=$HOME&lt;/code&gt; sets the value of the environmental variable &lt;code&gt;HOME&lt;/code&gt; to whatever it is on the host machine (e.g. &lt;code&gt;/home/username&lt;/code&gt;), so that when R tries to access &lt;code&gt;~/&lt;/code&gt;, it gets the user’s directory and not the root directory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;-e USER=$USER&lt;/code&gt; though this seems redundant, we set the user environmental variable by default in the cboettig/rstudio image, so this overrides that environmental variable with the current user.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;--user=$USER&lt;/code&gt; Specifies the user we log in as. This is rather important, otherwise the we find that we are the root (or whatever user has been set in the Dockerfile). That would cause any files we generate from the container to be owned by the root user, not our local user. Note that this only works if the specific user has already been created (e.g. by &lt;code&gt;adduser&lt;/code&gt;) on the container, otherwise this will fail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;strata&lt;/code&gt; the name of the container (could be &lt;code&gt;cboettig/ropensci&lt;/code&gt;, but my &lt;code&gt;strata&lt;/code&gt; image provides a few additional customizations, created by &lt;a href=&quot;&quot;&gt;it’s own Dockerfile&lt;/a&gt;. That Dockerfile (and its FROM dependencies) specify all the software available on this container. Importantly, it also already creates my username in it’s Dockerfile. Otherwise, the argument given above should use &lt;code&gt;--user=rstudio&lt;/code&gt;, since the &lt;code&gt;rstudio&lt;/code&gt; user is already created by the base image &lt;code&gt;cboettig/rstudio&lt;/code&gt;, and thus available in &lt;code&gt;cboettig/ropensci&lt;/code&gt; and &lt;code&gt;strata&lt;/code&gt;. Note that this user can be created interactively by passing the environmental variable &lt;code&gt;-e USER=$USER&lt;/code&gt; when running in deamon mode, since the user is then created by the start-up script. However, when we provide a custom command (like &lt;code&gt;/usr/bin/R&lt;/code&gt; in this example, the &lt;code&gt;CMD&lt;/code&gt; from the Dockerfile is overriden and the user isn’t created.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A stricter alias I considered first enforces running R as a container rather than a local operation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alias R=&amp;#39;docker run --rm -it -v $(pwd):/home/$USER/`basename $PWD` -w /home/$USER/`basename $PWD` -e HOME=$HOME --user=$USER strata /usr/bin/R&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;why-not-separate-containers-per-application&quot;&gt;Why not separate containers per application?&lt;/h2&gt;
&lt;p&gt;A more natural / more docker-esque approach might simply be to have separate containers for each application (R, pandoc, etc). This idealism belies the fact that I already need many of these tools installed on the same container, as they regularly interact in a deep way (e.g. R packages like &lt;code&gt;rmarkdown&lt;/code&gt; already depend on pandoc), so these should really be thought of as a single development environment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Server Backups</title>
	 <link href="/2014/09/09/server-backups.html"/>
   <updated>2014-09-09T00:00:00+00:00</updated>
   <id>/09/09/server-backups</id>
   <content type="html">&lt;h2 id=&quot;digital-ocean-snapshots&quot;&gt;Digital Ocean Snapshots&lt;/h2&gt;
&lt;p&gt;At $0.02 per gig per month, this looks like this is the cheapest way to make complete backups.&lt;/p&gt;
&lt;p&gt;The process is rather manual: we have to &lt;code&gt;sudo poweroff&lt;/code&gt; the droplet and then trigger the snapshot (the container will come back online after that, though we have to restart the services / active docker containers). We also have to delete old snapshots manually. Some of this can be automated from the API. DigitalOcean uses redundant storage for these (paying $0.01/month/gigabyte to Amazon Glacier), but at the moment we can’t export these images. Snapshots are also handy to deploy to a larger (but not smaller) droplet.&lt;/p&gt;
&lt;h3 id=&quot;digital-ocean-backups&quot;&gt;Digital Ocean Backups&lt;/h3&gt;
&lt;p&gt;These backups are an automated, always-online alternative to snapshots but must me initialized when the droplet is created and cost more (20% of server cost).&lt;/p&gt;
&lt;h2 id=&quot;manually-configuring-backups&quot;&gt;Manually configuring backups&lt;/h2&gt;
&lt;p&gt;To have the flexibility to restore individual pieces, to move between machines, etc we need a different approach.&lt;/p&gt;
&lt;h3 id=&quot;container-backups&quot;&gt;Container backups&lt;/h3&gt;
&lt;p&gt;Docker containers, including running containers, should be effectively backed up by either of these approaches to the state we would be in after a power cycle (e.g. we may need to start stopped containers, but not rebuild them from scratch).&lt;/p&gt;
&lt;p&gt;Nevertheless we may want to back up containers themselves. For many containers this is trivial (e.g. our ssh container): we can just commit the running container to an image and save that as a tar archive (or equivalently, just export the container to a tarball).&lt;/p&gt;
&lt;p&gt;If the containers have a &lt;code&gt;VOLUME&lt;/code&gt; command in their dockerfile or in their execution however, this is insufficient. Containers using volumes (such as &lt;code&gt;sameersbn/gitlab&lt;/code&gt; and &lt;code&gt;mattgruter/drone&lt;/code&gt;) need &lt;a href=&quot;http://serverfault.com/questions/576490/docker-volume-backup-and-restore&quot;&gt;four things&lt;/a&gt; to be backed up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dockerfile (or container image, from &lt;code&gt;save&lt;/code&gt; or &lt;code&gt;commit&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;volume&lt;/li&gt;
&lt;li&gt;volume path in container&lt;/li&gt;
&lt;li&gt;name of the container the volume belongs to&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;a href=&quot;https://github.com/discordianfish/docker-backup&quot;&gt;utility&lt;/a&gt; makes this easier.&lt;/p&gt;
&lt;h3 id=&quot;sparkleshare&quot;&gt;Sparkleshare&lt;/h3&gt;
&lt;p&gt;Sparkleshare is a git-backed dropbox alterantive. With binaries for most major platforms (Windows, Mac, Ubuntu/Linux) it’s pretty easy to set up and acts in much the same way, with automated synch and notifications. The backend just needs a server running git – Gitlab is a great way to set this up to permit relatively easy sharing / user management. (Ignore the information about setting up separately on a server, Gitlab is much easier. Also ignore advice about building from source on Ubuntu, installing the binary is far more straight forward: &lt;code&gt;apt-get install sparkleshare&lt;/code&gt;. Certainly it is not as feature rich as dropbox (e.g. email links to add users, web links to share individual files), but easy sharing over the server at no extra cost. The Sparkleshare directory is also a fully functional git repo.&lt;/p&gt;
&lt;h3 id=&quot;encrypted-backup-of-filesystem-with-duplicity&quot;&gt;Encrypted backup of filesystem with duplicity&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-use-duplicity-with-gpg-to-securely-automate-backups-on-ubuntu&quot;&gt;Duplicity setup&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Good for backing up to another host for which we have ssh access, or to an Amazon S3 bucket, etc. (Unclear if this works with Glacier due to upload-only et-up).&lt;/p&gt;
&lt;h3 id=&quot;some-other-rates-for-data-storage&quot;&gt;Some other rates for data storage:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compare to S3 ($0.03 /gig/month)&lt;/li&gt;
&lt;li&gt;EBS ($0.12 /gig/month) (really for computing I/O, not storage).&lt;/li&gt;
&lt;li&gt;Remarkably, Google Drive and Dropbox now offer 1 TB at $10 / mo. Clearly a lot can be saved by ‘overselling’ (most users will not use their capacity) and by shared files (counting against the space for all users but requiring no more storage capacity). Nonetheless, impressive, on par with Glacier (without the bandwidth charges or delay).&lt;/li&gt;
&lt;li&gt;For comparison, (non-redundant, non-enterprise, disk-based) storage is roughly $100/TB, or on order of that annual cost.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Server Security Basics</title>
	 <link href="/2014/09/08/server-security-basics.html"/>
   <updated>2014-09-08T00:00:00+00:00</updated>
   <id>/09/08/server-security-basics</id>
   <content type="html">&lt;h2 id=&quot;security-configuration&quot;&gt;Security configuration&lt;/h2&gt;
&lt;p&gt;We set up SSH key-only login on non-standard port, with root login forbidden. We then set up &lt;code&gt;ufw&lt;/code&gt; firewall, &lt;code&gt;fail2ban&lt;/code&gt;, and &lt;code&gt;tripwire&lt;/code&gt;.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Configure an &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2&quot;&gt;SSH key login&lt;/a&gt;. Next, &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-12-04&quot;&gt;Create a user, add to sudoers, and then disable root login.&lt;/a&gt;. Edits &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Disabling root logins. (We’ll need to add ourselves to sudo first: (&lt;code&gt;adduser&lt;/code&gt;, edit &lt;code&gt;/etc/sudoers&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Change ssh port from default to something else.&lt;/li&gt;
&lt;li&gt;Whitelist user login ids&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, let’s be sure to disable password authentication: Add &lt;code&gt;PasswordAuthentication no&lt;/code&gt; to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;. (editing PermitRootLogin only doesn’t do this).&lt;/p&gt;
&lt;p&gt;Locally add an entry in &lt;code&gt;~/.ssh/config&lt;/code&gt; to alias the host and port to avoid having to remember these numbers for login. Run &lt;code&gt;ssh-copy-id &amp;lt;droplet-ip&amp;gt;&lt;/code&gt; to enable key-based login for the user.&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-setup-a-firewall-with-ufw-on-an-ubuntu-and-debian-cloud-server&quot;&gt;Install and configure&lt;/a&gt; &lt;code&gt;ufw&lt;/code&gt; firewall. As we’re not using the default &lt;code&gt;ssh&lt;/code&gt; port, we need to explicitly tell &lt;code&gt;ufw&lt;/code&gt; which ssh port to allow.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;sudo ufw allow &amp;lt;PORT&amp;gt;/tcp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The &lt;code&gt;/tcp&lt;/code&gt; part is optional, saying only allow &lt;code&gt;tcp&lt;/code&gt; protocol over that port, not other protocols.)&lt;/p&gt;
&lt;p&gt;We must also tell ufw to &lt;a href=&quot;http://docs.docker.com/installation/ubuntulinux/#docker-and-ufw&quot;&gt;allow Docker&lt;/a&gt;: In &lt;code&gt;/etc/default/ufw&lt;/code&gt; change &lt;code&gt;DEFAULT_FORWARD_POLICY&lt;/code&gt; to ACCEPT, then:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; ufw reload
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; ufw allow 2375/tcp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and similarly allow any ports we export for our various services (Gitlab, Drone, etc).&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-protect-ssh-with-fail2ban-on-ubuntu-12-04&quot;&gt;Install and configure&lt;/a&gt; &lt;code&gt;fail2ban&lt;/code&gt;. Prevents brute force password attacks. Be sure to assign the config to match chosen ssh port.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-use-tripwire-to-detect-server-intrusions-on-an-ubuntu-vps&quot;&gt;Install and configure&lt;/a&gt; &lt;code&gt;tripwire&lt;/code&gt; (intrusion detection).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update software:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; apt-get -q update &lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; apt-get -qy dist-upgrade&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then also update tripwire log:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; tripwire --check --interactive&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: Clearly all these steps need to be running on the server itself, not merely in a container image deployed on server so that they are securing access to the actual host.&lt;/p&gt;
&lt;h2 id=&quot;additional-configuration&quot;&gt;Additional configuration&lt;/h2&gt;
&lt;p&gt;While we’re doing things, add user to the docker group for convenience: &lt;code&gt;sudo addgroup cboettig docker&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-add-swap-on-ubuntu-12-04&quot;&gt;Enable swap&lt;/a&gt; on a small instance. Here we set up 1GB of swap (setting swap at twice the available RAM is the recommended rule-of-thumb, though makes less sense once RAM is large)&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; fallocate -l 1G /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; chmod 600 /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; mkswap /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; swapon /swapfile&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make this persistant on reboot edit &lt;code&gt;/etc/fstab&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; echo &lt;span class=&quot;st&quot;&gt;&amp;quot;/swapfile       none    swap    sw      0       0&amp;quot;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/fstab&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For better performance, we might tweak swappiness to 10 (default is 60 out of 100, where 0 is never swap and 1 is swap frequently):&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;echo&lt;/span&gt; 10 &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; tee /proc/sys/vm/swappiness
&lt;span class=&quot;kw&quot;&gt;echo&lt;/span&gt; vm.swappiness = 10 &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; tee -a /etc/sysctl.conf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set ownership&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; chown root:root /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; chmod 0600 /swapfile&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;server-modules&quot;&gt;Server modules&lt;/h2&gt;
&lt;p&gt;Running different services as their own docker containers offers serveral advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Containers often make it easier to install and deploy existing services, since the necessary configuration is scripted in the Dockerfile and we can often find Dockerfiles already made on &lt;a href=&quot;http://hub.docker.com&quot;&gt;Docker Hub&lt;/a&gt; for common services. This note illustrates several examples.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Containers may provide an added level of stability, since they run largely in isolation from each other.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Containers can be &lt;a href=&quot;http://stackoverflow.com/questions/16084741/how-do-i-set-resources-allocated-to-a-container-using-docker&quot;&gt;resource limited&lt;/a&gt;, e.g.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -m 100m -c 100 ubuntu /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;would provide the container with 100 MB of RAM and 100 “shares” of CPU (acts kind of like a niceness, where the default share of a container is 1024. On multicore machines you can also pass &lt;code&gt;--cpuset &amp;quot;0&amp;quot;&lt;/code&gt; or &lt;code&gt;--cpuset &amp;quot;0,1&amp;quot;&lt;/code&gt; etc, which is a list of which cpus (numbered 0 to n-1, as in &lt;code&gt;/proc/cpuinfo&lt;/code&gt;) the container is permitted to use.&lt;/p&gt;
&lt;p&gt;As noted in the link, restricting disk space is more tricky, though might become easier down the road.&lt;/p&gt;
&lt;h3 id=&quot;ssh-server&quot;&gt;ssh server:&lt;/h3&gt;
&lt;p&gt;Permit users to ssh directly into a container rather than access the server itself. Despite its simplicity, I found this a bit tricky to set up correctly, particularly in managing users.&lt;/p&gt;
&lt;p&gt;Here’s the basic Dockerfile for an image we’ll call &lt;code&gt;ssh&lt;/code&gt;. This creates a user given by the environmental variable. A few tricks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use &lt;code&gt;adduser&lt;/code&gt; instead of &lt;code&gt;useradd&lt;/code&gt; so that we get the home directory for the user created and granted the correct permissions automaticalliy. We need the &lt;code&gt;--gecos&lt;/code&gt; information so that we’re not prompted to enter the user’s full name etc. We use &lt;code&gt;--disabled-password&lt;/code&gt; rather than set a password here.&lt;/li&gt;
&lt;li&gt;Login is still possible through ssh key (as well as through nsenter on the host machine). We go ahead and add the ssh key now, though this could be done after the container is running by using nsenter.&lt;/li&gt;
&lt;li&gt;In this dockerfile, we’ve added the user to &lt;code&gt;sudo&lt;/code&gt;ers group for root access on the container (installing software, etc). This won’t be active until the user has a password.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;FROM     ubuntu:14.04
ENV USER cboettig
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y openssh-server
RUN mkdir /var/run/sshd
RUN adduser --disabled-password --gecos &amp;quot;&amp;quot; $USER
RUN adduser $USER sudo
ADD authorized_keys /home/$USER/.ssh/authorized_keys
RUN chown $USER /home/$USER/.ssh/authorized_keys
EXPOSE 22
CMD    [&amp;quot;/usr/sbin/sshd&amp;quot;, &amp;quot;-D&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When building the image, note that a copy of &lt;code&gt;authorized_keys&lt;/code&gt; (contains the contents of the &lt;code&gt;id_rda.pub&lt;/code&gt; public key) file must be found in the same directory as the Dockerfile so that it can be added to the image.&lt;/p&gt;
&lt;p&gt;Start the ssh server on port 2200:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 2200:22 --name=&amp;quot;ssh&amp;quot; ssh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add to the firewall permissions&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo ufw add 2200/tcp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here I can now ssh in from the computer housing the private key pair to the public key that is added to the image here. However, that user doesn’t have root access since we haven’t provided a password.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;nsenter&lt;/code&gt; to enter the instance:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -v /usr/local/bin:/target jpetazzo/nsenter
&lt;span class=&quot;kw&quot;&gt;nsenter&lt;/span&gt; -m -u -n -i -p -t &lt;span class=&quot;kw&quot;&gt;`docker&lt;/span&gt; inspect --format &lt;span class=&quot;st&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt; ssh&lt;span class=&quot;kw&quot;&gt;`&lt;/span&gt; /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a password for the user to enable root access:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;$USER:&amp;lt;password&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;chpasswd&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can create other users and add them to sudoers or not as desired, e.g. add interactively using:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;useradd&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;username&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;users can later change their passwords once they log in.&lt;/p&gt;
&lt;h2 id=&quot;restoring-containers-when-restarting&quot;&gt;Restoring containers when restarting&lt;/h2&gt;
&lt;p&gt;A certain times we need to power cycle the server (e.g. after certain software updates), using the &lt;code&gt;sudo reboot now&lt;/code&gt; command or the DigitalOcean console. Any running containers will be automatically stopped. Once the machine is back up and we log back in, these containers can be brought back up with &lt;code&gt;docker restart &amp;lt;container_id&amp;gt;&lt;/code&gt; (or &lt;code&gt;&amp;lt;container_name&amp;gt;&lt;/code&gt;), and then everything is back to normal.&lt;/p&gt;
&lt;p&gt;Note that while we can stop and restart a container, it seems we cannot simply save a container (e.g. with &lt;code&gt;docker commit&lt;/code&gt; or &lt;code&gt;docker save&lt;/code&gt; and re-run it and expect the server functionality to be restored after the container is destroyed (e.g. by &lt;code&gt;docker rm -f&lt;/code&gt;). (See previous notes 2014-09-05) for an illustration of this problem. This occurs because the container image does not include the volume where it writes its data, and that volume address is generated uniquely each time a container is run.&lt;/p&gt;
&lt;p&gt;Consequently, a different (probably more traditional) approach is needed to backup the configuration of a server such as Gitlab or Drone-CI even when running in a container. Will explore this later.&lt;/p&gt;
&lt;p&gt;Meanwhile, remember remove unneeded containers with&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; rm &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; ps -a &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;grep&lt;/span&gt; Exited &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and not with &lt;code&gt;-f&lt;/code&gt; (destroying running containers!)&lt;/p&gt;
&lt;h2 id=&quot;key-security&quot;&gt;Key security&lt;/h2&gt;
&lt;p&gt;We can largely avoid needing a private ssh key for the server, though may use https authentication to let us use git (rather than, say, rsync) to develop and save changes made remotely (say, through RStudio server).&lt;/p&gt;
&lt;h3 id=&quot;backing-up-keys&quot;&gt;Backing up keys&lt;/h3&gt;
&lt;p&gt;Probably unnecessary to have a backup of the ssh private RSA key, as we can access the DigitalOcean Server or Github through the web consoles and add a new public key and replace our private key.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts On Harte Interview</title>
	 <link href="/2014/09/05/thoughts-on-Harte-interview.html"/>
   <updated>2014-09-05T00:00:00+00:00</updated>
   <id>/09/05/thoughts-on-Harte-interview</id>
   <content type="html">&lt;p&gt;Read this delightful &lt;a href=&quot;http://www.biodiverseperspectives.com/2014/08/31/diverse-introspectives-a-conversation-with-john-harte/&quot;&gt;John Harte Interview&lt;/a&gt; (ht &lt;span class=&quot;citation&quot; data-cites=&quot;DynamicEcology&quot;&gt;@DynamicEcology&lt;/span&gt;) which resonates rather well with me (no surprise given our common background). Nevertheless, I would have to push back on a few pieces.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;From general laws flow absolutely bullet-proof insights and this is what we most need&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I must disagree. Bullet-proof insights are mathematical theorems. Their generality is an empirical question about their assumptions (not their predictions). (And lastly, I’m pretty much in support of the “what we most need” part, but it deserves to be substantiated. Certain objectives could be met just fine with a black box/crystal ball predictor of the future that provided no general laws or insights. It remains to be demonstrated that such predictions are (a) not all that is needed or (b) are impossible without general laws).&lt;/p&gt;
&lt;p&gt;The critique against &lt;em&gt;fitting&lt;/em&gt; mechanistic models is a very different argument than a critique against writing down a proposed mechanism merely to study it in the absence of data. Such are the role of theorems in ecology. I’ve no doubt John appreciates the importance of such work, though perhaps too rarely do we acknowledge that such contributions are more fundamental and robust, not less, precisely because they do not involve fitting parameters to data. Theoretical ecology has a rich contribution independent of any observation. Conditions under which populations can oscillate without being driven periodically. The necessary conditions for coexistence of &lt;span class=&quot;math&quot;&gt;\(n\)&lt;/span&gt; species, and role of spatial/temporal heterogeneity therein. Threshold dynamics/&lt;span class=&quot;math&quot;&gt;\(r_0\)&lt;/span&gt;. The evolution of dispersal (e.g. why the existence of spatial heterogeneity alone is not sufficient), of demonstrating that connecting two “sink” patches in which a resident population cannot persist can enable persistence. This kind of work has the bullet-proof status of theorem without making any claim on generality. These are laws in a mathematical sense. Whether they are laws in an ecological sense and whether they are general or not is a question of how often their assumptions are met. This is why, as Tony Ives says, we must test &lt;em&gt;assumptions&lt;/em&gt;, not (merely) &lt;em&gt;predictions&lt;/em&gt; (or worse, “post-dictions” of model fit). How we testing assumption is not as statistically well-posed as how we fit models, but it need not be hard. The assumption that space and time are not everywhere homogeneous is relatively easy to establish across a wide range of scales. Not all assumptions are so easy: That dynamics should be largely restricted to low-dimensional manifolds is far from obvious (at almost any scale).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One of them is the ease with which we can simulate numerically and handle massive data sets. There is a risk that this will divorce people from what really matters, which is the natural world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we can draw meaningful conclusions from the exercises in mathematical logic above (which John no doubt appreciates we have, and would no doubt agree could guide and clarify a lot of muddled thinking), then the same can be said of numerics. Like anything else, of course it can be done poorly, but numerical studies are no more inherently divorced from the natural world than good analytic theory.&lt;/p&gt;
&lt;p&gt;It is unfortunate that ‘massive data sets’ fall in the same sentence, as the term is not synonymous with ‘numerical simulation’. Again, it is perhaps easy to misuse such data in ignorance of the natural world, just like anything else can be done poorly. Yet John has eloquently argued the danger of the ‘mechanistic approach’ allowing intuition to select a few arbitrary features we consider to be important, and the same might be said of the observations themselves. No one would argue that all that is interesting happens to be things we can observe and manipulate on the temporal and spatial scales of an individual human being (e.g. in the domain of field work). Aggregating data across larger temporal and spatial scales, being able to take advantage of rich information about environment, climate, genetics, phylogeny, and so forth in our understanding of patterns and process is important.&lt;/p&gt;
&lt;p&gt;John has an excellent bit in the interview about science advancing by failures rather than successes; that we learn the most when we can observe deviations from the model. In this I see his views and Tony’s views as two sides of the same coin: it is in discovering and then understanding the mechanisms behind the deviations from general theory that we most advance. John’s example from statistical mechanics resonates strongly with me here – deviations from the ideal gas law not only expose interesting new science (forget dipole moments, this is the whole molecular, atomized world view replacing a continuous one), while also recovering the general law under the appropriate limits (of temperature and pressure but also number of atoms).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Drone Ci And Docker</title>
	 <link href="/2014/09/05/drone-ci-and-docker.html"/>
   <updated>2014-09-05T00:00:00+00:00</updated>
   <id>/09/05/drone-ci-and-docker</id>
   <content type="html">&lt;h2 id=&quot;drone-ci-continous-integration-in-custom-docker-environments&quot;&gt;Drone CI: Continous integration in custom docker environments&lt;/h2&gt;
&lt;p&gt;Having gotten accustomed to Docker, configuring the appropriate build environment for a Continuous Integration system like Travis CI or Shippable CI starts to feel incredibly tedious and archaic (particularly if you work primarily in a language like R or haskell that usually isn’t supported out of the box).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We do not have to hack together a custom image environment&lt;/li&gt;
&lt;li&gt;We can build and test our environment locally instead of having to rely on trial-and-error pushes to the CI server&lt;/li&gt;
&lt;li&gt;We do not have to download, compile and install the development environment each time, (which frequently takes longer than the CI checks themselves and can break)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Shippable provides a persistent environment too, by preserving the state of your ‘minion’. But unlike Shippable, I believe the Drone approach is unlikely that you can create troublesome side-effects in your environment, such as removing a necessary dependency from the &lt;code&gt;shippable.yml&lt;/code&gt; and yet not catching it since the dependency is still available on the minion from before. In the Drone approach, we start on the same docker image each time, but merely avoid the few minutes it might take to download that image).&lt;/p&gt;
&lt;p&gt;Unfortunately, custom images are not available on the fully hosted &lt;a href=&quot;http://drone.io&quot;&gt;drone.io&lt;/a&gt; system. (Though perhaps they’d accept pull requests that would add an R environment to &lt;a href=&quot;https://github.com/drone/images&quot;&gt;their image library&lt;/a&gt;). Fortunately, the Drone team kindly provides an &lt;a href=&quot;https://github.com/drone.drone&quot;&gt;open source version&lt;/a&gt; of their platform that can be hosted on a self-hosted / private server (such as the new web darling DigitalOcean or Amazon’s EC2). This has other advantages as well – such as using privately hosted repositories (it also integrates with BitBucket and GitLab) or running very long tests / simulations (since we’re now paying for the server time ourselves, after all).&lt;/p&gt;
&lt;h2 id=&quot;the-easy-way-use-docker&quot;&gt;The easy way: use docker&lt;/h2&gt;
&lt;p&gt;We can deploy the Drone CI server somewhat more seamlessly by running it in a container itself. Rather than worry about the above configuration, we can simply launch an existing docker image for Drone, rather cleverly created by Matt Gruter:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --name=&lt;span class=&quot;st&quot;&gt;&amp;#39;drone&amp;#39;&lt;/span&gt; -d -p 8080:80 --privileged mattgruter/drone&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now we can follow the &lt;a href=&quot;http://drone.readthedocs.org/en/latest/setup.html&quot;&gt;setup instructions&lt;/a&gt;. Be sure to use the matching case in the application name (&lt;code&gt;Drone&lt;/code&gt; not &lt;code&gt;drone&lt;/code&gt;) and the appropriate URLs for the authorization call back.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that we must use a different port than 80, and that we must give this port explicitly in the Authorization callback URL: &lt;code&gt;http://localhost:8080/auth/login/github&lt;/code&gt; in order to authenticate.&lt;/p&gt;
&lt;p&gt;Also note that in this approach, the Drone CI’s docker image library will be separate from the docker image library. To manage or update the images available, we have to first &lt;code&gt;nsenter&lt;/code&gt; into the Drone CI container.&lt;/p&gt;
&lt;p&gt;This runs rather nicely on a tiny DigitalOcean droplet. Bare in mind that the tiny droplet has only 20 GB of disk space though, which can be consumed rather quickly with too many images. If many of the images use the same base templates, the total disk space required will fortunately be much lower than the sum of their virtual sizes.&lt;/p&gt;
&lt;h3 id=&quot;experimenting-with-saving-images&quot;&gt;experimenting with saving images&lt;/h3&gt;
&lt;p&gt;Being a docker image, we can snapshot and export it for later use, and meanwhile can even destroy our server instance.&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; export drone &lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; dronedroplet.tar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not clear that this works. Consider saving an image instead? Save container named drone as image named drone:droplet&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; commit drone drone:droplet
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; save drone:droplet &lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; dronedroplet.tar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;are these identical?&lt;/p&gt;
&lt;p&gt;Hmm, doesn’t seem to store configuration, login is no longer valid. Starting a stopped container maintains the configuration of course, but not launching from scratch (e.g. the sqlite database is local to the container, not accessible through an externally linked volume).&lt;/p&gt;
&lt;p&gt;Note that this tarball does not include the Drone CI image library itself, which is not part of the container but rather connected as a volume. This makes it quite a bit smaller, and that library can presumably be reconstructed from the docker hub.&lt;/p&gt;
&lt;h2 id=&quot;configuring-drone-ci-the-hard-way&quot;&gt;Configuring Drone CI: the hard way&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install and launch drone: (see &lt;a href=&quot;https://github.com/drone/drone&quot;&gt;drone/README&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;DOCKER_OPTS=&amp;quot;-H 127.0.0.1:4243 -d&amp;quot;&lt;/code&gt; to &lt;code&gt;/etc/default/docker&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Kill the docker deamon and restart docker. Or run docker with the explicit binding:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; docker -d -H 127.0.0.1:4243 &lt;span class=&quot;kw&quot;&gt;&amp;amp;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;You may need to &lt;a href=&quot;http://docs.docker.com/installation/ubuntulinux/#docker-and-ufw&quot;&gt;configure firewall&lt;/a&gt;, if ufw is running.&lt;/li&gt;
&lt;li&gt;References: &lt;a href=&quot;https://github.com/drone/drone/issues/149&quot;&gt;drone/issues/149&lt;/a&gt;, &lt;a href=&quot;https://github.com/drone/drone/issues/24&quot;&gt;drone/issues/24&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Now we can follow the &lt;a href=&quot;http://drone.readthedocs.org/en/latest/setup.html&quot;&gt;setup instructions&lt;/a&gt;. Be sure to use the matching case in the application name (&lt;code&gt;Drone&lt;/code&gt; not &lt;code&gt;drone&lt;/code&gt;) and the appropriate URLs for the authorization call back.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;configuring-an-already-running-docker-session&quot;&gt;Configuring an already-running docker session&lt;/h2&gt;
&lt;p&gt;Launch a named repository in deamon mode:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -d -p 8787:8787 --name=&lt;span class=&quot;st&quot;&gt;&amp;#39;drone&amp;#39;&lt;/span&gt; mattgruter/drone&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use a docker-based install to add &lt;code&gt;nsenter&lt;/code&gt; into your executable path:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -v /usr/local/bin:/target jpetazzo/nsenter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run &lt;code&gt;nsenter&lt;/code&gt; to log into the docker image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nsenter -m -u -n -i -p -t `docker inspect --format &amp;#39;{{ .State.Pid }}&amp;#39; drone` /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can update or delete images with &lt;code&gt;docker pull&lt;/code&gt;, &lt;code&gt;docker rmi&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;This is useful with many containers, for instance, with our ssh container or rstudio container we may want to modify usernames and passwords, etc:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;useradd&lt;/span&gt; -m &lt;span class=&quot;ot&quot;&gt;$USER&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$USER&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$PASSWORD&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;chpasswd&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;making-this-easier&quot;&gt;Making this easier:&lt;/h3&gt;
&lt;p&gt;Add to &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;fu&quot;&gt; dock&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; nsenter -m -u -n -i -p -t &lt;span class=&quot;kw&quot;&gt;`docker&lt;/span&gt; inspect --format  &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;`&lt;/span&gt; /bin/bash&lt;span class=&quot;kw&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines the function &lt;code&gt;dock&lt;/code&gt; such that &lt;code&gt;dock &amp;lt;name&amp;gt;&lt;/code&gt; will enter a running container named &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt;. Note that we have to have &lt;code&gt;nsenter&lt;/code&gt; bound to the executable path as indicated above. Yay less typing.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Docker tricks of the trade and best practices thoughts</title>
	 <link href="/2014/08/29/docker-notes.html"/>
   <updated>2014-08-29T00:00:00+00:00</updated>
   <id>/08/29/docker-notes</id>
   <content type="html">&lt;h2 id=&quot;best-practices-questions&quot;&gt;Best practices questions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Here are some tricks that may or may not be in keeping with best practices, input would be appreciated.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep images small: use the &lt;code&gt;--no-install-recommends&lt;/code&gt; option for &lt;code&gt;apt-get&lt;/code&gt;, install true dependencies rather than big metapackages (like &lt;code&gt;texlive-full&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Avoid creating additional AUFS layers by combining RUN commands, etc? (limit was once 42, but is now at least 127).&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can use &lt;code&gt;RUN git clone ...&lt;/code&gt; to add data to a container in place of ADD, which invalidates caching.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use automated builds linked to Github-based Dockerfiles rather than pushing local image builds. Not only does this make the Dockerfile transparently available and provide a link to the repository where one can file issues, but it also helps ensure that the image available on the hub gets its base image (&lt;code&gt;FROM&lt;/code&gt; entry) from the hub instead of whatever was available locally. This can help avoid various out-of-sync errors that might otherwise emerge.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;dockers-use-of-tags&quot;&gt;Docker’s use of tags&lt;/h3&gt;
&lt;p&gt;Unfortunately, Docker seems to use the term &lt;code&gt;tag&lt;/code&gt; to refer both to the label applied to an image (e.g. in &lt;code&gt;docker build -t imagelabel .&lt;/code&gt; the &lt;code&gt;-t&lt;/code&gt; argument “tags” the image as ‘imagelabel’ so we need not remember its hash), but also uses &lt;code&gt;tag&lt;/code&gt; to refer to the string applied to the end of an image name after a colon, e.g. &lt;code&gt;latest&lt;/code&gt; in &lt;code&gt;ubuntu:latest&lt;/code&gt;. The latter is the definition of “tags” as listed under the “tags” tab on the Docker Hub. Best practices for this kind of tag (which I’ll arbitrarily refer to as a ‘version tag’ to distinguish it) are unclear.&lt;/p&gt;
&lt;p&gt;One case that is clear is tagging specific versions. Docker’s automated builds lets a user link a “version tag” to either to a branch or a tag in the git history. A “branch” in this case can refer either to a different git branch or merely a different sub-directory. Matching to a Git tag provides the most clear-cut use of the docker version-tag; providing a relatively static version stable link. (I say “relatively” static because even when we do not change the Dockerfile, if we re-build the Dockerfile we may get a new image due the presence of newer versions of the software included. This can be good with respect to fixing security vulnerabilities, but may also break a previously valid environment).&lt;/p&gt;
&lt;p&gt;The use case that is less clear is the practice of using these “version tags” in Docker to indicate other differences between related images, such as &lt;code&gt;eddelbuettel/docker-ubuntu-r:add-r&lt;/code&gt; and &lt;code&gt;eddelbuettel/docker-ubuntu-r:add-r-devel&lt;/code&gt;. Why these are different tags instead of different roots is unclear, unless it is for the convenience of multiple docker files in a single Github repository. Still, it is perfectly possible to configure completely separate docker hub automated builds pointing at the same Github repo, rather than adding additional builds as tags in the same docker hub repo.&lt;/p&gt;
&lt;p&gt;Docker linguistics borrow from git terminology, but it’s rather dangerous to interpret these too literally.&lt;/p&gt;
&lt;h2 id=&quot;keeping-a-clean-docker-environment&quot;&gt;Keeping a clean docker environment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;run interactive containers with &lt;code&gt;--rm&lt;/code&gt; flag to avoid having to remove them later.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove all stopped containers:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; rm &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; ps -a &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;grep&lt;/span&gt; Exited &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Clean up un-tagged docker images:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; rmi &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; images -q --filter &lt;span class=&quot;st&quot;&gt;&amp;quot;dangling=true&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Stop and remove all containers (including running containers!)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; rm -f &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; ps -a -q&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;docker-and-continuous-integration&quot;&gt;Docker and Continuous Integration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can install but cannot run Docker on &lt;a href=&quot;http://travis.org&quot;&gt;Travis-CI&lt;/a&gt; at this time. It appears the linux kernel available there is much too old. Maybe when they upgrade to Ubuntu 14:04 images…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We cannot run Docker on the docker-based &lt;a href=&quot;http://shippable.com&quot;&gt;Shippable-CI&lt;/a&gt; (at least without a vagrant/virtualbox layer in-between). Docker on Docker is not possible (see below).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the same reason, we cannot run Docker on &lt;a href=&quot;http://drone.io&quot;&gt;drone.io&lt;/a&gt; CI. However, Drone provides an open-source version of it’s system that can be run on your own server, which unlike the fully hosted offering, permits custom images. Unfortunately I &lt;a href=&quot;https://github.com/drone/drone/issues/54&quot;&gt;cannot get it working&lt;/a&gt; at this time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;docker-inside-docker&quot;&gt;Docker inside docker:&lt;/h2&gt;
&lt;p&gt;We cannot directly install docker inside a docker container. We can get around this by adding a complete virtualization layer – e.g. docker running in vagrant/virtualbox running in docker.&lt;/p&gt;
&lt;p&gt;Alternatively, we can be somewhat more clever and tell our docker to simply use a different volume to store its AUFS layers. Matt Gruter has a &lt;a href=&quot;https://github.com/mattgruter/dockerfile-doubledocker&quot;&gt;very clever example&lt;/a&gt; of this, which can be used, e.g. to run a Drone server (which runs docker) inside a Docker container (&lt;a href=&quot;http://registry.hub.docker.com/u/mattgruter/drone/&quot;&gt;mattgruter/drone&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I believe this only works if we run the outer docker image with &lt;code&gt;--privileged&lt;/code&gt; permissions, e.g. we cannot use this approach on a server like Shippable that is dropping us into a prebuilt docker container.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reproducible research environments with Docker</title>
	 <link href="/2014/08/25/reproducible-research-environments-with-Docker.html"/>
   <updated>2014-08-25T00:00:00+00:00</updated>
   <id>/08/25/reproducible-research-environments-with-Docker</id>
   <content type="html">&lt;p&gt;Academic research takes place in computational environments of continually increasing complexity. This creates ever-higher barriers not only to reproducing or extending the results of other researchers, but also barriers to collaboration and training of new researchers.&lt;/p&gt;
&lt;p&gt;Wouldn’t it be nice if we could all work in equivalent computing environments, such that whatever worked for me on my computer would work for you on yours? Wouldn’t it be nice if we could just clone and copy our entire software environment when we needed to move our computations over to a more powerful cloud or cluster computer? Or perhaps roll back our environment to the state when everything was working yesterday?&lt;/p&gt;
&lt;h2 id=&quot;reproducibility-and-r&quot;&gt;Reproducibility and R&lt;/h2&gt;
&lt;p&gt;The situation for users for R is much better than most, since CRAN not only handles most dependencies but provides binaries for most major operating systems. Though subsequent changes in packages can break this chain and cause scripts that once worked to no longer due so, emerging solutions like &lt;a href=&quot;https://github.com/rstudio/packrat&quot;&gt;packrat&lt;/a&gt; try to tackle this problem.&lt;/p&gt;
&lt;p&gt;Nevertheless, R is not immune from these issues. Many packages make use of code libraries from other languages, from &lt;code&gt;pandoc&lt;/code&gt;‘s system for rendering documents to interfaces with SQL databases, to manipulation of other formats like XML, to highly optimized linear algebra libraries. While an experienced user following careful documentation can usually install the appropriate libraries to create a functionally equivalent system, this raises substantial barriers for collaborators, students, and others. A &lt;a href=&quot;https://storify.com/hlapp/reproducibility-repeatability-bigthink&quot;&gt;reproducibility study at NESCENT&lt;/a&gt; recently found that even computationally savvy experts (though not necessarily specific to the computer language or domain in question) working with code provided by authors who were explicitly committed to reproducibility (including me), could not even reconstruct the necessary computational environment. This inability to install all the correct components for software to run in the same way as it did for the original researchers is commonly known as ’dependency hell’ and has often been shown as the primary cause that computational components of research could not be replicated, (not least because it is one of the very first steps required before replication can be attempted).&lt;/p&gt;
&lt;p&gt;Just about anyone who has taught a course involving running software in class will be familiar with the challenges and lost instruction time from installing such dependencies on student’s machines. Even when all dependencies are installed successfully, there is a risk that these changes will break some of the user’s existing code by altering it’s environment.&lt;/p&gt;
&lt;h2 id=&quot;jumping-in&quot;&gt;Jumping in&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.docker.com/installation&quot;&gt;Install Docker&lt;/a&gt; for your operating system using &lt;code&gt;boot2docker&lt;/code&gt;. (Note the Linux instructions do not require boot2docker. I recommend the 1-line curl-script install method for Ubuntu). Now you can launch &lt;code&gt;boot2docker&lt;/code&gt; to open a terminal window from where we will run Docker. NOTE: RStudio example requires Docker version &lt;code&gt;&amp;gt;= 1.2&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There’s several different ways we can interact with the container. The simplest approach is just run an R terminal on the container:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; docker run --rm -it cboettig/rstudio /usr/bin/R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should open up the R command line in the current window.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download and run an RStudio server instance:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; docker run -d -p 8787:8787 cboettig/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(These commands will be slow on the first run since the image must be downloaded. Afterwards they should be pretty quick.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can now reach your RStudio server at &lt;code&gt;http://&amp;lt;system_ip_address&amp;gt;:8787&lt;/code&gt;. For Windows/Mac users, run &lt;code&gt;boot2docker ip&lt;/code&gt; to get the value of &lt;code&gt;system_ip_address&lt;/code&gt;. (This should be &lt;code&gt;http://92.168.59.103:8787&lt;/code&gt; but may vary as it is set dynamically).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Linux users, you can just use &lt;code&gt;http://localhost:8787&lt;/code&gt;. For cloud instances, check your server’s public IP address, and append the port (&lt;code&gt;:8787&lt;/code&gt;)..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Login using the default rstudio:rstudio for user:pw, or configure particular users (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;options&quot;&gt;Options&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Replace &lt;code&gt;cboettig/rstudio&lt;/code&gt; with &lt;code&gt;cboettig/ropensci&lt;/code&gt; to run a richer (but larger) development environment. See below for details.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set user name and password using environmental variables, e.g.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -d -p 8787:8787 -e USER=&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;username&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; -e PASSWORD=&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;password&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; -e EMAIL=you@somewhere.com cboettig/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Link the container to a local folder (directory) using the &lt;code&gt;-v&lt;/code&gt; option:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run --rm -it -v &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;:/home/rstudio/&lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;basename&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt; cboettig/rstudio /usr/bin/R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in this example we have launched an interactive R terminal, rather than an RStudio server. This linking would work just as well with RStudio-server. However, because we have linked the working directory to the container, we are now free to use all our favorite tools from our native operating system to edit and manage our files, rather than being confined to RStudio.&lt;/p&gt;
&lt;p&gt;Note that the volumes link (&lt;code&gt;-v&lt;/code&gt;) is just taking the path to the directory on the host followed by the path where it should appear inside the container. We could specify these manually, but &lt;code&gt;$(pwd)&lt;/code&gt; is just a convenient way to get the full path of the current working directory, and &lt;code&gt;$basename &amp;quot;$PWD&amp;quot;&lt;/code&gt; a way to get just the name of that directory. So if working in &lt;code&gt;foo&lt;/code&gt;, the directory &lt;code&gt;/path/to/foo&lt;/code&gt; would be found at &lt;code&gt;/home/rstudio/foo&lt;/code&gt; on the container. Here we’re using the default user, &lt;code&gt;rstudio&lt;/code&gt;, but you would want to change that if specifying a different user name as shown in the previous example.&lt;/p&gt;
&lt;p&gt;Linking files on a Mac or PC requires an extra step. This links us to the &lt;code&gt;boot2docker&lt;/code&gt; volume, but we still must link boot2docker virtual machine to the host OS. It seems like &lt;code&gt;boot2docker&lt;/code&gt; may still be working on this issue. Meanwhile, it is straight-forward to do this linking by using &lt;a href=&quot;http://vagrantup.com&quot;&gt;vagrant&lt;/a&gt; to launch boot2docker. I discuss this below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download and run an RStudio server instance: &lt;code&gt;sudo docker run -d -p 8787:8787 cboettig/rstudio&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can now reach your RStudio server at &lt;code&gt;http://&amp;lt;system_ip_address&amp;gt;:8787&lt;/code&gt;. For Windows/Mac users, run &lt;code&gt;boot2docker ip&lt;/code&gt; to get the value of &lt;code&gt;system_ip_address&lt;/code&gt;. (This should be &lt;code&gt;http://92.168.59.103:8787&lt;/code&gt; but may vary as it is set dynamically). For linux users, you can just use &lt;code&gt;localhost&lt;/code&gt;. For cloud instances, check your server’s public IP address.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;login using the default &lt;code&gt;rstudio:rstudio&lt;/code&gt; for &lt;code&gt;user:pw&lt;/code&gt;, or configure particular users (see below).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rstudio-on-digital-ocean&quot;&gt;RStudio on Digital Ocean&lt;/h2&gt;
&lt;p&gt;Docker also makes it very fast and easy to deploy an RStudio instance in the cloud. More importantly, it lets you deploy an instance that already has your preferred computational environment already completely installed and configured through your docker container. Running Docker on a cloud machine can be useful for many purposes, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Giving your collaborator access to your computational environment&lt;/li&gt;
&lt;li&gt;Scaling up a computation to a larger machine&lt;/li&gt;
&lt;li&gt;running RStudio from your ipad&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because we use RStudio for this, we have both a completely secured connection and a familiar environment in which to interact with R. RStudio in the browser looks and feels identical to the desktop edition. Getting started with RStudio in the cloud is particularly easy using Docker and the DigitalOcean cloud provider. We can run RStudio in the &lt;a href=&quot;https://www.digitalocean.com/pricing/&quot;&gt;DigitalOcean cloud&lt;/a&gt; for less than 1 cent an hour ($5 / month if run continuously). Creating an account and launching your first DigitalOcean droplet is dead easy. Head over to &lt;a href=&quot;http://digitalocean.com&quot;&gt;DigitalOcean.com&lt;/a&gt; because they do a much better job of walking you through the few simple steps than I would. Create the cheapest droplet type to get started and ssh into your image once it is up and running, using the ip-address for your droplet and the password sent to you by email or an &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2&quot;&gt;ssh key&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ssh&lt;/span&gt; root@ip-address&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;enable-swapping-if-testing-on-the-smallest-image&quot;&gt;Enable swapping if testing on the smallest image&lt;/h3&gt;
&lt;p&gt;The smallest Digital Ocean servers have only 512 MB of memory and no swap enabled. Adding swap lets the machine cache things it doesn’t need in active memory, which can be important for running things like &lt;code&gt;install.packages&lt;/code&gt;. On larger droplets this probably is not so much of an issue. Here, we enable 1GB of swap. SSH into your cloud server (or use the shell DigitalOcean provides in the browser), and run:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; fallocate -l 1G /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; chmod 600 /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; mkswap /swapfile
&lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; swapon /swapfile&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;launch-rstudio&quot;&gt;Launch RStudio&lt;/h3&gt;
&lt;p&gt;We’re now ready to run RStudio. If you didn’t select the &lt;code&gt;docker&lt;/code&gt; application while creating your image, you can install it now:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;curl&lt;/span&gt; -sSL https://get.docker.io/ubuntu/ &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;sudo&lt;/span&gt; sh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re now ready to launch RStudio using docker, just as we did locally above.&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -e USER=your_username -e=PASSWORD a_secure_pw \
  -d -p 8787:8787 --name rstudio cboettig/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we pass a custom username and secure password in using the &lt;code&gt;-e&lt;/code&gt; arguments since our RStudio instance will be publically reachable by it’s ip. We’ve also given the container a name &lt;code&gt;rstudio&lt;/code&gt; so we can easily refer to it later. We now go to &lt;code&gt;http://&amp;lt;your-droplet-ip-address&amp;gt;:8787&lt;/code&gt; and login with these custom credentials and we’re ready to compute.&lt;/p&gt;
&lt;h3 id=&quot;saving-your-work&quot;&gt;Saving your work&lt;/h3&gt;
&lt;p&gt;The best way to save your work is to commit your entire docker image. This will save all installed packages, active RStudio sessions, files, and everything else, even if we destroy the droplet later. This gives us the option of running our environment on a larger DigitalOcean image when the need arises, or running it locally using the &lt;code&gt;boot2docker&lt;/code&gt; approach outlined above.&lt;/p&gt;
&lt;p&gt;From an ssh terminal into your droplet where you first ran docker, this command will save your container as an image called &lt;code&gt;user/rstudio&lt;/code&gt; (use any name combination you like).&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; commit rstudio user/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you create an account on &lt;a href=&quot;https://hub.docker.com&quot;&gt;hub.docker.com&lt;/a&gt; we can upload this as a public or private image that we can access from anywhere for later use:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; push user/rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Be sure to use this name to run your container in the future to pick up where you left off. If you’d rather not use the Docker Hub to host your image, you can download the image file instead:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; save -o rstudio.tar rstudio&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then from your local machine, &lt;code&gt;scp&lt;/code&gt; the tar file to download it:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;scp&lt;/span&gt; root@&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;ip-address&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;:~/rstudio.tar .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now destroy your droplet to avoid future charges when not in use. To deploy this image later, copy it over to the new machine and load in docker:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;scp&lt;/span&gt; rstudio.tar root@&lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;new-ip-address&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;:~
&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; load rstudio.tar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then call &lt;code&gt;docker run&lt;/code&gt; as before to get up and running.&lt;/p&gt;
&lt;h3 id=&quot;misc-remote-linux-clusters-without-root&quot;&gt;Misc: Remote Linux clusters without root&lt;/h3&gt;
&lt;p&gt;You can run the docker images on a remote linux cluster where you don’t have root access, even if it doesn’t have a web-accessible API (such as a university server). Ask your friendly system administrator to install vagrant and virtualbox. Then we can use a lightweight virtualbox in which we can run Docker (this is just what boot2docker does in windows and mac, in fact, we’ll use an image based on boot2docker since it has just what we need, and at just 24 MB is way smaller than a standard ubuntu virtualbox image).&lt;/p&gt;
&lt;p&gt;A Vagrantfile for getting up and running with this image is found in the &lt;code&gt;vagrant&lt;/code&gt; directory. This handles exporting ports to the host machine, and sharing files with the host machine (which can also be tricky for boot2docker users on Mac/Windows, so this might be a work-around for them). Run this image with:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;vagrant&lt;/span&gt; up&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may want to first adjust the Vagrantfile to allocate more or less memory and CPU nodes to the virtual machine. Vagrant’s default is 512 MB and 1 CPU, though the file currently puts this at 2 CPUS and 1024 MB RAM.&lt;/p&gt;
&lt;p&gt;You can now connect to the RStudio server using some ssh port forwarding:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ssh&lt;/span&gt; -o &lt;span class=&quot;st&quot;&gt;&amp;quot;ExitOnForwardFailure yes&amp;quot;&lt;/span&gt; -f -N -L 8787:localhost:8888 &lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;your.server&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first number (8787) is the port we want things to appear on localhost, e.g. we can now connect by visiting http://localhost:8787. The second (8888) is the port we configured for the host machine with Vagrant.&lt;/p&gt;
&lt;p&gt;Note that sometimes you need to tunnel through a head node to the compute node where you want R to be running, replacing &lt;code&gt;&amp;lt;your.server&amp;gt;&lt;/code&gt; with:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ssh&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;head.node.name&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; nc -q0 &lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt;compute.node.name&lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; 22&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Pdg Controlfest Notes</title>
	 <link href="/2014/08/14/pdg-controlfest-notes.html"/>
   <updated>2014-08-14T00:00:00+00:00</updated>
   <id>/08/14/pdg-controlfest-notes</id>
   <content type="html">&lt;p&gt;Just wanted to give a quick update on stuff relevant to our adjustment costs paper in events of this week.&lt;/p&gt;
&lt;p&gt;I think the talk on Tuesday went all right, (though thanks to a technology snafu going from reveal.js to pdf my most useful figure actually showing the bluefin tuna didn’t display – I tried not to let on). I tried to keep the focus pretty big-picture throughout (we ignore these costs when we model, they matter) and avoid being too bold / prescriptive (e.g. not suggesting we found the ‘right’ way to model these costs). I also could not stop myself from calling the adjustment cost models L1 L2 L3 instead of “linear” “quadratic” and “fixed”, or _1,2,3. whoops.&lt;/p&gt;
&lt;p&gt;One question asked about asymmetric costs. You may recall we started off doing but ran into some unexpected results where they just looked like the cost free case, possibly due to problems with the code. We should probably at least say this is an area for further study.&lt;/p&gt;
&lt;p&gt;Another question asked about just restricting the period of adjustment, say, once every 5 years or so. I answered that we hoped to see what cost structures “induced” that behavior rather than enforcing it explicitly; but I should probably add some mention of this to the text as well.&lt;/p&gt;
&lt;p&gt;I think the other questions were more straight forward but don’t remember any particulars.&lt;/p&gt;
&lt;p&gt;The Monday meeting was very helpful for me in framing the kind of big questions around the paper:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Can we make this story about more than TAC-managed fisheries? My ideal paper would be something people could cite to show that simply using profit functions with diminishing returns is not a sufficient way to reflect this reality (could be the opposite if reality is more like a transaction fee), and that this mistake can be large. But all our examples are in the fisheries context, so this may take some finesse. (Since we’re aiming for Eco Apps rather than, say, Can Jor Fisheries)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Emphasizing the “Pretty Darn Good” angle – thinking of the policies we derive with adjustment costs not as the “True optimum” but as a “Pretty Darn Good” policy that can be more robust to adjustment costs – (Provided you have intuition to know if those costs are more like a fixed transaction fee or some proportional cost). The last two figures help with this, since they show using policies under different cost regimes than those under which they were computed to be optimal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Need to figure out what to say about policies that can ‘self-adjust’, e.g. when you don’t have to change the law to respond to the fluctuations. (Jim pointed out that Salmon are the best/only case where you can actually manage by “escapement” since you get a complete population census from the annual runs).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Stripping down the complexity of the charts&lt;/li&gt;
&lt;li&gt;Conversely, may need to show some examples of the fish stock dynamics (In search for simplicity I’ve focused almost all the graphs on harvest dynamics).&lt;/li&gt;
&lt;li&gt;Calibrating and running the case of quadratic control term for comparison&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a bonus, I quickly ran the tipping point models, and it looks like these stay really close to the Reed solution – e.g. relative to the safer Beverton Holt world, they are much happier to pay whatever the adjustment cost might be to stick with the optimal than they are to risk total collapse. Not sure but maybe should add this into the paper…&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Docker Notes</title>
	 <link href="/2014/08/14/docker-notes.html"/>
   <updated>2014-08-14T00:00:00+00:00</updated>
   <id>/08/14/docker-notes</id>
   <content type="html">&lt;p&gt;Ticking through a few more of the challenges I raised in my &lt;a href=&quot;http://www.carlboettiger.info/2014/08/07/too-much-fun-with-docker.html&quot;&gt;first post on docker&lt;/a&gt;; here I explore some of the issues about facilitating interaction with a docker container so that a user’s experience is more similar to working in their own environment and less like working on a remote terminal over ssh. While technically minor, these issues are probably the first stumbling blocks in making this a valid platform for new users.&lt;/p&gt;
&lt;h2 id=&quot;sharing-a-local-directory&quot;&gt;Sharing a local directory&lt;/h2&gt;
&lt;p&gt;Launch a bash shell on the container that shares the current working directory of the host machine (from &lt;code&gt;pwd&lt;/code&gt;) with the &lt;code&gt;/host&lt;/code&gt; directory on the container (thanks to Dirk for this solution):&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v &lt;span class=&quot;ot&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;ot&quot;&gt;)&lt;/span&gt;:/host cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows a user to move files on and off the container, use a familiar editor and even handle things like git commits / pulls / pushes in their working directory as before. Then the code can be executed in the containerized environment which handles all the dependencies. From the terminal docker opens, we just &lt;code&gt;cd /host&lt;/code&gt; where we find our working directory files, and can launch R and run the scripts. A rather clean way of maintaining the local development experience but containerizing execution.&lt;/p&gt;
&lt;p&gt;In particular, this frees us from having to pass our git credentials etc to the container, though is not so useful if we’re wanting to interact with the container via the RStudio server instead of R running in the terminal. (More on getting around this below).&lt;/p&gt;
&lt;p&gt;Unfortunately, Mac and Windows users have to run Docker inside an already-virualized environment such as provided by &lt;code&gt;boot2docker&lt;/code&gt; or &lt;code&gt;vagrant&lt;/code&gt;. This means that it is only the directories on the virtualized environment, not those on the native OS, can be shared in this way. While one could presumably keep a directory synced between this virtual environment and the native OS, (standard in in &lt;code&gt;vagrant&lt;/code&gt;), this is a problem for the easier-to-use &lt;code&gt;boot2docker&lt;/code&gt; at this time: (&lt;a href=&quot;https://github.com/docker/docker/issues/7249&quot;&gt;docker/issues/7249&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;a-docker-desktop&quot;&gt;A Docker Desktop&lt;/h2&gt;
&lt;p&gt;Dirk brought this &lt;a href=&quot;http://blog.docker.com/2013/07/docker-desktop-your-desktop-over-ssh-running-inside-of-a-docker-container&quot;&gt;docker-desktop&lt;/a&gt; to my attention; which uses Xpra (in place of X11 forwarding) to provide a window with fluxbox running on Ubuntu along with common applications like libreoffce, firefox, and rox file manager. Pretty clever, and worked just fine for me, but needs Xpra on the client machine and requires some extra steps (run the container, query for passwords and ports, run ssh to connect, then run Xpra to launch the window). The result is reasonably responsive but still slower than virtualbox, and probably too slow for real work.&lt;/p&gt;
&lt;h2 id=&quot;base-images&quot;&gt;Base images?&lt;/h2&gt;
&lt;p&gt;The basic Ubuntu:14.04 seems like a good lightweight base image (at 192 MB), but other images try to give more useful building blocks, like &lt;a href=&quot;https://github.com/phusion/baseimage-docker#contents&quot;&gt;phusion/baseimage&lt;/a&gt; (423 MB). Their &lt;code&gt;docker-bash&lt;/code&gt; script and other utilities provide some handy features for managing / debugging containers.&lt;/p&gt;
&lt;h2 id=&quot;other-ways-to-share-files&quot;&gt;Other ways to share files?&lt;/h2&gt;
&lt;p&gt;Took a quick look at this &lt;a href=&quot;https://github.com/gfjardim/docker-dropbox/blob/master/Dockerfile&quot;&gt;Dockerfile for running dropbox&lt;/a&gt;, which works rather well (at least on a linux machine, since it requires local directory sharing). Could probably be done without explicit linking to local directories to faciliate moving files on and off the container. Of course one can always scp/rsync files on and off containers if ssh is set up, but that is unlikely to be a popular solution for students.&lt;/p&gt;
&lt;p&gt;While we have rstudio server running nicely in a Docker container for local or cloud use, it’s still an issue getting Github ssh keys set up to be able to push changes to a repo. We can get around this by linking to our keys directory with the same &lt;code&gt;-v&lt;/code&gt; option shown above. We still need a few more steps: setting the Git username and email, and running &lt;code&gt;ssh-add&lt;/code&gt; for the key. Presumably we could do this with environmental variables and some adjustment to the Dockerfile:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;docker&lt;/span&gt; run -it -v /path/to/keys:/home/rstudio/.ssh/ -e &lt;span class=&quot;st&quot;&gt;&amp;quot;USERNAME=Carl Boettiger&amp;quot;&lt;/span&gt; -e &lt;span class=&quot;st&quot;&gt;&amp;quot;EMAIL=cboettig@example.org&amp;quot;&lt;/span&gt; cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would prevent storing these secure values on the image itself.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An appropriate amount of fun with docker?</title>
	 <link href="/2014/08/08/an-appropriate-amount-of-fun-with-docker.html"/>
   <updated>2014-08-08T00:00:00+00:00</updated>
   <id>/08/08/an-appropriate-amount-of-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;em&gt;An update on my exploration with Docker. Title courtesy of &lt;a href=&quot;https://twitter.com/DistribEcology/status/497523435371638784&quot;&gt;Ted&lt;/a&gt;, with my hopes that this really does move us in a direction where we can spend less time thinking about the tools and computational environments. Not there yet though&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I’ve gotten RStudio Server working in the &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;ropensci-docker&lt;/a&gt; image (Issues/pull requests welcome!).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 8787:8787 cboettig/ropensci-docker&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will make an RStudio server instance available to you in your browser at localhost:8787. (Change the first number after the -p to have a different address). You can log in with username:pw rstudio:rstudio and have fun.&lt;/p&gt;
&lt;p&gt;One thing I like about this is the ease with which I can now get an RStudio server up and running in the cloud (e.g. I took this for sail on DigitalOcean.com today). This means in few minutes and 1 penny you have a URL that you and any collaborators could use to interact with R using the familiar RStudio interface, already provisioned with your data and dependencies in place.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For me this is a pretty key development. It replaces a lot of command-line only interaction with probably the most familiar R environment out there, online or off. For more widespread use or teaching this probably needs to get simpler still. I’m still skeptical that this will make it out beyond the crazies, but I’m less skeptical than I was when starting this out.&lt;/p&gt;
&lt;p&gt;The ropensci-docker image could no doubt be more modular (and better documented). I’d be curious to hear if anyone has had success or problems running docker on windows / mac platforms. Issues or pull requests on the repo would be welcome! https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile (maybe the repo needs to be renamed from it’s original fork now too…)&lt;/p&gt;
&lt;p&gt;Rich et al highlighted several “remaining challenges” in their original post. Here’s my take on where those stand in the Docker framework, though I’d welcome other impressions:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;dependencies could still be missed by incompletely documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think this one is largely addressed, at least assuming a user loads the Docker image. I’m still concerned that later builds of the docker image could simply break the build (though earlier images may still be available). Does anyone know how to roll back to earlier images in docker?&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The set of scripts for managing reproducibility are at least as complex as the analysis itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think a lot of that size is due to the lack of an R image for Travis and the need to install many common tools from scratch. Because docker is both modular and easily shared via docker hub, it’s much easier to write a really small script that builds on existing images, (as I show in cboettig/rnexml)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Travis.org CI constraints: public/open github repository with analyses that run in under 50 minutes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Docker has two advantages and also some weaknesses here: (1) it should be easy to run locally, while accomplishing much of the same thing as running on travis (though clearly that’s not as nice as running automatically &amp;amp; in the cloud on every push). (2) It’s easier to take advantage of caching – for instance, cboettig/rnexml provides the knitr cache files in the image so that a user can start exploring without waiting for all the data to download and code to run.&lt;/p&gt;
&lt;p&gt;It seems that Travis CI doesn’t currently support docker since the linux kernel they use is too old. (Presumably they’ll update one day. Anyone try Shippable CI? (which supports docker))&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;The learning curve is still prohibitive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think that’s still true. But what surprised me is that I’m not sure that it’s gotten any worse by adding docker than it was to begin with using Travis CI. Because the approach can be used both locally and for scaling up in the cloud, I think it offers some more immediate payoffs to users than learning a Github+CI approach does. (Notably it doesn’t require any git just to deploy something ‘reproducible’, though of course it works nicely with git.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Too Much Fun With Docker</title>
	 <link href="/2014/08/07/too-much-fun-with-docker.html"/>
   <updated>2014-08-07T00:00:00+00:00</updated>
   <id>/08/07/too-much-fun-with-docker</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This post was originally drafted as a set of questions to the revived &lt;a href=&quot;https://groups.google.com/forum/#!forum/ropensci-discuss&quot;&gt;ropensci-discuss list&lt;/a&gt;, hopefully readers might join the discussion from there.&lt;/p&gt;
&lt;p&gt;Been thinking about Docker and the discussion about reproducible research in the comments of Rich et al’s recent post on the &lt;a href=&quot;ropensci.org/blog/2014/06/09/reproducibility/&quot;&gt;rOpenSci blog&lt;/a&gt; where quite a few of people mentioned the potential for Docker as a way to facilitate this.&lt;/p&gt;
&lt;p&gt;I’ve only just started playing around with Docker, and though I’m quite impressed, I’m still rather skeptical that non-crazies would ever use it productively. Nevertheless, I’ve worked up some Dockerfiles to explore how one might use this approach to transparently document and manage a computational environment, and I was hoping to get some feedback from all of you.&lt;/p&gt;
&lt;p&gt;For those of you who are already much more familiar with Docker than me (or are looking for an excuse to explore!), I’d love to get your feedback on some of the particulars. For everyone, I’d be curious what you think about the general concept.&lt;/p&gt;
&lt;p&gt;So far I’ve created a &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/blob/master/add-r-ropensci/Dockerfile&quot;&gt;dockerfile&lt;/a&gt; and &lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/ropensci-docker/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have docker up and running, perhaps you can give it a test drive:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/ropensci-docker /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should find R installed with some common packages. This image builds on Dirk Eddelbuettel’s R docker images and serves as a starting point to test individual R packages or projects.&lt;/p&gt;
&lt;p&gt;For instance, my RNeXML manuscript draft is a bit more of a bear then usual to run, since it needs &lt;code&gt;rJava&lt;/code&gt; (requires external libs), &lt;code&gt;Sxslt&lt;/code&gt; (only available on Omegahat and requires extra libs) and latest &lt;code&gt;phytools&lt;/code&gt; (a tar.gz file from Liam’s website), along with the usual mess of pandoc/latex environment to compile the manuscript itself. By building on ropensci-docker, we need a &lt;a href=&quot;https://github.com/ropensci/RNeXML/tree/master/manuscripts/Dockerfile&quot;&gt;pretty minimal docker file&lt;/a&gt; to compile this environment:&lt;/p&gt;
&lt;p&gt;You can test drive it (&lt;a href=&quot;https://registry.hub.docker.com/u/cboettig/rnexml&quot;&gt;docker image here&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it cboettig/rnexml /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once in bash, launch R and run &lt;code&gt;rmarkdown::render(&amp;quot;manuscript.Rmd&amp;quot;)&lt;/code&gt;. This will recompile the manuscript from cache and leave you to interactively explore any of the R code shown.&lt;/p&gt;
&lt;h2 id=&quot;advantages-goals&quot;&gt;Advantages / Goals&lt;/h2&gt;
&lt;p&gt;Being able to download a pre-compiled image means a user can run the code without dependency hell (often not as much an R problem as it is in Python, but nevertheless one that I hit frequently, particularly as my projects age), and also without altering their personal R environment. Third (in principle) this makes it easy to run the code on a cloud server, scaling the computing resources appropriately.&lt;/p&gt;
&lt;p&gt;I think the real acid test for this is not merely that it recreates the results, but that others can build and extend on the work (with fewer rather than more barriers than usual). I believe most of that has nothing to do with this whole software image thing – providing the methods you use as general-purpose functions in an R package, or publishing the raw (&amp;amp; processed) data to Dryad with good documentation will always make work more modular and easier to re-use than cracking open someone’s virtual machine. But that is really a separate issue.&lt;/p&gt;
&lt;p&gt;In this context, we look for an easy way to package up whatever a researcher or group is already doing into something portable and extensible. So, is this really portable and extensible?&lt;/p&gt;
&lt;h2 id=&quot;concerns&quot;&gt;Concerns:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;This presupposes someone can run docker on their OS – and from the command line at that. Perhaps that’s the biggest barrier to entry right now, (though given docker’s virulent popularity, maybe something smart people with big money might soon solve).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The only way to interact with thing is through a bash shell running on the container. An RStudio server might be much nicer, but I haven’t been able to get that running. &lt;em&gt;Anyone know how to run RStudio server from docker?&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(I tried &amp;amp; &lt;a href=&quot;https://github.com/mingfang/docker-druid/issues/2&quot;&gt;failed&lt;/a&gt;)&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;I don’t see how users can move local files on and off the docker container. In some ways this is a great virtue – forcing all code to use fully resolved paths like pulling data from Dryad instead of their hard-drive, and pushing results to a (possibly private) online site to view them. But obviously a barrier to entry. &lt;em&gt;Is there a better way to do this?&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;alternative-strategies&quot;&gt;Alternative strategies&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Docker is just one of many ways to do this (particularly if you’re not concerned about maximum performance speed), and quite probably not the easiest. Our friends at Berkeley D-Lab opted for a GUI-driven virtual machine instead, built with Packer and run in Virtualbox, after their experience proved that students were much more comfortable with the mouse-driven installation and a pixel-identical environment to the instructor’s (see their excellent &lt;a href=&quot;https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl&quot;&gt;paper&lt;/a&gt; on this).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Will/should researchers be willing to work and develop in virtual environments? In some cases, the virtual environment can be closely coupled to the native one – you use your own editors etc to do all the writing, and then execute in the virtual environment (seems this is easier in docker/vagrant approach than in the BCE.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--

I would like a way for a collaborator who knows a little R to be able to open my .Rmd manuscript on his/her own computer, edit some parameters, recompile and view the pdf. RStudio w/ rmarkdown has gone a long way to making that happen.
--&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/08/06/notes.html"/>
   <updated>2014-08-06T00:00:00+00:00</updated>
   <id>/08/06/notes</id>
   <content type="html">&lt;h2 id=&quot;writing&quot;&gt;Writing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Working on / finishing up &lt;a href=&quot;http://io.carlboettiger.info/pdg_control/presentation.html&quot;&gt;slides&lt;/a&gt; for &lt;a href=&quot;http://eco.confex.com/eco/2014/webprogram/Session9683.html&quot;&gt;ESA talk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and slides for &lt;a href=&quot;http://hastingsfest.wordpress.com&quot;&gt;hastingsfest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;exploring&quot;&gt;Exploring&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Poking around a bit more in Docker yesterday, built some basic R containers for testing ropensci packages. &lt;a href=&quot;https://github.com/ropensci/docker-ubuntu-r/tree/master/add-r-ropensci&quot; class=&quot;uri&quot;&gt;https://github.com/ropensci/docker-ubuntu-r/tree/master/add-r-ropensci&lt;/a&gt; . Somewhat skeptical but still curious about the potential (or the need) to publish a docker image as a reproducible research object.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Attempted to add Travis builds to the Docker repo (just for fun, eh?), but appears Docker doesn’t support the older kernel of 12.04&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Attempted an i386 build for the BCE, but no luck. &lt;a href=&quot;https://github.com/cboettig/collaboratool/blob/master/provisioning/BCE-14.04-i386.json&quot; class=&quot;uri&quot;&gt;https://github.com/cboettig/collaboratool/blob/master/provisioning/BCE-14.04-i386.json&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;p&gt;Fun &lt;a href=&quot;http://www.theguardian.com/higher-education-network/blog/2014/aug/05/why-we-should-publish-less-scientific-research?CMP=twt_gu&quot;&gt;piece in the Guardian&lt;/a&gt; from Digital Science manager Timo Hannay on the future of scientific publishing. I think (or choose to believe that) the thesis is at the end rather than in the title.&lt;/p&gt;
&lt;p&gt;Also commented here on DocZen’s &lt;a href=&quot;http://neurodojo.blogspot.com/2014/08/better-deluge-than-drought.html&quot;&gt;post&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/07/31/notes.html"/>
   <updated>2014-07-31T00:00:00+00:00</updated>
   <id>/07/31/notes</id>
   <content type="html">&lt;h2 id=&quot;berkeley-collaborative-environment&quot;&gt;Berkeley Collaborative Environment&lt;/h2&gt;
&lt;p&gt;Trying out the Berkeley image (Essentially ubuntu 14.04 XFCE with ipython and RStudio installed, but finely tuned to improve user experience; e.g. solid colors for faster remote window connections.)&lt;/p&gt;
&lt;p&gt;Highly recommend &lt;a href=&quot;https://t.co/TJ9ObbL8i8&quot;&gt;their paper&lt;/a&gt; for the first explanation I’ve actually been able to follow that provides a definition of “DevOpts” (essentially, using scripts rather than documentation to manage consistent cross-platform installation) and explains the differences and similarities between the various programs operating in this sphere, sometimes as alternatives and simultaneously.&lt;/p&gt;
&lt;h4 id=&quot;virtual-machines.&quot;&gt;Virtual machines.&lt;/h4&gt;
&lt;p&gt;Their approach focuses on running on top of a complete virtual machine. Primarily considers Oracle’s &lt;code&gt;virtualbox&lt;/code&gt; for local use, Amazon’s AMI for cloud use. (For an emerging alternative to the full virtual machine approach, they discuss Docker).&lt;/p&gt;
&lt;h4 id=&quot;configuration-management-cm-tools&quot;&gt;Configuration management (CM) tools:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ansible&lt;/strong&gt; (“playbooks”) used by the BCE to specify the complete software environment. Compares to: &lt;strong&gt;Chef&lt;/strong&gt; (Ruby-based, “recipes”), &lt;strong&gt;Salt&lt;/strong&gt; (Python based, “states”), and &lt;strong&gt;Puppet&lt;/strong&gt; (“manifests”).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;provisioning-tools&quot;&gt;Provisioning tools:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Packer&lt;/strong&gt; Used at build time to create a machine image for the VM. Packer can use Ansible/Chef/Salt/Puppet files to do this. Results in a nice AMI for Amazon web console or an image for the virtualbox GUI.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Vagrant&lt;/strong&gt; Offers a different approach. Rather than the developer creating a VM image using Packer and Ansible script that is ready to run on virtualbox or Amazon, the end user installs vagrant instead of virtualbox. Vagrant also handles the job of Packer, in preparing an environment to run (on Vagrant’s virtual machine, rather than on virtualbox). (Note that conversely, Packer can create a Vagrant virtual machine just as easily as it can create the Oracle virtualbox or Amazon AMI). Vagrant feels a lot more native, as the user works within their familiar OS tools for editing, etc, while vagrant makes sure that the execution of the software happens in a controlled, identical environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt; offers a more modular alternative to a full virtual machine, with performance that is more like running on ‘bare metal’, sharing the kernel of the native OS; though at the moment it requires that be a linux kernel. Docker is typically deployed using Vagrant, (though stand-alone setup is emerging).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From first glance, Vagrant sounds like a more elegant approach than having end users install Oracle’s virtual machine and then learn to live in a virtual window emulating the Ubuntu-XFCE desktop (particularly if those users are developers!). However, it seems that the BCE team found that Vagrant was harder for students to work with, since it required knowledge of the commandline.&lt;/p&gt;
&lt;p&gt;The paper also provides a fabulous case study of a major scientific software project using the “DevOpts” philosophy but without any of these new and emerging tools, relying only on scripts, makefiles, and Linux distribution package managers.&lt;/p&gt;
&lt;h3 id=&quot;test-drive-impressions&quot;&gt;Test drive impressions&lt;/h3&gt;
&lt;p&gt;Unfortunately, no luck getting virtualization running on my laptop (due to &lt;a href=&quot;ttps://github.com/dlab-berkeley/collaboratool/issues/created_by/cboettig&quot;&gt;BIOS issues&lt;/a&gt;). Testing on Ubuntu desktop required a newer version of virtualbox than what’s in the 12.04 repos, but otherwise just worked. It’s straight forward to install additional needed software, and virtualbox gives the option of preserving the machine state on exit, presumably with the software. Not clear how that should be managed to avoid re-creating the problems that using a consistent image set out to avoid in the first place; perhaps requires re-provisioning a divergent image?&lt;/p&gt;
&lt;p&gt;Nice experience testing out the Amazon machine image, but I think the workflow would be improved if it provided RStudio server for a more interactive interface.&lt;/p&gt;
&lt;h2 id=&quot;misc-tasks&quot;&gt;Misc tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Working on reveal.js slides&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RNeXML manuscript edits from Francois’s comments&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;thoughts-on-namespaces-in-r&quot;&gt;Thoughts on namespaces (in R)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Chatting with Scott about namespace practices, thought I’d put some of this down.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I go back and forth on having a small namespace (particularly vs convenience functions). It is certainly easier to maintain, but from the user’s perspective I think it’s pretty easy to just ignore extra functions; as long as it’s well documented what functions they need to know to get started.&lt;/p&gt;
&lt;p&gt;I guess if a user needs to know too many functions to do anything, than it becomes hard to keep track of. That’s partly why I started wrapping exposed functions. For instance, you can just use &lt;code&gt;nexml_write&lt;/code&gt; all the time and never use add_characters, add_trees, etc, since &lt;code&gt;nexml_write&lt;/code&gt; takes additional trees and characters as arguments.&lt;/p&gt;
&lt;p&gt;But I like function calls to be as semantic as possible so the code is more self documenting. Sometimes &lt;code&gt;add_characters(nex)&lt;/code&gt; is more self explanatory than &lt;code&gt;nexml_write(nex, characters = characters)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The XML package really got me started on this. There’s usually like 4 or 5 ways to do the same thing. Sometimes that’s really annoying, but sometimes it helps write more transparent code or less verbose code (e.g. adding child nodes with &lt;code&gt;addChildren&lt;/code&gt; vs passing as &lt;code&gt;.children&lt;/code&gt; argument to &lt;code&gt;newXMLNode&lt;/code&gt; – the former tends to be more semantic, the latter often more consise).&lt;/p&gt;
&lt;h2 id=&quot;code-tricks-vim-pandoc&quot;&gt;Code tricks: vim pandoc&lt;/h2&gt;
&lt;p&gt;Vim pandoc syntax highlighting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recognize &lt;code&gt;.Rmd&lt;/code&gt; as a pandoc-syntax file extension. In &lt;code&gt;.vimrc&lt;/code&gt; do:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;vim&quot;&gt;&lt;code&gt;au BufRead,BufNewFile *.Rmd set filetype=pandoc&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Enable syntax highlighting inside code blocks, by language. In vim session, do: &lt;code&gt;PandocHighlight r&lt;/code&gt;. Unfortunately doesn’t recognize the default &lt;code&gt;.Rmd&lt;/code&gt; format used by RStudio. This then enables syntax highlighting, folding, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If I recall correctly, knitr’s default markdown syntax,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;was intended to be a valid markdown syntax. It seems Github Flavored markdown is happy to recognize this as markdown syntax for a code block in the R language, but while pandoc recognizes this as a code block, it does not recognize the language.&lt;/p&gt;
&lt;p&gt;I believe Pandoc does recognize the format&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{.r options}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as the notation to specify the R language in this notation. I like this notation because it means that pandoc-aware syntax highlighters will highlight my code chunks as R code, which does not happen in the default syntax.&lt;/p&gt;
&lt;p&gt;I realize I could define this as an input hook for knitr, but am reluctant to do so as it makes my code slightly less familiar/less portable to other users (e.g. RStudio expects and integrates with only the standard notation.)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/07/21/notes.html"/>
   <updated>2014-07-21T00:00:00+00:00</updated>
   <id>/07/21/notes</id>
   <content type="html">&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;p&gt;Looking for this in my notes and couldn’t find it: An excellent paper from the Software Sustainability Institute and friends outlining the need and possible structure for sustaining career paths for software developers. “&lt;a href=&quot;http://dirkgorissen.com/2012/09/13/the-research-software-engineer/&quot;&gt;The research software engineer&lt;/a&gt;”, Dirk Gorissen. Provides a good response to the &lt;a href=&quot;http://www.timeshighereducation.co.uk/news/save-your-work-give-software-engineers-a-career-track/2006431.article&quot;&gt;software issues highlighted by climategate&lt;/a&gt;, etc.&lt;/p&gt;
&lt;h2 id=&quot;remote-conferencing&quot;&gt;Remote conferencing&lt;/h2&gt;
&lt;p&gt;(Based on earlier unposted notes). With so much going on, it’s nice to be able to follow highlights from some conferences remotely&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From ISIC: Ben Bolker’s &lt;a href=&quot;http://t.co/ft2sJjRNFp&quot;&gt;slides on statistial machismo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From BOSC: C. Titus Brown’s keynote &lt;a href=&quot;http://ivory.idyll.org/blog/2014-bosc-keynote.html&quot;&gt;A history of bioinformatics (in 2039)&lt;/a&gt; (see links to slides, storify).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From PyCon: Greg Wilson’s talk (&lt;a href=&quot;https://www.youtube.com/watch?v=1e26rp6qPbA&quot;&gt;youtube recording&lt;/a&gt;) is fantastic, ending with compelling thesis for large-scale collaboration via diff/merge as the foundation of transformational change / open science, and the puzzling case of why this model has not yet been more widely adopted in generating teaching materials. Greg mentions several excellent resources in the talk as well: &lt;a href=&quot;http://www.slideshare.net/richardcookau/john-hattie-effect-sizes-on-achievement&quot;&gt;John Hattie’s slides on effect sizes of different classroom methods&lt;/a&gt;, Carnegie Mellon’s summary text on decades of research, “How Learning Works” (&lt;a href=&quot;http://c4ed.lib.kmutt.ac.th/sites/default/files/HowLearningWorks-Ambrose.pdf&quot;&gt;pdf&lt;/a&gt;), and Mark Guzdial’s &lt;a href=&quot;http://computinged.wordpress.com/about/&quot;&gt;blog on CS education&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From ESIP: Daniel Katz’s slides on &lt;a href=&quot;http://t.co/1fUkycXAMP&quot;&gt;Working towards Sustainable Software for Science (an NSF and community view)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;misc-code-tricks&quot;&gt;Misc code-tricks&lt;/h2&gt;
&lt;p&gt;For a question raised during the Mozilla sprint: had to remember how to write custom hooks for knitr (e.g. for kramdown compatibility):&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;hook.t &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x, options) &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;~~~&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(x, &lt;span class=&quot;dt&quot;&gt;collapse=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;), &lt;span class=&quot;st&quot;&gt;&amp;quot;~~~&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;)
hook.r &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x, options) {
       &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;~~~ &amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;tolower&lt;/span&gt;(options$engine), &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;, &lt;span class=&quot;kw&quot;&gt;paste0&lt;/span&gt;(x, &lt;span class=&quot;dt&quot;&gt;collapse=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;), &lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;~~~&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;)
  }
knitr::knit_hooks$&lt;span class=&quot;kw&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;source=&lt;/span&gt;hook.r, &lt;span class=&quot;dt&quot;&gt;output=&lt;/span&gt;hook.t, &lt;span class=&quot;dt&quot;&gt;warning=&lt;/span&gt;hook.t,
                              &lt;span class=&quot;dt&quot;&gt;error=&lt;/span&gt;hook.t, &lt;span class=&quot;dt&quot;&gt;message=&lt;/span&gt;hook.t)&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>UPS and data vs optimal control</title>
	 <link href="/2014/07/21/UPS-and-data-vs-optimal-control.html"/>
   <updated>2014-07-21T00:00:00+00:00</updated>
   <id>/07/21/UPS-and-data-vs-optimal-control</id>
   <content type="html">&lt;p&gt;&lt;em&gt;Random idea for possible further exploration:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The use of ‘big data’ by UPS to perform lots of small efficiency gains seems to be everybody’s favorite example (&lt;a href=&quot;http://www.npr.org/blogs/money/2014/05/02/308640135/episode-536-the-future-of-work-looks-like-a-ups-truck&quot;&gt;NPR&lt;/a&gt;, &lt;a href=&quot;http://www.economist.com/news/business/21607816-businesses-should-aim-lots-small-wins-big-data-add-up-something-big-little&quot;&gt;The Economist&lt;/a&gt;). During a typical applications of optimal control for ecological conservation talk yesterday I couldn’t help thinking back to that story. The paradigm shift is not so much the kind or amount of the data being used as it is the control levers themselves. As the Economist (rightly) argues, everyone typically assumes that a few principle actions are responsible for 80% of possible improvement.&lt;/p&gt;
&lt;p&gt;Optimal control tends to focus on these big things, which are also usually particularly thorny optimizations. Most of the classic textbook hard optimization problems could have come right from the UPS case: the traveling salesman, the inventory packing/set cover problems, and so forth. Impossible to solve exactly on large networks, approximate dynamic programming approaches have since been the work-around. Yet the “Big Data” approach takes a rather different strategy all together, tackling many small problems instead of one big one. Our typical approach of theoretical abstractions to simple models is designed to focus on these big overarching problems. In abstracting the problem, we focus on the big picture stuff that should matter most – stuff like figuring out the optimal route to travel, and so forth. But when the gains through increasing optimization of these things are marginal, focusing on the “other 20%” can make more sense. However, that means abandoning the abstraction and going back to the original messy problem. It means knowing about all the other little levers and switches we can control. In the UPS context, this means thinking about how many times a truck backs up, or idles at a stop light, or what hand the deliveryman holds the pen in. Given both the data and the ability to control so many of these little things, optimizing each one in the first place can be more valuable than focusing on the big abstract optimizations.&lt;/p&gt;
&lt;p&gt;So, does this work only once the heuristic solutions to the big problems are nearly optimal, so improved approximations have very limited gains? Or can this also be a route forward when the big problems are primarily intractable as well? The former certainly seems the more likely, but if the latter is true, it could prove very interesting.&lt;/p&gt;
&lt;p&gt;So this got me thinking – if we accept the latter premise we find a case closely analogous to the very messy optimizations we face in conservation decision-making. Could the many little levers be an alternative? It’s unlikely given both the need for the kind of arbitrarily detailed data at almost no cost available to the UPS problem, and also the kind of totalitarian control UPS can apply to control all the little levers, while the conservation problem more frequently has nothing bit a scrawny blunt stick to toggle in the first place. Nevertheless, it’s hard to know what possible gains we have already excluded when we focus only on the big abstractions and the controls relevant to them. Could conservation decision-making think more outside the box about the many little things we might be able to more effectively influence?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/07/18/notes.html"/>
   <updated>2014-07-18T00:00:00+00:00</updated>
   <id>/07/18/notes</id>
   <content type="html">&lt;h2 id=&quot;cran-trivia&quot;&gt;CRAN trivia&lt;/h2&gt;
&lt;h3 id=&quot;when-should-you-bump-version-for-a-rejected-resubmission&quot;&gt;When should you bump version for a (rejected) resubmission?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Once accepted, any change other than to the metadata (essentially the DESCRIPTION file) needs an increased version. For submissions, we prefer (but do not insist on) a new number of each attempt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;–Prof Brian Ripley&lt;/p&gt;
&lt;h3 id=&quot;using-non-cran-repositories-in-suggests-or-enahances&quot;&gt;Using non-CRAN repositories in SUGGESTS or ENAHANCES&lt;/h3&gt;
&lt;p&gt;More dubious tricks, from Yihui:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FYI, here is how R core checks dependencies: https://github.com/wch/r-source/blob/trunk/src/library/tools/R/QC.R#L5195&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Because I know this, sometimes I intentionally use something like (function(pkg) library(pkg, character.only = TRUE))(“foo”) to silence R CMD check and cheat when I (optionally) need a package but do not want CRAN maintainers to know it&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;package-maintainence&quot;&gt;Package maintainence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;knitcitations 0.1.1 on CRAN now.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See RNeXML check results: http://cran.r-project.org/web/checks/check_results_RNeXML.html&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RNeXML Submitted a series of patches that allow tests not to fail when external resources (packages, web APIs) that are not available.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rfigshare updated &lt;a href=&quot;https://github.com/ropensci/rfigshare/issues/84&quot;&gt;#84&lt;/a&gt;. Triggered occassional errors from API failing, so most tests now skipped if authentication call fails. (Reworked authentication a bit)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;bugfix: &lt;a href=&quot;https://github.com/cboettig/knitcitations/issues/63#issuecomment-49459723&quot;&gt;knitcitations/#63&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;
&lt;p&gt;Taking a look at &lt;a href=&quot;&quot;&gt;auto&lt;/a&gt; for bifurcation diagrams (ht Noam, who is using the XPP wrapper).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes On Tricks In Manuscript Submission And Collaboration</title>
	 <link href="/2014/07/10/notes-on-tricks-in-manuscript-submission-and-collaboration.html"/>
   <updated>2014-07-10T00:00:00+00:00</updated>
   <id>/07/10/notes-on-tricks-in-manuscript-submission-and-collaboration</id>
   <content type="html">&lt;h2 id=&quot;some-thoughts-on-collaborating-with-markdown-dynamic-document-workflow&quot;&gt;Some thoughts on collaborating with markdown / dynamic document workflow&lt;/h2&gt;
&lt;p&gt;Collaborating on manuscripts with other researchers when not writing in MS Word has been a perpetual nuisance for those of us not using Word (no doubt the others might say the same). When I first began writing papers I worked in LaTeX, and at that time I collaborated largely with others who already knew TeX (e.g. my mentors), so this wasn’t much of a problem. While moving my workflow into markdown has simplified collaborations with (often junior) researchers who know markdown better than tex, it has managed to make the potential for mismatches even greater, as it creates a barrier for both co-authors working in LaTeX as well as those working in Word.&lt;/p&gt;
&lt;p&gt;This has always been more of a nuisance than a real problem. I’ve usually just sent co-authors some derived copy (e.g. a pdf, or sometimes creating a Word or TeX document from the markdown using pandoc and sending that). This means I have to transcribe or at least copy and paste the edits, though that’s never all that time consuming a process.&lt;/p&gt;
&lt;p&gt;I still have high hopes that RStudio’s &lt;code&gt;rmarkdown&lt;/code&gt; format will make it practical for co-authors to edit and compile my &lt;code&gt;.Rmd&lt;/code&gt; files directly. Meanwhile, a mentor who frequently uses LaTeX in collaborating with Word users suggested a much simpler solution that has proven very pratical for me so far.&lt;/p&gt;
&lt;h3 id=&quot;a-simple-solution&quot;&gt;A simple solution&lt;/h3&gt;
&lt;p&gt;Based on his suggestion, I just paste the contents of the &lt;code&gt;.Rmd&lt;/code&gt; file into a Word (well, LibreOffice) document and send that. Most collaborators can just ignore the code blocks and LaTeX equations, etc, and edit the text directly. I send the compiled pdf as well for the figures and rendered equations. A collaborator cannot easily re-compile their changes, but I can by copy-pasting back into the &lt;code&gt;.Rmd&lt;/code&gt; file. They can track changes via Word, and I can track the same changes through the version control when I paste their changes back in. It’s not perfect, but it’s simple.&lt;/p&gt;
&lt;h1 id=&quot;challenges-in-submission-systems&quot;&gt;Challenges in submission systems&lt;/h1&gt;
&lt;h2 id=&quot;transparent-figures&quot;&gt;transparent figures&lt;/h2&gt;
&lt;p&gt;I use semi-transparent graphs to show ensemble distributions of stochastic trajectories. First, a quick example as to why:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ggplot&lt;/span&gt;(sims_data) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;geom_line&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(time, fishstock, &lt;span class=&quot;dt&quot;&gt;group=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;interaction&lt;/span&gt;(reps,method), &lt;span class=&quot;dt&quot;&gt;color=&lt;/span&gt;method), &lt;span class=&quot;dt&quot;&gt;alpha=&lt;/span&gt;&lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;scale_colour_manual&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;values=&lt;/span&gt;colorkey, &lt;span class=&quot;dt&quot;&gt;guide =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;guide_legend&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;override.aes =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;alpha =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;))) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;facet_wrap&lt;/span&gt;(~method) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;guides&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;legend.position=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://io.carlboettiger.info/nonparametric-bayes/replicates.svg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Statistical summaries can get around this approach, but don’t really reflect the true rather binary nature of the ensemble (that some trajectories go to zero while others remain around a constant level):&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;ggplot&lt;/span&gt;(sims_data) +

&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;stat_summary&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(time, fishstock), &lt;span class=&quot;dt&quot;&gt;fun.data =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;mean_sdl&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;geom=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;ribbon&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fill=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;grey80&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;col=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;grey80&amp;#39;&lt;/span&gt;) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;stat_summary&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(time, fishstock, &lt;span class=&quot;dt&quot;&gt;col=&lt;/span&gt;method), &lt;span class=&quot;dt&quot;&gt;fun.y =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;mean&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;geom=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;line&amp;quot;&lt;/span&gt;) +

&lt;span class=&quot;co&quot;&gt;#  geom_line(aes(time, fishstock, group=interaction(reps,method), color=method), alpha=0.1) +&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;scale_colour_manual&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;values=&lt;/span&gt;colorkey, &lt;span class=&quot;dt&quot;&gt;guide =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;guide_legend&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;override.aes =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;alpha =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;))) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;facet_wrap&lt;/span&gt;(~method) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;guides&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;legend.position=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://io.carlboettiger.info/nonparametric-bayes/replicates-stat-summary-sdl.svg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Tweaking the statistical definitions can remove the more obvious errors from this, but still give the wrong impression:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;mymin &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x) &lt;span class=&quot;kw&quot;&gt;mean&lt;/span&gt;(x) -&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;sd&lt;/span&gt;(x)
&lt;span class=&quot;kw&quot;&gt;ggplot&lt;/span&gt;(sims_data) +

&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;stat_summary&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(time, fishstock), &lt;span class=&quot;dt&quot;&gt;fun.y =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;mean&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fun.ymin =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;mymin&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fun.ymax=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;max&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;geom=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;ribbon&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;fill=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;grey80&amp;#39;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;col=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;#39;grey80&amp;#39;&lt;/span&gt;) +
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;stat_summary&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(time, fishstock, &lt;span class=&quot;dt&quot;&gt;col=&lt;/span&gt;method), &lt;span class=&quot;dt&quot;&gt;fun.y =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;mean&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;geom=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;line&amp;quot;&lt;/span&gt;) +

&lt;span class=&quot;co&quot;&gt;#  geom_line(aes(time, fishstock, group=interaction(reps,method), color=method), alpha=0.1) +&lt;/span&gt;
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;scale_colour_manual&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;values=&lt;/span&gt;colorkey, &lt;span class=&quot;dt&quot;&gt;guide =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;guide_legend&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;override.aes =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;alpha =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;))) +
&lt;span class=&quot;st&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;facet_wrap&lt;/span&gt;(~method) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;guides&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;legend.position=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;http://io.carlboettiger.info/nonparametric-bayes/replicates-stat-summary.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;technical-challenges-in-submitting-transparent-figures&quot;&gt;Technical challenges in submitting transparent figures&lt;/h3&gt;
&lt;p&gt;All this works great with R+ggplot2, generating svg versions shown here or in generating pdfs for the final manuscript. Try and upload those pdfs to manuscriptcentral though (or arXiv, actually) and they will render improperly or not at all. What ever happened to the “portable” in “portable document format”? (Note that submission systems can take EPS/PS, though we’d need to change our LaTeX flavor for it, but R’s graphic devices for those formats don’t seem to support transparency.&lt;/p&gt;
&lt;p&gt;It seems the problem for pdfs arises from different versions (thanks to Rich FitzJohn for figuring this out, I would never have managed). Transparency is natively supported in pdf &amp;gt;= 1.4, while in earlier versions it is just emulated. R can generate pdfs in 1.3 (using &lt;code&gt;dev.args = list(version=&amp;quot;1.3&amp;quot;)&lt;/code&gt; as the knitr chunk option), but unfortunately, ggplot promotes pdfs to version 1.4:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Saving 7 x 6.99 in image
Warning message:
In grid.Call.graphics(L_lines, x$x, x$y, index, x$arrow) :
  increasing the PDF version to 1.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m not quite clear what part of the ggplot command triggers this, as some ggplot figures do render in version 1.3. To add one more gotcha, RStudio’s &lt;code&gt;rmarkdown&lt;/code&gt; by default runs pdfcrop which also promotes the pdfs to version 1.4.&lt;/p&gt;
&lt;p&gt;It seems that pdf 1.5 works however – opening the v1.4 pdf in inkscape and saving as v1.5 seems to do the trick. (the current R version seems to be 1.7, though R supports up to 1.4). This is the route I took for the time being with mansucriptcentral, though frustrating that it requires a step external to the &lt;code&gt;rmarkdown::render&lt;/code&gt; process.&lt;/p&gt;
&lt;p&gt;They can also take TIFF for graphics (though (pdf)LaTeX can’t). I suppose one could submit jpg/png images as supplementary files for the tex compilation, which would have been a workable solution if annoying to use rasters when a vector graphic is preferred (and much smaller – I can’t understand why manuscriptcentral takes 10^4 times as long to upload a document as arXiv or other platforms).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Knitcitations Updates</title>
	 <link href="/2014/06/12/knitcitations-updates.html"/>
   <updated>2014-06-12T00:00:00+00:00</updated>
   <id>/06/12/knitcitations-updates</id>
   <content type="html">&lt;p&gt;Used some down-time while traveling to hammer out a long overdue update to my &lt;a href=&quot;https://github.com/cboettig/knitcitations&quot;&gt;knitcitations&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;My first task inovled a backwards-compatible update fixing a few minor issues (see &lt;a href=&quot;https://github.com/cboettig/knitcitations/blob/master/NEWS&quot;&gt;NEWS&lt;/a&gt;) and providing pandoc style inline citations &lt;a href=&quot;https://github.com/cboettig/knitcitations/releases/tag/v0.6-2&quot;&gt;&lt;code&gt;v0.6-2&lt;/code&gt;&lt;/a&gt;, on CRAN.&lt;/p&gt;
&lt;p&gt;I followed this with a ground-up rewrite, as I summarize in NEWS:&lt;/p&gt;
&lt;h2 id=&quot;v1.0-1&quot;&gt;v1.0-1&lt;/h2&gt;
&lt;p&gt;This version is a ground-up rewrite of knitcitations, providing a more powerful interface while also streamlining the back end, mostly by relying more on external libraries for knitty gritty. While an effort has been made to preserve the most common uses, some lesser-used functions or function arguments have been significantly altered or removed. Bug reports greatly appreciated.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;citet&lt;/code&gt;/&lt;code&gt;citep&lt;/code&gt; now accept more options. In addition to the four previously supported options (DOI, URL, bibentry or bibkey (of a previously cited work)), these now accept a plain text query (used in a CrossRef Search), or a path to a PDF file (which attempts metadata extraction).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Citation key generation is now handled internally, and cannot be configured just by providing a named argument to &lt;code&gt;citet&lt;/code&gt;/&lt;code&gt;citep&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;cite&lt;/code&gt; function is replaced by &lt;code&gt;bib_metadata&lt;/code&gt;. This function takes any argument to &lt;code&gt;citet&lt;/code&gt;/&lt;code&gt;citep&lt;/code&gt; as before (including the new arguments), see docs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Linked inline citations now use the configuration: &lt;code&gt;cite_options(style=&amp;quot;markdown&amp;quot;, hyperlink=&amp;quot;to.doc&amp;quot;)&lt;/code&gt; provides a link to the DOI or URL of the document, using markdown format.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for cito and tooltip have been removed. These may be restored at a later date. (The earlier implementation did not appropriately abstract the use of these features from the style/formatting of printing the citation, making generalization hard.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;bibliography&lt;/code&gt; now includes CSL support directly for entries with a DOI using the &lt;code&gt;style=&lt;/code&gt; argument. No need to provide a CSL file itself, just the name of the journal (or rather, the name of the corresponding csl file: full journal name, all lower case, spaces as dashes). See https://github.com/cboettig/knitcitations/issues/38&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;bibliography&lt;/code&gt; formatting has otherwise been completely rewritten, and no longer uses &lt;code&gt;print_markdown&lt;/code&gt;, &lt;code&gt;print_html&lt;/code&gt;, and &lt;code&gt;print_rdfa&lt;/code&gt; methods. rdfa is no longer available, and other formats are controlled through &lt;code&gt;cite_options&lt;/code&gt;. For formal publication pandoc mode is recommended instead of &lt;code&gt;bibliography&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This version was developed on a separate branch (&lt;code&gt;v1&lt;/code&gt;), and has only just been merged back into master. CRAN doesn’t like getting multiple updates in the same month or so, but hopefully waiting a bit longer will give users and I a chance to shake out bugs anway. Meanwhile grab it from github with:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;devtools::&lt;span class=&quot;kw&quot;&gt;install_github&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;cboettig/knitcitations@v1&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see this package in use, for instance, in providing dynamic citations for my &lt;code&gt;RNeXML&lt;/code&gt; &lt;a href=&quot;https://github.com/ropensci/RNeXML/blob/7a6be7bd0106bc91a5586ee614b3cf5249627692/manuscripts/manuscript.Rmd&quot;&gt;mansucript draft&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Is statistical software harmful?</title>
	 <link href="/2014/06/04/is-statistical-software-harmful.html"/>
   <updated>2014-06-04T00:00:00+00:00</updated>
   <id>/06/04/is-statistical-software-harmful</id>
   <content type="html">&lt;p&gt;Ben Bolker has an excellent post on this complex issue over &lt;a href=&quot;http://dynamicecology.wordpress.com/2014/06/04/guest-post-is-statistical-software-harmful&quot;&gt;at Dynamic Ecology&lt;/a&gt;, which got me thinking about writing my own thoughts on the topic in reply.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Google recently announced that it will be making it’s own self-driving cars, rather than modifying those of others. &lt;a href=&quot;http://www.automotive.com/news/1405-google-envisions-self-driving-cars-with-no-steering-wheel/&quot;&gt;Cars that won’t have steering wheels and pedals&lt;/a&gt;. Just a button that says “stop.” What does this tell us about the future of user-friendly complex statistical software?&lt;/p&gt;
&lt;p&gt;Ben quotes prominent statisticians voicing fears that echo common concerns about self-driving cars:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Andrew Gelman attributes to Brad Efron the idea that “recommending that scientists use Bayes’ theorem is like giving the neighbourhood kids the key to your F-16″.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think it is particularly interesting and instructive that the quote Gelman attributes to Efron is about a mathematical theorem rather than about software (e.g. Bayes Theorem, not WinBUGS). Even relatively simple statistical concepts like &lt;span class=&quot;math&quot;&gt;\(p\)&lt;/span&gt; values can cause plenty of confusion, statistical package or no. The concerns are not unique to software, so the solutions cannot come through limiting access to software.&lt;/p&gt;
&lt;p&gt;I am very wary of the suggestion that we should address concerns of appropriate application by raising barriers to access. Those arguments have been made about knowledge of all forms, from access to publications, to raw data, to things as basic as education and democratic voting.&lt;/p&gt;
&lt;p&gt;There are many good reasons for not creating a statistical software implementation of a new method, but I argue here that fear of misuse just is not one of them.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;em&gt;The barriers created by not having a convenient software implementation are not an appropriate filter to keep out people who can miss-interpret or miss-use the software. As you know, a fundamentally different skillset is required to program a published algorithm (say, MCMC), than to correctly interpret the statistical consequences.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We must be wary of a different kind of statistical machismo, in which we use the ability to implement a method by one’s self as a proxy for interpreting it correctly.&lt;/p&gt;
&lt;p&gt;1a) One immediate corollary of (1) is that: &lt;em&gt;Like it or not, someone is going to build a method that is “easy to use”, e.g. remove the programming barriers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;1b) The second corollary is that: &lt;em&gt;individuals with excellent understanding of the proper interpretation / statistics will frequently make mistakes in the computational implementation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Both mistakes will happen. And both are much more formidable problems in the complex methodology of today than when computer was a job description.&lt;/p&gt;
&lt;p&gt;So, what do we do? I think we should abandon the &lt;a href=&quot;http://www.r-bloggers.com/what-is-correctness-for-statistical-software/&quot;&gt;false dichotomy between “usability” and “correctness.”&lt;/a&gt;. Just because software that is easy to use is easy to misuse, does not imply that decreasing usability increases correctness. I think that is a dangerous fallacy.&lt;/p&gt;
&lt;p&gt;A software implementation should aim first to remove the programming barriers rather than statistical knowledge barriers. Best practices such as modularity and documentation should make it easy for users and developers to understand and build upon it. I agree with Ben that software error messages are poor teachers. I agree that a tool cannot be foolproof, no tool ever has been.&lt;/p&gt;
&lt;p&gt;Someone does not misuse a piece of software merely because they do not understand it. Misuse comes from mistakenly thinking you understand it. The premise that most researchers will use something they do not understand just because it is easy to use is distasteful.&lt;/p&gt;
&lt;p&gt;Kevin Slavin gives &lt;a href=&quot;http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world&quot;&gt;a fantastic Ted talk&lt;/a&gt; on the ubiquitous role of algorithms in today’s world. His conclusion is neither one of panacea or doom, but rather that we seek to understand and characterize them, learn their strengths and weaknesses like a naturalist studies a new species.&lt;/p&gt;
&lt;p&gt;More widespread adoption of software such as BUGS &amp;amp; relatives has indeed increased the amount of misuse and false conclusions. But it has also dramatically increased awareness of issues ranging from computational aspects peculiar to particular implementations to general understanding and discourse about Bayesian methods. Like Kevin, I don’t think we can escape the algorithms, but I do think we can learn to understand and live with them.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Plos Data Sharing Policy Reflections</title>
	 <link href="/2014/05/30/PLoS-data-sharing-policy-reflections.html"/>
   <updated>2014-05-30T00:00:00+00:00</updated>
   <id>/05/30/PLoS-data-sharing-policy-reflections</id>
   <content type="html">&lt;p&gt;PLOS has posted an &lt;a href=&quot;http://blogs.plos.org/biologue/2014/05/30/plos-data-policy-update/&quot;&gt;excellent update&lt;/a&gt; reflecting on their experiences a few months in to their new data sharing policy, which requires authors to include a statement of where the data can be obtained rather than providing it upon request. They do a rather excellent job of highlighting common concerns and offering well justified and explained replies where appropriate.&lt;/p&gt;
&lt;p&gt;At the end of the piece they pose several excellent questions, which I reflect on here (mostly as a way of figuring out my own thoughts on these issues).&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;When should an author choose Supplementary Files vs a repository vs figures and tables?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To me, repositories should always be the default. Academic repositories provide robust permanent archiving (such as &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt; backup), independent DOIs to content, often tracking of use metrics, enhanced discoverability, clear and appropriate licenses, richer metadata, as well as frequently providing things like API access and easy-to-use interfaces. They are the Silicon Valley of publishing innovation today.&lt;/p&gt;
&lt;p&gt;Today I think it is much more likely that some material is not appropriate for a ‘journal supplement’ rather than not being able to find an appropriate repository (enough are free, subject agnostic and accept almost any file types). In my opinion the primary challenge is for publishers to tightly integrate the repository contents with their own website, something that the repositories themselves can support with good APIs and embedding tools (many do, PLOS’s coordination with figshare for individual figures being a great example).&lt;/p&gt;
&lt;p&gt;I’m not clear on “vs figures and tables”, as this seems like a content question of “What” should be archived rather than “Where” (unless it is referring to separately archiving the figures and tables of the main text, which sounds like a great idea to me).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Should software/code be treated any differently from ‘data’? How should materials-sharing differ?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the highest level I think it is possible to see software as a ‘type’ of data. Like other data, it is in need of appropriate licensing, a management plan, documentation/metadata, and conforming to appropriate standards and placed in appropriate repositories. Of course what is meant by “appropriate” differs, but that is also true between other types of data. The same motivations for requiring data sharing (understanding and replicating the work, facilitating future work, increasing impact) apply.&lt;/p&gt;
&lt;p&gt;I think we as a scientific community (or rather, many loosely federated communities) are still working out just how best to share scientific code and the unique challenges that it raises. Traditional scientific data repositories are well ahead in establishing best practices for other data, but are rapidly working out approaches to code. The &lt;a href=&quot;http://openresearchsoftware.metajnl.com/about/editorialPolicies&quot;&gt;guidelines&lt;/a&gt; from the Journal of Open Research Software from the UK Software Sustainability Institute are a great example. (I’ve written on this topic before, such as &lt;a href=&quot;http://www.carlboettiger.info/2013/06/13/what-I-look-for-in-software-papers.html&quot;&gt;what I look for in software papers&lt;/a&gt; and on the topic of the &lt;a href=&quot;www.carlboettiger.info/2013/09/25/mozilla-software-review.html&quot;&gt;Mozilla Science Code review pilot&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I’m not informed enough to speak to sharing of non-digital material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What does peer review of data mean, and should reviewers and editors be paying more attention to data than they did previously, now that they can do so?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In as much as we are satisfied with the current definition of peer review for journal articles I think this is a false dichotomy. Landmark papers, at least in my field, five or six decades ago (e.g. about as old as the current peer review system) frequently contained all the data in the paper (papers were longer and data was smaller). Somehow the data outgrew the paper and it just became okay to omit it, just as methods have gotten more complex and papers today frequently gloss over methodological details. The problem, then, is not one of type but one of scale: how do you review data when it takes up more than half a page of printed text.&lt;/p&gt;
&lt;p&gt;The problem of scale is of course not limited to data. Papers frequently have many more authors than reviewers, often representing disparate and highly specialized expertise over possibly years of work, depend upon more than 100 citations and be accompanied by as many pages of supplemental material. To the extent that we’re satisfied with how reviewers and editors have coped with these trends, we can hope for the same for data.&lt;/p&gt;
&lt;p&gt;Meanwhile, data transparency and data reuse may be more effective safe guards. Yes, errors in the data may cause trouble before they can be brought to light, just like bugs in software. But in this way they do eventually come to light, and that is somewhat less worrying if we view data the way we currently build publications (e.g. as fundamental building blocks of research) and publications as we currently view data (e.g. as a means to an ends, illustrated in the idea that it is okay to have mistakes in the data as long as they don’t change the conclusions). Jonathan Eisen has some &lt;a href=&quot;http://www.slideshare.net/phylogenomics/jonathan-eisen-talk-on-open-science-at-bosc2012-ismb&quot; title=&quot;see slide 13&quot;&gt;excellent&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=oWZzUe3Kxeo&quot;&gt;examples&lt;/a&gt; in which openly sharing the data led to rapid discovery and correction of errors that might have been difficult to detect otherwise.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And getting at the reason why we encourage data sharing: how much data, metadata, and explanation is necessary for replication?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I agree that the “What” question is a crux issue, and one we are still figuring out by community. There are really two issues here: what data to include, and what metadata (which to me includes any explanation or other documentation of the data) to provide for whatever data is included.&lt;/p&gt;
&lt;p&gt;On appropriate metadata, we’ll never have a one-size-fits-all answer, but I think the key is to at least uphold current community best-practices (best != mode), whatever they may be. Parts of this are easy: scholarly archives everywhere include basic &lt;a href=&quot;http://en.wikipedia.org/wiki/Dublin_Core&quot;&gt;Dublin Core Elements&lt;/a&gt; metadata like title, author, date, subject and unique identifier, and most data repositories will attach this information in a machine-readable metadata format with minimal burden on the author (e.g. &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt;, or to lesser extent, &lt;a href=&quot;http://figshare.org&quot;&gt;figshare&lt;/a&gt;). Many fields already have well-established and tested standards for data documentation, such as the [Ecological Metadata Langauge], which helps ecologists document things like column names and units in an appropriate and consistent way without constraining how the data is collected or structured.&lt;/p&gt;
&lt;p&gt;What data we include in the first place is more challenging, particularly as there is no good definition of ‘raw data’ (one person’s raw data being another person’s highly processed data). I think a useful minimum might be to provide any data shown in a figure or used in a statistical test that appears in the paper.&lt;/p&gt;
&lt;p&gt;Journal policies can help most in each of these cases by pointing authors to the policies of repositories and to subject-specific publications on these best practices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A crucial issue that is much wider than PLOS is how to cite data and give academic credit for data reuse, to encourage researchers to make data sharing part of their everyday routine.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again I agree that credit for data reuse is an important and largely cultural issue. Certainly editors can play there part as they already do in encouraging authors to cite the corresponding papers on the methods used, etc.&lt;/p&gt;
&lt;p&gt;I think the cultural challenge is much greater for the “long tail” content than it is for the most impactful data. I think most of the top-cited papers over the last two decades have been methods papers (or are cited for the use of a method that has become the standard of a field; often as software). As with any citation, there’s a positive feedback as more people are aware of it. I suspect that the papers announcing the first full genomes of commonly studied organisms (essentially data papers, though published by the most exclusive journals) did not lack citations. For data (or methods for that matter) that do not anticipate that level of reuse, the concern of appropriate credit is more real. Even if a researcher can assume they will be cited by future reuse of their data, they may not feel that sufficient compensation if it means one less paper to their name.&lt;/p&gt;
&lt;p&gt;Unfortunately I think these are not issues unique to data publication but germane to academic credit in general. Citations, journal names, and so forth are not meaningless metrics, but very noisy ones. I think it is too easy to fool ourselves by looking only at cases where statistical averages are large enough to see the signal – datasets like the human genome and algorithms like BLAST we know are impactful, and the citation record bears this out. Really well cited papers or well-cited journals tend to coincide with our notions of impact, so it is easy to overestimate the fidelity of citation statistics when the sample size is much smaller. Besides, academic work is a high-dimensional creature not easily reduced to a few scalar metrics. &lt;!--(I think that is why, at least in the US, we tend
to place more trust in the opinions of people over current metrics.)--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;And for long-term preservation, we must ask who funds the costs of data sharing? What file formats should be acceptable and what will happen in the future with data in obsolete file formats? Is there likely to be universal agreement on how long researchers should store data, given the different current requirements of institutions and funders?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think these are questions for the scientific data repositories and the fields they serve, rather than the journals, and for the most part they are handling them well.&lt;/p&gt;
&lt;p&gt;Repositories like &lt;a href=&quot;http://datadryad.org&quot;&gt;Dryad&lt;/a&gt; have clear pricing schemes closely integrated with other publication costs, and standing at least an order of magnitude less than most journal publication fees look like a bargain. (Not so if you publish in subscription journals I hear you say. Well, I would not be surprised if we start seeing such repositories negotiate institutional subscriptions to cover the costs of their authors).&lt;/p&gt;
&lt;p&gt;I think the question of data formats is closely tied to that of metadata, as they are all topics of best-practices in archiving. Many scientific data repositories have usually put a lot of thought into these issues and also weigh them against the needs and ease-of-use of the communities they serve. Journal data archiving policies can play their part by encouraging best practices by pointing authors to repository guidelines as well as published articles from their community (such as the &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4608&quot;&gt;Nine Simple Ways&lt;/a&gt; paper by White et al.)&lt;/p&gt;
&lt;p&gt;I feel the almost rhetorical question about ‘universal agreement’ is unnecessarily pessimistic. I suspect that much of the variance in recommendations for the duration a researcher should archive their own work predates the widespread emergence of data repositories, which have vastly simplified the issue from when it was left up to each individual lab. Do we ask this question of the scientific literature? No, largely because many major journals have already provided robust long term archiving with &lt;a href=&quot;http://clockss.org&quot;&gt;CLOCKSS&lt;/a&gt;/LOCKSS backup agreements. Likewise scientific data repositories seem to have settled for indefinite archiving. It seems both reasonable and practical that data archiving can be held to the same standard as the journal article itself. (Sure there are lots of challenging issues to be worked out here, the key is only to leave it in the hands of those already leading the way and not re-invent the wheel).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>packrat and rmarkdown</title>
	 <link href="/2014/05/28/notes.html"/>
   <updated>2014-05-28T00:00:00+00:00</updated>
   <id>/05/28/notes</id>
   <content type="html">&lt;p&gt;I’m pretty happy with the way &lt;code&gt;rmarkdown&lt;/code&gt; looks like it can pretty much replace my &lt;a href=&quot;&quot;&gt;Makefile approach&lt;/a&gt; with a simple R command to &lt;code&gt;rmarkdown::render()&lt;/code&gt;. Notably, a lot of the pandoc configuration can already go into the document’s &lt;code&gt;yaml&lt;/code&gt; header (bibliography, csl, template, documentclass, etc), avoiding any messing around with the Makefile, etc.&lt;/p&gt;
&lt;p&gt;Even more exciting is the pending RStudio integration with pandoc. This exposes the features of the &lt;code&gt;rmarkdown&lt;/code&gt; package to the RStudio IDE buttons, but more importantly, seems like it will simplify the pandoc/latex dependency issues cross-platform.&lt;/p&gt;
&lt;p&gt;In light of these developments, I wonder if I should separate my manuscripts from their corresponding R packages entirely (and/or treat them as vignettes?) I think it would be ideal to point people to a single &lt;code&gt;.Rmd&lt;/code&gt; file and say “load this in RStudio” rather than passing along a whole working directory.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rmarkdown::render&lt;/code&gt; workflow doesn’t cover installing the dependencies, or downloading the a pre-built cache. I’ve been relying on the R package mechanism itself to handle dependencies, though I list all packages loaded by the manuscript but not needed by the package functions themselves as &lt;code&gt;SUGGESTS&lt;/code&gt;, as one would do with a vignette. Consequently, I’ve had to add an &lt;a href=&quot;&quot;&gt;install.R&lt;/a&gt; script to my template, to make sure these packages are installed before a user attempts to run the document. The install script feels like a bit of a hack, and makes me think that RStudio’s packrat may be what I actually want for this. So I finally got around to playing with &lt;a href=&quot;http://rstudio.github.io/packrat/&quot;&gt;packrat&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;packrat&quot;&gt;packrat&lt;/h2&gt;
&lt;p&gt;Packrat isn’t yet on CRAN, and for an RStudio package I admit that it feels a bit clunky still. Having a single &lt;code&gt;packrat.lock&lt;/code&gt; file (think &lt;code&gt;Gemfile.lock&lt;/code&gt; I suppose) seems like a great idea. Carting around the hidden files &lt;code&gt;.Rprofile&lt;/code&gt;, &lt;code&gt;.Renviron&lt;/code&gt;, and the &lt;code&gt;tar.gz&lt;/code&gt; sources for all the dependences (in &lt;code&gt;packrat.sources&lt;/code&gt;) seems heavy and clunky, and logging in and out all the time feels like a hack.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Am I really supposed to commit the &lt;code&gt;.tar.gz&lt;/code&gt; files? &lt;a href=&quot;https://github.com/rstudio/packrat/issues/59&quot;&gt;packrat/issues/59&lt;/a&gt; (Summary: option coming)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do I really need to restart R &lt;a href=&quot;https://github.com/rstudio/packrat/issues/60&quot;&gt;packrat/issues/60&lt;/a&gt; (Summary: yes).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first discussion led to an interesting question about just how big are CRAN packages these days anyhow? Thanks to this clever &lt;code&gt;rsync&lt;/code&gt; trick from Duncan, I could quickly explore this:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;txt =&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;system&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;rsync --list-only cran.r-project.org::CRAN/src/contrib/ | grep .tar.gz&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;intern =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;TRUE&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;setAs&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;character&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;num.with.commas&amp;quot;&lt;/span&gt;, function(from) &lt;span class=&quot;kw&quot;&gt;as.numeric&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;gsub&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;, from) ) )
ans =&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;read.table&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;textConnection&lt;/span&gt;(txt), &lt;span class=&quot;dt&quot;&gt;colClasses=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;character&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;num.with.commas&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;Date&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;character&amp;quot;&lt;/span&gt;))
&lt;span class=&quot;kw&quot;&gt;ggplot&lt;/span&gt;(ans, &lt;span class=&quot;kw&quot;&gt;aes&lt;/span&gt;(V3, V2)) +&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;geom_point&lt;/span&gt;()
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(ans$V2&amp;gt;&lt;span class=&quot;fl&quot;&gt;1e6&lt;/span&gt;)
&lt;span class=&quot;kw&quot;&gt;sum&lt;/span&gt;(ans$V2/&lt;span class=&quot;fl&quot;&gt;1e6&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;https://cloud.githubusercontent.com/assets/222586/3122393/cbe422b8-e766-11e3-9048-016dc21c55e9.png&quot; alt=&quot;cran&quot; /&gt;&lt;figcaption&gt;cran&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Note that there are 711 packages over 1 MB, for a total weight of over 2.8 GB. Not huge but more than you might want in a Git repo all the same.&lt;/p&gt;
&lt;p&gt;Nevertheless, packrat works pretty well. Using a bit of a hack &lt;a href=&quot;https://groups.google.com/forum/#!topic/packrat-discuss/sm46dsvLxSk&quot;&gt;we can&lt;/a&gt; just version manage/ship the &lt;code&gt;packrat.lock&lt;/code&gt; file and let packrat try and restore the rest.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;packrat::&lt;span class=&quot;kw&quot;&gt;packify&lt;/span&gt;()
&lt;span class=&quot;kw&quot;&gt;source&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;.Rprofile&amp;quot;&lt;/span&gt;); &lt;span class=&quot;kw&quot;&gt;readRenviron&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;.Renviron&amp;quot;&lt;/span&gt;)
packrat::&lt;span class=&quot;kw&quot;&gt;restore&lt;/span&gt;()
&lt;span class=&quot;kw&quot;&gt;source&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;.Rprofile&amp;quot;&lt;/span&gt;); &lt;span class=&quot;kw&quot;&gt;readRenviron&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;.Renviron&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;source&lt;/code&gt;/&lt;code&gt;readRenviron&lt;/code&gt; calls should really be restarts to R. Tried replacing this with calls to &lt;code&gt;Rscript -e &amp;quot;packrat::packify()&lt;/code&gt; etc. but that fails to find &lt;code&gt;packrat&lt;/code&gt; on the second call. (Attempting to reinstall it doesn’t work either).&lt;/p&gt;
&lt;p&gt;Provided the sources haven’t disappeared from their locations on Github, CRAN, etc., I think this strategy should work just fine. More long-term, we would want to archive a tarball with the &lt;code&gt;packrat.sources&lt;/code&gt;, perhaps downloading it from a script as I currently do with the cache archive.&lt;/p&gt;
&lt;h2 id=&quot;knitcitations&quot;&gt;knitcitations&lt;/h2&gt;
&lt;p&gt;Debugging check reveals some pretty tricky behavior on R’s part: it wants to check the R code in my vignette even though it’s not building the vignette. It does this by tangling out the code chunks, which ignores in-line code. Not sure if this should be a bug in knitr or R, but it can’t be my fault ;-). See &lt;a href=&quot;https://github.com/yihui/knitr/issues/784&quot;&gt;knitr/issues/784&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With checks passing, have sent v0.6 to CRAN. Fingers crossed…&lt;/p&gt;
&lt;p&gt;Milestones for &lt;a href=&quot;https://github.com/cboettig/knitcitations/issues?milestone=4&amp;amp;state=open&quot;&gt;version 0.7&lt;/a&gt; should be able to address the print formatting issues, hopefully as a new &lt;code&gt;citation_format&lt;/code&gt; option and without breaking backwards compatibility.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/05/27/notes.html"/>
   <updated>2014-05-27T00:00:00+00:00</updated>
   <id>/05/27/notes</id>
   <content type="html">&lt;h2 id=&quot;nonparametric-bayes&quot;&gt;nonparametric-bayes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nonparametric-bayes &lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/sensitivity-trends.html&quot;&gt;sensitivity-trends&lt;/a&gt; runs.&lt;/li&gt;
&lt;li&gt;nonparametric-bayes sensitivity.R runs: explore facets to see what setting causes the below-perfect performance cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;knitcitations&quot;&gt;knitcitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;updates to knitcitations (paste fron NEWS)&lt;/li&gt;
&lt;li&gt;Looking at adapting CSL for inline text formatting &lt;a href=&quot;https://github.com/citation-style-language/documentation/issues/33&quot;&gt;csl/issues/33&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implemented pandoc rendering&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;ropensci&quot;&gt;ropensci&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use of &lt;code&gt;.Renviron&lt;/code&gt; vs &lt;code&gt;.Rprofile&lt;/code&gt; for API keys&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Seems the difference between .Rprofile and .Renviron is that (a) the latter is just a named character vector, and (b) the latter is accessed by Sys.getenv(). This keeps the working space clean (so does our use of &lt;code&gt;options&lt;/code&gt;, instead of just writing the API key into the .Rprofile directly). Sys.getenv automatically seems to load the environmental variables of the shell (for instance, Sys.getenv(“USER”) returns the username of the computer/active shell, even if no .Renviron file exists). This is kinda convenient, e.g. with travis, if you were encrypting your API keys you could load them with Sys.getenv() without any further step. I guess it makes sense to think of security credentials as environmental variables. In principle (e.g. in the travis case) a user might do this when accessing the same keys across different software, rather than storing different files for R vs python etc.&lt;/p&gt;
&lt;p&gt;Note that a user could have a different .Renviron file in each working directory which is loaded first, which could allow separate projects in separate working directories to only load their own keys.&lt;/p&gt;
&lt;p&gt;I was wondering if we should provide helper functions that would write the keys to .Rprofile or .Renviron or wherever they should go from R, rather than asking the user to locate these hidden files?&lt;/p&gt;
&lt;h2 id=&quot;rmarkdown-exploration&quot;&gt;rmarkdown exploration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Yes, you need to write a custom template to have multiple pdf templates. &lt;a href=&quot;https://github.com/rstudio/rmarkdown/issues/113&quot;&gt;rmarkdown/issues/113&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Whoops, for RStudio 0.98b (preview) I do need to &lt;code&gt;apt-get install texlive-fonts-recommended&lt;/code&gt; even though my command-line pandoc/latex has no trouble finding some other copy of these fonts on my system.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Who knew? rmarkdown sets &lt;code&gt;dev=png&lt;/code&gt; for html and &lt;code&gt;dev=pdf&lt;/code&gt; for pdf automatically. &lt;a href=&quot;https://github.com/rstudio/rmarkdown/issues/111&quot;&gt;rmarkdown/issues/111&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>DIMACS Workshop on Global Change</title>
	 <link href="/2014/05/21/notes.html"/>
   <updated>2014-05-21T00:00:00+00:00</updated>
   <id>/05/21/notes</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dimacs.rutgers.edu/Workshops/GlobalChange/&quot;&gt;Program Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://io.carlboettiger.info/globalchange&quot;&gt;My slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
Venue: 2063 Valley Life Science Building.

8:00 -  8:30  Registration and Breakfast

 8:30 -  9:00  Welcome and Background

 9:00 -  9:40  &lt;a href=abstracts.html#mccaffrey&gt;Revolutionizing Science &amp; Mathematics Education: the global change challenge&lt;/a&gt;
               Mark McCaffrey, National Center of Science Education

 9:40 - 10:20  &lt;a href=abstracts.html#kaper&gt;Mathematics and Climate: A New Partnership&lt;/a&gt;
               Hans Kaper, Georgetown University

10:20 - 10:50  Break

10:50 - 11:30  &lt;a href=abstracts.html#harte&gt;The Application of Information Theory to Ecology&lt;/a&gt;
               John Harte, University of California - Berkeley

11:30 - 12:10  A GIS Global Change Case Study
               Kevin Koy, University of California - Berkeley

12:10 -  1:30  Lunch

 1:30 -  3:00  Panel 1: &lt;a href=abstracts.html#berbeco&gt;Communicating Global Change&lt;/a&gt;:
               Minda Berbeco, National Center of Science Education, Barbara Cozzens, Holly Gaff, Old Dominion University

 3:00 -  3:30  Break

 3:30 -  5:30  Contributed Talks:
               &lt;a href=abstracts.html#sudakov&gt;Sea-ice Albedo Feedback and the Tipping Points in Algae Dynamics&lt;/a&gt;
               Ivan Sudakov, University of Utah

               &lt;a href=abstracts.html#kitzes&gt;Predicting Future Extinction Debt from Present-Day Community Patterns&lt;/a&gt;
               Justin Kitzes, University of California-Berkeley

               &lt;a href=abstracts.html#lampert&gt;Optimal Control of Restoration - the Role of Economic Threshold&lt;/a&gt;
               Adam Lampert, University of California-Davis

               &lt;a href=abstracts.html#weaver&gt;Both Climate Change and Land Use Change Influenceinvasive Species&#39; Future Ranges&lt;/a&gt;
               Jennifer Weaver, University of California-Berkeley

               &lt;a href=abstracts.html#manore&gt;Towards a National Early Warning System for Human West Nile Virus Incidence&lt;/a&gt;
               Carrie Manore, Tulane University

               &lt;a href=abstracts.html#sagar&gt;Wastes to Fuel - Waste a Valuable Resource&lt;/a&gt;
               Viral Sagar, Rutgers University

 6:00 -  9:00  Banquet Dinner and Talk
               Finding the Sweet Spot
               Richard Salter, Oberlin College

               Location:
               The Faculty Club
               Howard Room
               University of California, Berkeley

&lt;b&gt;Tuesday, May 20, 2014&lt;/b&gt;

 8:00 -  8:30  Breakfast

 8:30 -  9:10  &lt;a href=abstracts.html#boettiger&gt;Massive Data Set Management and Analysis in the Context of Global Change&lt;/a&gt;
               Carl Boettiger, University of California - Santa Cruz

 9:10 -  9:50  &lt;a href=abstracts.html#martinez&gt;Understanding Socio-Ecosystems as Complex Networks in Changing Environments&lt;/a&gt;
               Neo Martinez, University of Arizona

 9:50 - 10:30  &lt;a href=abstracts.html#blackburn&gt;Applications of GIS in Emerging Zoonotic Processes&lt;/a&gt;
               Jason Blackburn, University of Florida

10:30 - 11:00  Break

11:00 - 12:20  Panel 2: Data Deluge or Drought (Quality and Quantity):
               David Ackerly, University of California - Berkeley,
               Fred Roberts, Rutgers University,
               Philip Stark, University of California - Berkeley

12:20 -  1:30  Lunch

 1:30 -  3:00  Workshops 1 and 2 (in parallel)
               1: Student driven: Using Mathematics to Interface Global and Ecosystem Processes

               2: Student driven: Using Mathematics to Link Individual and Population Level Processes

 3:00 -  3:30  Break

 3:30 -  4:30  Workshops 1 and 2 (continue)
               1: Student driven: Using Mathematics to Interface Global and Ecosystem Processes

               2: Student driven: Using Mathematics to Link Individual and Population Level Processes

 4:30 -  5:00  Workshop report back

&lt;b&gt;Wednesday, May 21, 2014&lt;/b&gt;

 8:00 -  8:30  Breakfast

 8:30 -  9:10  &lt;a href=abstracts.html#saltelli&gt;When All Models are Wrong&lt;/a&gt;
               Andrea Saltelli, European Commission JRC

 9:10 - 10:30  Panel 3: Are Our Models Adequate for Policy Formation:
               Solomon Hsiang, University of California - Berkeley,
               Donald Lucas, Lawrence Livermore National Laboratories,
               A. Marm Kilpatrick, University of California - Santa Cruz

10:30 - 11:00  Break

11:00 - 12:00  Way forward discussion
&lt;/pre&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;scratch-notes&quot;&gt;Scratch notes&lt;/h2&gt;
&lt;p&gt;One-at-a-time sensitivity: (OAT), instead of exhaustive combinations (Latin hypercube)&lt;/p&gt;
&lt;p&gt;VV&lt;/p&gt;
&lt;p&gt;Validation: are we solving the equations correctly Verification: are we solving the correct equations&lt;/p&gt;
&lt;p&gt;UQ: Uncertainty Quantification&lt;/p&gt;
&lt;p&gt;Using a particular scenario (double C02) -&amp;gt; a PDF Different models -&amp;gt; different PDFs&lt;/p&gt;
&lt;p&gt;What are the nobs, what are the ranges?&lt;/p&gt;
&lt;p&gt;Spread in a single model greater than spread across model means…&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Integrating Github Project Repos Into The Notebook</title>
	 <link href="/2014/05/07/integrating-github-project-repos-into-the-notebook.html"/>
   <updated>2014-05-07T00:00:00+00:00</updated>
   <id>/05/07/integrating-github-project-repos-into-the-notebook</id>
   <content type="html">&lt;p&gt;For a while now most of my active research is developed through &lt;code&gt;.Rmd&lt;/code&gt; scripts connected to a particular project repository (something I discuss at length in &lt;a href=&quot;http://www.carlboettiger.info/2014/05/05/knitr-workflow-challenges.html&quot;&gt;deep challenges with knitr workflows&lt;/a&gt;). In the &lt;a href=&quot;http://www.carlboettiger.info/2014/05/06/steps-to-a-more-portable-workflow.html&quot;&gt;previous post&lt;/a&gt; I discuss creating a &lt;code&gt;template&lt;/code&gt; package with a more transparent organization of files, such as moving manuscripts from &lt;code&gt;inst/doc/&lt;/code&gt; to simply &lt;code&gt;manuscripts/&lt;/code&gt;. This left these exploratory analysis scripts in &lt;code&gt;inst/examples&lt;/code&gt; in a similarly unintuitive place. Though I like having these scripts as part of the repository (which keeps everything for a project in one place, as it were), like the manuscript they aren’t really part of the R package, particularly as I have gotten better at creating proper unit tests in place of just rerunning dynamic scripts occasionally.&lt;/p&gt;
&lt;p&gt;I’ve also been nagged by the idea of having to always just link to these nice dynamic documents from my lab notebook. Sure Github renders the markdown so that it’s easy enough to see highlighted code and figures etc., but it still makes them seem rather external. Occasionally I would copy the complete &lt;code&gt;.md&lt;/code&gt; file into a notebook post, but this divorced it of it’s original version history and associated &lt;code&gt;.Rmd&lt;/code&gt; source.&lt;/p&gt;
&lt;p&gt;One option would be to move them all directly into my lab notebook, &lt;code&gt;.Rmd&lt;/code&gt; files and all. This would integrate the scripts more nicely than Github’s own rendering, matching the URL and look and feel of my notebook. It would also allow for javascript elements such as MathJax equations, Google Analytics, and Disqus that are not possible when only linking to an &lt;code&gt;.md&lt;/code&gt; file on Github.&lt;/p&gt;
&lt;p&gt;In the recent &lt;a href=&quot;https://github.com/ropensci/docs&quot;&gt;ropensci/docs&lt;/a&gt; project we are exploring a way to have Jekyll automatically compile (potentially with caching) a site that uses &lt;code&gt;.Rmd&lt;/code&gt; posts and deploy to Github all using &lt;code&gt;travis&lt;/code&gt;, but we’re not quite finished and this is potentially fragile particularly with the hundreds of posts in this notebook. Besides this, the notebook structure is rather temporally oriented, (posts are chronological and reflected in my URL structure) while these scripts are largely project-oriented. (Consistent use of categories and tags would ameliorate this).&lt;/p&gt;
&lt;h3 id=&quot;embedding-images-in-.rmd-outputs&quot;&gt;Embedding images in &lt;code&gt;.Rmd&lt;/code&gt; outputs&lt;/h3&gt;
&lt;p&gt;A persistent challenge has been how best to deal with images created by these scripts, some of which I may run many times. By default &lt;code&gt;knitr&lt;/code&gt; creates &lt;code&gt;png&lt;/code&gt; images, which as binary files are ill suited for committing to Github, and which could bloat a repository rather quickly. For a long while I have used custom hooks to push these images to &lt;code&gt;flickr&lt;/code&gt;, (see &lt;a href=&quot;http://flickr.com/cboettig&quot;&gt;flickr.com/cboettig&lt;/a&gt;), inserting the permanent flickr URL into the output markdown.&lt;/p&gt;
&lt;p&gt;Recently Martin Fenner convinced me that &lt;code&gt;svg&lt;/code&gt; files would both render more nicely across a range of devices (being vector graphics), and could be easily committed to Github as they are text-based (XML) files, so that reproducing the same image in repeated runs wouldn’t take up any more space. We can then browse a nice version history of the any particular figure, and this also keeps all the output material together, making it easier to archive permanently (certainly nicer than my old archiving solution using data URIs.). Lastly, &lt;code&gt;svg&lt;/code&gt; is both web native, being a standard namespace of HTML5, and potentially interactive, as the &lt;a href=&quot;http://www.omegahat.org/SVGAnnotation/&quot;&gt;SVGAnnotation&lt;/a&gt; R package illustrates. So, lots of advantages in using &lt;code&gt;svg&lt;/code&gt; graphics.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;svg&lt;/code&gt; files also bring some unique challenges. Unlike when &lt;code&gt;png&lt;/code&gt; files are added to Github, webpages cannot directly link them since Github enforces rendering them as text instead of an image through its choice of HTML header, for security reasons. This means the only way to link to an &lt;code&gt;svg&lt;/code&gt; file on Github is to have that file on a &lt;code&gt;gh-pages&lt;/code&gt; branch, where it can be rendered as a website. A distinct disadvantage of this approach is that while we can link to a specific version of any file on Github, we see only the most recent version rendered on the website created by a &lt;code&gt;gh-pages&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;On the other hand, having the &lt;code&gt;svg&lt;/code&gt; files on the &lt;code&gt;gh-pages&lt;/code&gt; branch further keeps down the footprint of the project &lt;code&gt;master&lt;/code&gt; branch. This leads rather naturally to the idea that the &lt;code&gt;.Rmd&lt;/code&gt; files and their &lt;code&gt;.md&lt;/code&gt; outputs should also appear on the &lt;code&gt;gh-pages&lt;/code&gt; branch. This removes them from their awkward home in &lt;code&gt;inst/examples/&lt;/code&gt;, and enables all the benefits of custom CSS, custom javascript, and custom URLs that we don’t have on Github’s rendering.&lt;/p&gt;
&lt;p&gt;To provide a consistent look and feel, I merely copied over the &lt;code&gt;_layouts&lt;/code&gt; and &lt;code&gt;_includes&lt;/code&gt; from my lab notebook, tweaking them slightly to use the assets already hosted there. I add custom domain name for the all my &lt;code&gt;gh-pages&lt;/code&gt; as a sub-domain, &lt;code&gt;io.carlboettiger.info&lt;/code&gt; &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, and now instead of having script output appear like so:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/7dd8fc444cb9d20d839286eac8068b3099ea9b6a/inst/examples/gaussian-process-basics.md&quot;&gt;nonparametric-bayes/inst/examples/gaussian-process-basics.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have the same page rendered on my &lt;code&gt;io&lt;/code&gt; sub-domain:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&quot;&gt;io.carlboettiger.info/nonparametric-bayes/gaussian-process-basics.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;with its mathjax, disqus, matching css, URL and nav elements.&lt;/p&gt;
&lt;h2 id=&quot;landing-pages&quot;&gt;Landing pages&lt;/h2&gt;
&lt;p&gt;An obvious extension of this approach is to grab a copy of the repository README and rename it &lt;code&gt;index.md&lt;/code&gt; and add a yaml header such that it serves as a landing page for the repository. A few lines of Liquid code can then generate the links to the other output scripts, as in this example:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://io.carlboettiger.info/nonparametric-bayes/&quot;&gt;io.carlboettiger.info/nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;template&quot;&gt;Template&lt;/h2&gt;
&lt;p&gt;I have added a &lt;code&gt;gh-pages&lt;/code&gt; branch with this set up to my new &lt;code&gt;template&lt;/code&gt; repository, with some more &lt;a href=&quot;http://io.carlboettiger.info/template/README&quot;&gt;basic documentation and examples&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;There’s no need to use a different sub-domain than the rest of my website, other than that it would require my notebook be hosted on the &lt;a href=&quot;https://github.com/cboettig/cboettig.github.com&quot;&gt;cboettig.github.com&lt;/a&gt; repo instead of &lt;a href=&quot;https://github.com/cboettig/labnotebook&quot;&gt;labnotebook&lt;/a&gt;. However I prefer keeping my hosting on the repository I already have, and it also seems a bit unorthodox to host all my repositories on my main domain. In particular, it increases the chance for URL collisions if I create a repository with the same name as a page or directory on my website. Having gh-pages on the &lt;code&gt;io&lt;/code&gt; sub-domain feels like just the right amount of separation to me.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Steps To A More Portable Workflow</title>
	 <link href="/2014/05/06/steps-to-a-more-portable-workflow.html"/>
   <updated>2014-05-06T00:00:00+00:00</updated>
   <id>/05/06/steps-to-a-more-portable-workflow</id>
   <content type="html">&lt;p&gt;While I have made &lt;a href=&quot;http://www.carlboettiger.info/2012/05/06/research-workflow.html&quot;&gt;my workflow&lt;/a&gt; for most of my ongoing projects available on Github for some time, this does not mean that it has been particularly easy to follow. Further, as I move from project to project I have slowly improved how I handle projects. For instance, I have since added unit tests (with &lt;code&gt;testthat&lt;/code&gt;) and continuous integration (with &lt;a href=&quot;http://travis-ci.org&quot;&gt;travis-ci&lt;/a&gt;) to my repositories, and my handling of manuscripts has gotten more automated, with richer latex templates, yaml metadata, and simpler and more powerful makefiles.&lt;/p&gt;
&lt;p&gt;Though I have typically used my most recent project as a template for my next one (not so trivial as I work on several at a time), I realized it would make sense to just maintain a general template repo with all the latest goodies. I have now launched my &lt;a href=&quot;https://github.com/cboettig/template&quot;&gt;template&lt;/a&gt; on Github.&lt;/p&gt;
&lt;p&gt;I toyed with the idea of just treating the manuscript as a standard vignette, but this would make &lt;code&gt;pandoc&lt;/code&gt; an external dependency for the package, putting an unecessary burden on &lt;code&gt;travis&lt;/code&gt; and users. I settled on creating a &lt;code&gt;manuscripts&lt;/code&gt; directory in the project root folder as the most semantically obvious place. This is added to &lt;code&gt;.Rbuildignore&lt;/code&gt; as it doesn’t fit the standard structure of an R package, but since it is not a vignette and cannot be built with the package dependencies anyhow, this seems to make sense to me.&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The manuscript itslef is written in &lt;code&gt;.Rmd&lt;/code&gt;, with a &lt;code&gt;yaml&lt;/code&gt; header for the usual metadata of authors, affiliations, and so forth. Pandoc’s recent support for &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/manuscript.Rmd#L1-27&quot;&gt;yaml metadata&lt;/a&gt; makes it much easier to use &lt;code&gt;.Rmd&lt;/code&gt; with a LaTeX template, making &lt;code&gt;.Rnw&lt;/code&gt; rather unnecessary. &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/components/elsarticle.latex&quot;&gt;My template&lt;/a&gt; includes a custom &lt;code&gt;LaTeX&lt;/code&gt; template that includes pandoc’s macros for inserting authors, affiliations, and so forth in the correct LaTeX elements, though pandoc’s &lt;a href=&quot;https://github.com/jgm/pandoc-templates/blob/master/default.latex&quot;&gt;default template&lt;/a&gt; is rather good and already has macros for most things in place (meaning you can merely declare the layout or font in the yaml header and magically see the tex interpret it).&lt;/p&gt;
&lt;p&gt;I have tried to keep the &lt;code&gt;manuscripts&lt;/code&gt; directory relatively clean, placing &lt;code&gt;csl&lt;/code&gt;, &lt;code&gt;bibtex&lt;/code&gt;, &lt;code&gt;figures/&lt;/code&gt;, &lt;code&gt;cache/&lt;/code&gt; and other such files in a &lt;code&gt;components/&lt;/code&gt; sub-directory. I have also tried to keep the &lt;code&gt;Makefile&lt;/code&gt; as platform-independent as possible by having it call little Rscripts (also housed in &lt;code&gt;components/&lt;/code&gt;) rather than commandline utilities like &lt;code&gt;sed -i&lt;/code&gt; and &lt;code&gt;wget&lt;/code&gt; that may not behave the same way on all platforms.&lt;/p&gt;
&lt;p&gt;Lastly, Ryan Batts recently convinced me that providing binary cache files of results was an important way to allow a reader to quickly engage in exploring an analysis without having to first let potentially long-running code execute. &lt;code&gt;knitr&lt;/code&gt; provides an excellent way to create and manage this caching on a code chunk by chunk level, which is also crucial when editing a dynamic document with intensive code (no one wants to rerun your MCMC just to rebuild the pdf). Since git/Github seems like a poor option for distributing binaries, I have for the moment just archived the cache on a (university) web server and added a Make/Rscript line to that can restore it from that location. Upon publication this cache could be permanently archived (along with plain text tables of the graphs) and then installed from that archive instead.&lt;/p&gt;
&lt;p&gt;I have also added a separate &lt;a href=&quot;https://github.com/cboettig/template/blob/master/manuscripts/README.md&quot;&gt;README&lt;/a&gt; in the manuscripts directory to provide some guidance to a user seeking to build the manuscript.&lt;/p&gt;
&lt;p&gt;Examples of an active projects currently using this layout for manuscripts, etc include &lt;a href=&quot;https://github.com/ropensci/RNeXML&quot;&gt;RNeXML&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/&quot;&gt;nonparametric-bayes&lt;/a&gt;&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Perhaps I should not have the manuscript on the master branch at all, but putting it on another branch would defeat the purpose of having it in an obviously-named directory of the repository home page where it is most easy to discover.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Deep challenges to dynamic documentation in daily workflows</title>
	 <link href="/2014/05/05/knitr-workflow-challenges.html"/>
   <updated>2014-05-05T00:00:00+00:00</updated>
   <id>/05/05/knitr-workflow-challenges</id>
   <content type="html">&lt;p&gt;We often discuss dynamic documents such as &lt;code&gt;Sweave&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt; in reference to final products such as publications or software package vignettes. In this case, all the elements involved are already fixed: external functions, code, text, and so forth. The dynamic documentation engine is really just a tool to combine them (knit them together). Using dynamic documentation on a day-to-day basis on ongoing research presents a compelling opportunity but a rather more complex challenge as well. The code base grows, some of it gets turned into external custom functions where it continues to change. One analysis script branches into multiple that vary this or that. The text and figures are likewise subject to the same revision as the code, expanding and contracting, or being removed or shunted off into an appendix.&lt;/p&gt;
&lt;p&gt;Structuring a dynamic document when all the parts are morphing and moving is one of the major opportunities for the dynamic approach, but also the most challenging. Here I describe some of those challenges along with various tricks I have adopted to deal with them, mostly in hopes that someone with a better strategy might be inspired to fill me in.&lt;/p&gt;
&lt;h2 id=&quot;the-old-way&quot;&gt;The old way&lt;/h2&gt;
&lt;p&gt;For a while now I have been using the &lt;a href=&quot;http://yihui.name/knitr&quot;&gt;knitr&lt;/a&gt; dynamic documentation/reproducible research software for my project workflow. Most discussion of dynamic documentation focuses on ‘finished’ products such as journal articles or reports. Over the past year, I have found the dynamic documentation framework to be particularly useful as I develop ideas, and remarkably more challenging to then integrate into a final paper in a way that really takes advantage of its features. I explain both in some detail here.&lt;/p&gt;
&lt;p&gt;My former workflow followed a pattern no doubt familiar to many:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash away in an R terminal, paste useful bits into an R script…&lt;/li&gt;
&lt;li&gt;Write manuscript separately, pasting in figures, tables, and in-line values returned from R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This doesn’t leave much of a record of what I did or why, which is particularly frustrating when some discussion reminds me of an earlier idea.&lt;/p&gt;
&lt;h2 id=&quot;dynamic-docs-.rmd-files&quot;&gt;Dynamic docs: &lt;code&gt;.Rmd&lt;/code&gt; files&lt;/h2&gt;
&lt;p&gt;When I begin a new project, I now start off writing a &lt;code&gt;.Rmd&lt;/code&gt; file, intermixing notes to myself and code chunks. Chunks break up the code into conceptual elements, markdown gives me a more expressive way to write notes than comment lines do. Output figures, tables, and in-line values inserted. So far so good. I version manage this creature in git/Github. Great, now I have a trackable history of what is going on, and all is well:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Document my thinking and code as I go along on a single file scratch-pad&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Version-stamped history of what I put in and what I got out on each step of the way&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rich markup with equations, figures, tables, embedded.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Caching of script chunks, allowing me to tweak and rerun an analysis without having to execute the whole script. While we can of course duplicate that behavior with careful save and load commands in a script, in knitr this comes for free.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;limitations-to-.rmd-alone&quot;&gt;Limitations to .Rmd alone&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;As I go along, the &lt;code&gt;.Rmd&lt;/code&gt; files starts getting too big and cluttered to easily follow the big picture of what I’m trying to do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Before long, my investigation branches. Having followed one &lt;code&gt;.Rmd&lt;/code&gt; script to some interesting results, I start a new &lt;code&gt;.Rmd&lt;/code&gt; script representing a new line of investigation. This new direction will nevertheless want to re-use large amounts of code from the first file.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;a-solution-the-r-package-research-compendium-approach&quot;&gt;A solution? The R package “research compendium” approach&lt;/h2&gt;
&lt;p&gt;I start abstracting tasks performed in chunks into functions, so I can re-use these things elsewhere, loop over them, and document them carefully somewhere I can reference that won’t be in the way of what I’m thinking. I start to move these functions into &lt;code&gt;R/&lt;/code&gt; directory of an R package structure, documenting with &lt;code&gt;Roxygen&lt;/code&gt;. I write unit tests for these functions (in &lt;code&gt;inst/tests&lt;/code&gt;) to have quick tests to check their sanity without running my big scripts (recent habit). The package structure helps me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reuse the same code between two analyses without copy-paste or getting our of sync&lt;/li&gt;
&lt;li&gt;Document complicated algorithms outside of my working scripts&lt;/li&gt;
&lt;li&gt;Test complicated algorithms outside of my working scripts (&lt;code&gt;devtools::check&lt;/code&gt; and/or unit tests)&lt;/li&gt;
&lt;li&gt;Manage dependencies on other packages (DESCRIPTION, NAMESPACE), including other projects of mine&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This runs into trouble in several ways.&lt;/p&gt;
&lt;h2 id=&quot;problem-1-reuse-of-code-chunks&quot;&gt;Problem 1: Reuse of code chunks&lt;/h2&gt;
&lt;p&gt;What to do with code I want to reuse across blocks but do not want to write as a function, document, or test?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Perhaps this category of problem doesn’t exist, except in my laziness.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This situation arises all the time, usually through the following mechanism: almost any script performs several steps that are best represented as chunks calling different functions, such as &lt;code&gt;load_data&lt;/code&gt;, &lt;code&gt;set_fixed_parameters&lt;/code&gt;, &lt;code&gt;fit_model&lt;/code&gt;, &lt;code&gt;plot_fits&lt;/code&gt;, etc. I then want to re-run almost the same script, but with a slightly different configuration (such as a different data set or extra iterations in the fixed parameters). For just a few such cases, it doesn’t make sense to write these into a single function,&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; instead, I copy this script to a new file and make the changes there.&lt;/p&gt;
&lt;p&gt;This is great until I want to change something in about the way both scripts behave that cannot be handled just by changing the &lt;code&gt;R/&lt;/code&gt; functions they share. Plotting options are a good example of this (I tend to avoid wrapping &lt;code&gt;ggplot&lt;/code&gt; calls as separate functions, as it seems to obfuscate what is otherwise a rather semantic and widely recognized, if sometimes verbose, function call).&lt;/p&gt;
&lt;p&gt;I have explored using &lt;code&gt;knitr&lt;/code&gt;’s support for external chunk inclusion, which allows me to maintain a single R script with all commonly used chunks, and then import these chunks into multiple &lt;code&gt;.Rmd&lt;/code&gt; files. An example of this can be seen in my &lt;code&gt;nonparametric-bayes&lt;/code&gt; repo, where several files (in the same directory) draw most of their code from &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes/blob/9232dfd814c40e3c48c5a837be110a870d8639da/inst/examples/BUGS/external-chunks.R&quot;&gt;external-chunks.R&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;problem-2-package-level-reproducibility&quot;&gt;Problem 2: package-level reproducibility&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Minor/relatively easy to fix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separate files can frustrate reproducibility of a given commit. As I change the functions in &lt;code&gt;R/&lt;/code&gt;, the &lt;code&gt;.Rmd&lt;/code&gt; file can give different results despite being unchanged. (Or fail to reflect changes because it is caching chunks and does not recognize the function definitions have changed underneath it). Git provides a solution to this: since the &lt;code&gt;.Rmd&lt;/code&gt; file lives in the same git repository (&lt;code&gt;inst/examples&lt;/code&gt;) as the package, I can make sure the whole repository matches the hash of the &lt;code&gt;.Rmd&lt;/code&gt; file: &lt;code&gt;install_github(&amp;quot;packagename&amp;quot;, &amp;quot;cboettig&amp;quot;, &amp;quot;hash&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This solution is not fail-safe: the installed version, the potentially uncommitted (but possibly installed) version of the R functions in the working directory, and the R functions present at the commit of the &lt;code&gt;.Rmd&lt;/code&gt; file (and thus matching the hash) could all be different. If we commit and install before every &lt;code&gt;knit&lt;/code&gt;, we can avoid these potential errors (at the cost of some computational overhead), restoring reproducibility to the chain.&lt;/p&gt;
&lt;h2 id=&quot;problem-3-synthesizing-results-into-a-manuscript&quot;&gt;Problem 3: Synthesizing results into a manuscript&lt;/h2&gt;
&lt;p&gt;In some ways this is the easiest part, since the code-base is relatively static and it is just a matter of selecting which results and figures to include and what code is necessary to generate it. A few organizational challenges remain:&lt;/p&gt;
&lt;p&gt;While we generally want &lt;code&gt;knitr&lt;/code&gt; code chunks for the figures and tables that will appear, we usually aren’t interested in displaying much, if any, of the actual code in the document text (unlike the examples until this point, where this was a major advantage of the knitr approach). In principle, this is as simple as setting &lt;code&gt;echo=FALSE&lt;/code&gt; in the global chunk options. In practice, it means there is little benefit to having the chunks interwoven in the document. What I tend to want is having all the chunks run at the beginning, such that any variables or results can easily be added (and their appearance tweaked by editing the code) as figure chunks or in-line expressions. The only purpose of maintaining chunks instead of a simple script is the piecewise caching of chunk dependencies which can help debugging.&lt;/p&gt;
&lt;p&gt;Since displaying the code is suppressed, we are then left with the somewhat ironic challenge of how best to present code as a supplement. One option is simply to point to the source &lt;code&gt;.Rmd&lt;/code&gt;, another is to use the &lt;code&gt;tangle()&lt;/code&gt; option to extract all the code as a separate &lt;code&gt;.R&lt;/code&gt; file. In either case, the user must also identify the correct version of the R package itself for the external &lt;code&gt;R/&lt;/code&gt; functions.&lt;/p&gt;
&lt;h2 id=&quot;problem-4-branching-into-other-projects&quot;&gt;Problem 4: Branching into other projects&lt;/h2&gt;
&lt;p&gt;Things get most complicated when projects begin to branch into other projects. In an ideal world this is simple: a new idea can be explored on a new branch of the version control system and merged back in when necessary, and an entirely new project can be built as a new R package in a different repo that depends on the existing project. After several examples of each, I have learned that it is not so simple. Despite the nice tools, I’ve learned I still need to be careful in managing my workflows in order to leave behind material that is understandable, reproducible, and reflects clear provenance. So far, I’ve learned this the hard way. I use this last section of the post to reflect on two of my own examples, as writing this helps me work through what I should have done differently.&lt;/p&gt;
&lt;h3 id=&quot;example-warning-signals-project&quot;&gt;example: warning-signals project&lt;/h3&gt;
&lt;p&gt;For instance, my work on early warning signals dates back to the start of my &lt;a href=&quot;http://openwetware.org/wiki/User:Carl_Boettiger/Notebook/Stochastic_Population_Dynamics/2010/02/09&quot;&gt;open notebook on openwetware&lt;/a&gt;, when my code lived on a Google code page which seems to have disappeared. (At the time it was part of my ‘stochastic population dynamics’ project). When I moved to Github, this project got it’s own repository, &lt;a href=&quot;https://github.com/cboettig/warningsignals&quot;&gt;warningsignals&lt;/a&gt;, though after a major re-factorization of the code I moved to a new repository, &lt;a href=&quot;https://github.com/cboettig/earlywarning&quot;&gt;earlywarning&lt;/a&gt;. Okay, so far that was due to me not really knowing what I was doing.&lt;/p&gt;
&lt;p&gt;My first paper on this topic was based on the master branch of that repository, which still contains the code required. When one of the R dependencies was moved from CRAN I was able to update the codebase to reflect the replacement package (see issue &lt;a href=&quot;https://github.com/cboettig/earlywarning/issues/10&quot;&gt;#10&lt;/a&gt;). Even before that paper appeared I started exploring other issues on different &lt;a href=&quot;https://github.com/cboettig/earlywarning/network&quot;&gt;branches&lt;/a&gt;, with the &lt;code&gt;prosecutor&lt;/code&gt; branch eventually becoming it’s own paper, and then it’s &lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/&quot;&gt;own repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That paper sparked a comment letter in response to it, and the analysis involved in my reply piece was just developed on the same master branch of the prosecutor-fallacy repository. This leaves me with a total of three repositories across four branches, with one repo that corresponds more-or-less directly to a paper, one to two papers, and one to no papers.&lt;/p&gt;
&lt;p&gt;All four branches have diverged and unmerge-able code. Despite sharing and reusing functions across these projects, I often found it better to simply change the function on the new branch or new repo as I desired for the new work. These changes could not be easily merged back as they broke the original function calls of the earlier work.&lt;/p&gt;
&lt;p&gt;Hindsight being 20-20, it would have been preferable that I had maintained one repository, perhaps developed each paper on a different branch and clearly tagged the commit corresponding to the submission of each publication. Ideally these could be merged back where possible to a master branch. Tagged commits provide a more natural solution than unmerged branches to deal with changes to the package that would break methods from earlier publications.&lt;/p&gt;
&lt;h3 id=&quot;example-optimal-control-projects&quot;&gt;example: optimal control projects&lt;/h3&gt;
&lt;p&gt;A different line of research began through a NIMBioS working group called “Pretty Darn Good Control”, beginning it’s digital life in my &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt; repository. Working in different break-out groups as well as further investigation on my own soon created several different projects. Some of these have continue running towards publication, others terminating in dead ends, and still others becoming completely separate lines of work. Later work I have done in optimal control, such &lt;a href=&quot;https://github.com/cboettig/nonparametric-bayes&quot;&gt;nonparametric-bayes&lt;/a&gt; and &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty&quot;&gt;multiple_uncertainty&lt;/a&gt; depend on this package for certain basic functions, though both also contain their own diverged versions of functions that first appeared in &lt;a href=&quot;https://github.com/cboettig/pdg_control&quot;&gt;pdg_control&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because the topics are rather different and the shared code footprint is quite small, separate repositories probably makes more sense here. Still, managing the code dependencies in separate repositories requires extra care, as checking out the right version of the focal repository does not guarantee that one will also have the right version of the [pdg_control] repository. Ideally I should note the hash of [pdg_control] on which I depend, and preferably install that package at that hash (easy enough thanks to &lt;code&gt;devtools&lt;/code&gt;), since depending on a separate project that is also still changing can be troublesome. Alternatively it might make more sense to just duplicate the original code and remove this potentially frail dependency. After all, documenting the provenance need not rely on the dependency, and it is more natural to think of these separate repos as divergent forks.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;If I have a lot of different configurations, it may make sense to wrap up all these steps into a single function that takes input data and/or parameters as it’s argument and outputs a data frame with the results and inputs.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>why I sign my reviews</title>
	 <link href="/2014/05/04/why-I-sign-my-reviews.html"/>
   <updated>2014-05-04T00:00:00+00:00</updated>
   <id>/05/04/why-I-sign-my-reviews</id>
   <content type="html">&lt;p&gt;For the past four years I have made an effort to sign all my reviews (which I try to keep to about one a month). It isn’t because I believe in radical openness or something crazy like that. Its really just my self interest involved – at least mostly. Writing a review is an incredibly time consuming, and largely thankless task. Supposedly anonymous peer review is supposed to protect the reviewer, particularly the scenario of the less established scientist critiquing the work of the more established. I am sure it occasionally serves that purpose. On the other hand, that very scenario can be the &lt;em&gt;most&lt;/em&gt; profitable time to sign a review. Really, when are you more likely to get an esteemed colleague to closely read your every argument than when you’re holding up their publication?&lt;/p&gt;
&lt;p&gt;While the possibility of a vindictive and powerful author sounds daunting, but rather inconsistent with my impression of most scientists, who are more apt to be impressed by an intelligent even if flawed critique than by simple praise. I find it hardest to sign a review that I have found very little constructive criticism to offer, though after a decade of being trained to critique science one can always find something. (Of course signing can be hard on the occasional terrible paper for which it is hard to offer much constructive criticism, but fortunately that has been very rare). Both authors and other reviewers (who are sometimes sent the other reviews, a practice I find very educational as a reviewer) have on occasion commented or complemented me on reviews or acknowledged me in the papers, suggesting that the practice does indeed provide for some simple recognition. At times, it may sow seeds for future collaboration.&lt;/p&gt;
&lt;p&gt;Signing my reviews has on occasion given the author a chance to follow up with me directly. While I’m not certain about journal policies in this regard, I suspect we can assume that we’re all adults capable of civil discussion. In any event, a phone call or even a few back-and-forth emails can be immensely more efficient in allowing an author to clarify elements that I have sometimes misunderstood or been unable to follow from the text, as well as making it easier to communicate my difficulties with the paper. In my experience this has resulted in both a faster and more satisfactory resolution to issues that have led to see some papers published more quickly and without as many tedious multiple rounds of revision. Given that many competitive journals simply cut off papers that might otherwise be successful with a bit more dialog between reviewer and author, because multiple “Revise and resubmits” put too much demand on editors, this seems like a desirable outcome for all involved. I’m not suggesting that such direct dialog is always desirable, but that no doubt many of us have been in the position in which a little dialog might have resolved issues more satisfactorily.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Scientific Computing Notes</title>
	 <link href="/2014/05/02/scientific-computing-notes.html"/>
   <updated>2014-05-02T00:00:00+00:00</updated>
   <id>/05/02/scientific-computing-notes</id>
   <content type="html">&lt;p&gt;In my experience EC2 is good for some things but not for others. Consequently having some funding allocated for it would be great, but it would still be necessary to have other resources as well. I’ve found EC2 very good for running some reasonably portable analysis where you temporarily want some extra processors or memory. On the other hand, I’ve had some frustrations with it as well. You don’t have a persistent development environment unless you explicitly make and maintain a machine image, which means installing any software dependencies from scratch; sometimes a particular nuisance when you aren’t familiar with the architecture. So this adds more overhead on your time relative to administering your own machine, and much more than a university cluster with a human administrator. Obviously the latter is much more costly, but seems to scale well at least where I’ve seen that in universities.&lt;/p&gt;
&lt;p&gt;Costs for maintaining a persistent image (e.g. for a web server), and costs for large data storage on things like S3 still seem a bit high, though actually the Berkeley cloud service listed on it’s IST site looks competitive. EC2 isn’t a great option for tasks that require more processors than can be fit with shared memory on a single node, where the architecture of a cluster is better for things that use MPI. Lastly, I want to think about what environment will be most accessible to my students and postdocs. While surely it’s good for them to learn their way around computing environments, at some point the barriers to enter or keep current with some of the cloud platforms in particular become a distraction from the research.&lt;/p&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;See Glenn Lockwood on &lt;a href=&quot;http://glennklockwood.blogspot.com/2013/04/quick-mpi-cluster-setup-on-amazon-ec2.html&quot;&gt;MPI setup&lt;/a&gt; and &lt;a href=&quot;http://glennklockwood.blogspot.com/2013/04/mpi-benchmarks-amazon-ec2-cluster.html&quot;&gt;MPI benchmarks&lt;/a&gt; on EC2.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Uncertainty Notes</title>
	 <link href="/2014/05/02/multiple-uncertainty-notes.html"/>
   <updated>2014-05-02T00:00:00+00:00</updated>
   <id>/05/02/multiple-uncertainty-notes</id>
   <content type="html">&lt;h3 id=&quot;logistic-recruitment-uniform-noise&quot;&gt;Logistic recruitment, uniform noise&lt;/h3&gt;
&lt;p&gt;(Sethi Fig 3 configuration)&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-scenarios.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;logistic-recruitment-lognormal-noise&quot;&gt;Logistic recruitment, lognormal noise&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-lognormal.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;beverton-holt-recruitement-uniform-noise&quot;&gt;Beverton-Holt recruitement, uniform noise&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-bevholt_uniform.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;beverton-holt-recruitement-lognormal-noise&quot;&gt;Beverton-Holt recruitement, lognormal noise&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-bevholt_lognormal.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;logistic-recruitment-uniform-noise-small-r&quot;&gt;Logistic recruitment, uniform noise, small r&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-logistic_unif_small_r.svg&quot; /&gt; ### Logistic recruitment, lognormal noise, small r&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.io/multiple_uncertainty/5b3b135-logistic_lognormal_small_r.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;metadata&quot;&gt;Metadata&lt;/h2&gt;
&lt;p&gt;From &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/25cc46841908c6ca4da92b8866b836bfeddd707f/inst/matlab/scenarios_meta.txt&quot;&gt;scenarios_meta.txt&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;sourceCode yaml&quot;&gt;&lt;code class=&quot;sourceCode yaml&quot;&gt;&lt;span class=&quot;fu&quot;&gt;Columns:&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;y_grid:&lt;/span&gt; The observed stock size
  &lt;span class=&quot;fu&quot;&gt;escapement:&lt;/span&gt; The optimal escapement policy. (y_grid value minus the escapement is the quota).
  &lt;span class=&quot;fu&quot;&gt;sigma_g:&lt;/span&gt; The growth noise scaling factor
  &lt;span class=&quot;fu&quot;&gt;sigma_m:&lt;/span&gt; The measurement error scaling factor (between observed stock x and measured stock y)
  &lt;span class=&quot;fu&quot;&gt;sigma_i:&lt;/span&gt; The implementation noise scaling factor (between quota q set and harvest h realized).
  &lt;span class=&quot;fu&quot;&gt;r:&lt;/span&gt; The growth rate parameter of the recuitment function
  &lt;span class=&quot;fu&quot;&gt;K:&lt;/span&gt; The carrying capacity parameter of the recruitment function
  &lt;span class=&quot;fu&quot;&gt;recruitment:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;1:&lt;/span&gt; Logistic, x+r*x.*(1-x/K)
    &lt;span class=&quot;fu&quot;&gt;2:&lt;/span&gt; Ricker, (1+r)*x.*exp(-(log(1+r)/K)*x)
    &lt;span class=&quot;fu&quot;&gt;3:&lt;/span&gt; Beverton-Holt, (1+r)*x./(1+r/K*x)
  &lt;span class=&quot;fu&quot;&gt;noise:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;1:&lt;/span&gt; uniform noise distribution
    &lt;span class=&quot;fu&quot;&gt;2:&lt;/span&gt; lognormal noise distribution
  &lt;span class=&quot;fu&quot;&gt;id:&lt;/span&gt; a unique id number for the scenario (for subsetting and plotting)


&lt;span class=&quot;fu&quot;&gt;Constants:&lt;/span&gt;
  &lt;span class=&quot;fu&quot;&gt;delta:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; 0.05
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; Discount rate
  &lt;span class=&quot;fu&quot;&gt;Tmax:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; 10
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; number of years (iterations) over which policy is optimized
  &lt;span class=&quot;fu&quot;&gt;x_grid:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; linspace(0,200,201)
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; True stock size
  &lt;span class=&quot;fu&quot;&gt;y_grid:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; linspace(0,200,201)
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; Observed stock size
  &lt;span class=&quot;fu&quot;&gt;h_grid:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; linspace(0,120,121)
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; Implemented harvest
  &lt;span class=&quot;fu&quot;&gt;q_grid:&lt;/span&gt;
    &lt;span class=&quot;fu&quot;&gt;value:&lt;/span&gt; linspace(0,120,121)
    &lt;span class=&quot;fu&quot;&gt;description:&lt;/span&gt; Target harvest quota

&lt;span class=&quot;fu&quot;&gt;Notes:&lt;/span&gt;
  See the scenarios.m file to confirm values of constants used.&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;source-code-version-archives&quot;&gt;Source code / version archives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Data for all the plots shown appears in &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab/scenarios.csv&quot;&gt;scenarios.csv&lt;/a&gt;. Because my collaborators have me using Matlab for this and Matlab can neither (afaik) write headers to csv files, append to csv files, or even conviently export non-numeric and numeric data (I write a matrix to the csv file), you’ll just have to consult the &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/25cc46841908c6ca4da92b8866b836bfeddd707f/inst/matlab/scenarios_meta.txt&quot;&gt;scenarios_meta.txt&lt;/a&gt; for relevant information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All results shown here correspond to files that can be found in &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/tree/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab&quot;&gt;Commit 4c71612&lt;/a&gt;, including the data file. The SVG plots are hosted through the gh-pages branch of the repo, and thus show the most recent versions of those plots. The names have been tagged with the commit hash to avoid accidentally overwriting when programatically generating images. (Clearly that’s not ideal, but cannot link directly to version and have it render).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Results generated by running &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab/scenarios.m&quot;&gt;scenarios.m&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Functions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The main routine is (still) in &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab/multiple_uncertainty.m&quot;&gt;multiple_uncertainty.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The scenario template over which we loop is defined as a seperate function in &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab/scenario.m&quot;&gt;scenario.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;scenarios.m also calls the script &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/4c71612448eb6f33378235e786d149abc8ce0074/inst/matlab/plot_scenarios.m&quot;&gt;plot_scenarios.m&lt;/a&gt;, which can be run to generate the plots from scenarios.csv directly without first regenerating the results data.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/04/29/notes.html"/>
   <updated>2014-04-29T00:00:00+00:00</updated>
   <id>/04/29/notes</id>
   <content type="html">&lt;h2 id=&quot;labnotebook&quot;&gt;Labnotebook&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Configure Github CDN&lt;/li&gt;
&lt;li&gt;Hmm, &lt;a href=&quot;https://github.com/ixti/jekyll-assets&quot;&gt;jekyll-assets&lt;/a&gt;. Not sure that this is preferable from just using the makefile to compile and minify assets seperately as needed.&lt;/li&gt;
&lt;li&gt;Move &lt;code&gt;www.carlboettiger.info&lt;/code&gt; to host from gh-pages branch of labnotebook. This self-contained system is more standard, and it is strange for other gh-pages hosting to be using the same domain/subdomain to reference content that is actually on a gh-pages branch of some totally different repository.&lt;/li&gt;
&lt;li&gt;Updated rakefile for this purpose, based on &lt;a href=&quot;http://evansosenko.com/posts/automatic-publishing-github-pages-travis-ci/&quot;&gt;evan sosenko’s&lt;/a&gt;, and see &lt;a href=&quot;https://github.com/razor-x/evansosenko.com&quot;&gt;evan’s sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;packages&quot;&gt;packages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fix emergent rfigshare bug due to API change.&lt;/li&gt;
&lt;li&gt;Ooh: Trigger &lt;a href=&quot;https://github.com/philou/daily-travis&quot;&gt;daily automated checks&lt;/a&gt; with travis. Could be good for builds too.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;multiple-uncertainty&quot;&gt;multiple-uncertainty&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;see repo.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Summary of the First Workshop on Sustainable Software for Science: Practice and Experiences (WSSSPE1) http://arxiv.org/abs/1404.7414&lt;/li&gt;
&lt;li&gt;PLOSBiology piece by MS Research and friends (Drew Purves): http://doi.org/10.1371/journal.pbio.1001841, also see: https://t.co/ODw9BDNgHe&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notebook Design Notes</title>
	 <link href="/2014/04/24/notebook-design-notes.html"/>
   <updated>2014-04-24T00:00:00+00:00</updated>
   <id>/04/24/notebook-design-notes</id>
   <content type="html">&lt;p&gt;Reading a little about Flat UI design (&lt;a href=&quot;http://www.sitepoint.com/getting-started-flat-ui-design/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Flat_UI_Design&quot;&gt;2&lt;/a&gt;). Basic concept is that users now realize they can click on things without them looking like actual three-dimensional buttons. (Along with links, checkboxes, and forms). Frequently seems to use many different &lt;a href=&quot;http://flatuicolors.com&quot;&gt;colors&lt;/a&gt; on a single page to bring some life back, though this might just make the design look like Windows 8 instead. I’ve started noticing this everwhere now that I know what it is.&lt;/p&gt;
&lt;p&gt;Easy enough to implement in web design thanks to projects like &lt;a href=&quot;http://designmodo.github.io/Flat-UI/&quot;&gt;Flat-UI&lt;/a&gt;. Not psyched about their colors or components myself, but rather nice to see the way they approach theming Bootstrap 3.1.1 without altering the original Bootstrap css. (Can potentially involve duplicating much of the original LESS).&lt;/p&gt;
&lt;p&gt;I’ve found it easy to theme by adding a customized &lt;code&gt;variables.less&lt;/code&gt; under the original (thereby overwriting the original where specified while otherwise using the original as default) and then adding my own &lt;code&gt;custom.less&lt;/code&gt; at the bottom of the &lt;code&gt;bootstrap.less&lt;/code&gt; file that wraps them all together.&lt;/p&gt;
&lt;p&gt;Looking into upgrading my css to Twitter &lt;a href=&quot;http://getbootstrap.com&quot;&gt;Bootstrap 3.1.1&lt;/a&gt;. They have replaced a simple Makefile with the rather heavy nodejs based Grunt to compile the css. No doubt there are good reasons for this, but a couple rather terrible gotchas in building from the LESS (see below). Otherwise the upgrades are mostly straight forward with a little regex to adopt the new grid class syntax. After swapping in my old &lt;code&gt;.less&lt;/code&gt; files as described above I only needed to update the navigation bar and the accordion folding on categories to have the transition completed. Changes are subtle, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design is now flatter, with the navbar and buttons having no 3D shading&lt;/li&gt;
&lt;li&gt;multiple-column layout can persist on mobile devices&lt;/li&gt;
&lt;li&gt;new (default) fonts&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Currently on fluid container, which can make the text a bit wide on widesreen displays.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Removed the alternative CSS styles. Writing the solarized themes and javascript toggles was a fun exercise but more of an amatuerish trick (that didn’t always reset correctly or provide any real functionality) than a mature feature&lt;/li&gt;
&lt;li&gt;Haven’t upgraded the icons, though have added the FontAwesome4 CDN to the header so that these icons can be used anywhere.&lt;/li&gt;
&lt;li&gt;Cleaned out some javascript and some old templates, mostly related to search.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;enabled code toggling with &lt;code&gt;toggleR&lt;/code&gt; (on pages with &lt;code&gt;page.code: yes&lt;/code&gt;), e.g. &lt;a href=&quot;http://www.carlboettiger.info/2012/06/13/Sethi-policy-functions.html&quot;&gt;2012-06-13 post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Building Bootstrap from LESS&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can’t work with Ubuntu’s &lt;code&gt;npm&lt;/code&gt;, but needs a custom ppa install. &lt;a href=&quot;http://stackoverflow.com/questions/23251351/fail-to-install-twitter-bootstrap-node-js-dependencies-on-ubuntu&quot;&gt;See SO question and solution&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even then things aren’t ready to run due to crazy-stupid namespace conflict over &lt;code&gt;node&lt;/code&gt;. The only thing worse than the uniformative error above is the complete absence of any error at all, again &lt;a href=&quot;http://stackoverflow.com/questions/20937313/grunt-command-doesnt-do-anything&quot;&gt;resolved thanks to SO&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After this I am able to start from scrach again and compile the less files. Which is good, since my Makefile fails now (as it seems taht lessc is a &lt;code&gt;nodejs&lt;/code&gt; creature anyway and my node environment is completely local, as I’ve removed the conflicting node &lt;code&gt;.deb&lt;/code&gt;s installed from apt…). However, it does mean learning to edit the Grunt file in order to make any changes to the build list (e.g. don’t build the docs css), or otherwise just working around these issues. I’ve taken the latter path with a new Makefile that calls Grunt to do the compile and then cleans up and deploys the output as I want, rather than making these edits to the Grunt file directly. By changing less of the boilerplate code, this should be easier to maintain for upgrades.&lt;/p&gt;
&lt;p&gt;Developing almost anything (Ruby, R, javascript, haskell, tex, etc) seems to require it’s own package mananger even though these things are packages in Ubuntu distributions. Meanwhile ubuntu focused things all want their own ppa installed. Maybe I should strip my sources.list back to just what ships with the base Ubuntu release since I have to extend this way instead of through &lt;code&gt;apt-get&lt;/code&gt; now anyway. Still, surely feels like this is asking for trouble.&lt;/p&gt;
&lt;h3 id=&quot;github-hosting&quot;&gt;Github hosting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Configuring Custom URL / Domain name.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Configuring &lt;a href=&quot;https://github.com/blog/1715-faster-more-awesome-github-pages&quot;&gt;Github CDN&lt;/a&gt; with custom domain. Not quite clear how this works, see query to &lt;a href=&quot;http://webmasters.stackexchange.com/questions/56826/do-i-set-a-dns-a-record-for-the-new-github-pages-to-use-their-cdn/56862#56862&quot;&gt;webmasters SO&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Relatively straight-forward to fix. Adjusted dreamhost settings for carlboettiger.info from DNS to a redirect to www.carlboettiger.info. With the CNAME already set for www.carlboettiger.info and the Github CNAME configured, the rest is automatic. Yay for for a free and easy CDN from Github.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Have to wonder: is it then still worth using alternative CDNs, e.g. for CSS and Javascript. Currently MathJax, FontAwesome4 icons, JQuery, and Twitter Bootstrap Javascript are all on external CDNs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;version-managing-the-output-as-well-as-the-source&quot;&gt;Version managing the output (as well as the source)&lt;/h3&gt;
&lt;p&gt;Checkout git by date:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;git&lt;/span&gt; rev-list -n1 --before=2013-7-4 master &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;xargs&lt;/span&gt; git checkout&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Uncertainty Notes</title>
	 <link href="/2014/04/23/multiple-uncertainty-notes.html"/>
   <updated>2014-04-23T00:00:00+00:00</updated>
   <id>/04/23/multiple-uncertainty-notes</id>
   <content type="html">&lt;h2 id=&quot;multiple-uncertainty&quot;&gt;Multiple uncertainty&lt;/h2&gt;
&lt;p&gt;Handling variation in different grids. Cleaned up a bunch of transpose expressions in the code and make sure that dimensions are always properly aligned. Now problem can be solved on arbitrarily different grid discritizations for stock &lt;span class=&quot;math&quot;&gt;\(x\)&lt;/span&gt;, observed stock &lt;span class=&quot;math&quot;&gt;\(y\)&lt;/span&gt;, harvest &lt;span class=&quot;math&quot;&gt;\(h\)&lt;/span&gt; and quota &lt;span class=&quot;math&quot;&gt;\(q\)&lt;/span&gt;. See &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/61e34979df93617e6229a6031ea624934acbe93c/inst/matlab/multiple_uncertainty.m&quot;&gt;multiple_uncertainty.m&lt;/a&gt; and example calls in &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/61e34979df93617e6229a6031ea624934acbe93c/inst/matlab/testing.m&quot;&gt;testing.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Taking a look a how we handle normalization when some of the probability density falls outside the discritized space. Currently we just renormalize, which distributes the weights over all non-zero weighted points. Might be more reasonable to map all that probability to the boundary, though this is tricky. I’ve taken a stab at this based on only normalizing the resulting matrix without knowing the underlying pdf, which has some challenges. See unused function: &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/61e34979df93617e6229a6031ea624934acbe93c/inst/matlab/norm_pile_bdry.m&quot;&gt;norm_pile_bdry.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also added line to enforce the assumption that quota should not exceed assessment estimate, though certainly it is sometimes optimal that it should when either implementation or measurement are uncertain. This seems easiest to enforce by just setting those terms in the value matrix to zero:&lt;/p&gt;
&lt;pre class=&quot;sourceCode matlab&quot;&gt;&lt;code class=&quot;sourceCode matlab&quot;&gt; V = tril(V);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(see in code, &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/61e34979df93617e6229a6031ea624934acbe93c/inst/matlab/multiple_uncertainty.m#L111&quot;&gt;#L111&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&quot;lognormal&quot;&gt;lognormal&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/lognormal_multiple_uncertainty.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;uniform&quot;&gt;Uniform&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/multiple_uncertainty.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;comparisons&quot;&gt;Comparisons&lt;/h3&gt;
&lt;p&gt;Took a quick look at Paul’s code yesterday. He does enforce the quota &amp;lt;= assessment value, which while a logical constraint, is also trivially sub-optimal for some certain measurement and implementation uncertainty.&lt;/p&gt;
&lt;p&gt;It also looks like he has a very coarse grid for the shocks, which I don’t quite understand. It looks like each of the uncertainties is defined as a set of weights over only 7 points; I’m not quite sure why that is so low when he uses quite a fine grid for the state space and action space. I define my shocks over the same grid as the state space.&lt;/p&gt;
&lt;p&gt;His approach looks quite interesting: he enumerates all possible combinations of state-action pairs and computes the expected reward at each of these pairs ahead of time as part of the model statement, as a sum over all possible measurement and implementation shocks:&lt;/p&gt;
&lt;pre class=&quot;sourceCode matlab&quot;&gt;&lt;code class=&quot;sourceCode matlab&quot;&gt;R=&lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;;
for i=&lt;span class=&quot;fl&quot;&gt;1&lt;/span&gt;:length(zi)      &lt;span class=&quot;co&quot;&gt;% Grid size for implementation error&lt;/span&gt;
  for j=&lt;span class=&quot;fl&quot;&gt;1&lt;/span&gt;:length(zm)    &lt;span class=&quot;co&quot;&gt;% Grid size for measurement error&lt;/span&gt;
    R=R + harv(X(:,&lt;span class=&quot;fl&quot;&gt;1&lt;/span&gt;) / zm(j), X(:,&lt;span class=&quot;fl&quot;&gt;2&lt;/span&gt;), zi(i)) * pm(j) * pi(i);
  end
end&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which might be read as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \sum_i \sum_j \min \left(\textrm{assessed stock} / \textrm{measurement grid}_j,  \textrm{quota} * \textrm{implementation grid}_i\right)   * \textrm{measurement shock}_j * \textrm{implementation shock}_i \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(Not clear to me why assessed stock is divided by the grid.)&lt;/p&gt;
&lt;p&gt;He also computes these grids for the uncertainty, ee=rectgrid(zg,zi,zm,zm), where zg is the growth uncertainty grid, etc. Again I like this, as it seems like a nice abstraction of the uncertainty process, but I can’t quite penetrate what this does but he uses it to create the transition matrix, P, through some g2P function I haven’t looked at. Also not sure why zm appears twice, though perhaps it is related to handling both the current and predicted stock sizes (after all, the M matrix appears twice in my equations too).&lt;/p&gt;
&lt;p&gt;For a lark I tried setting the uncertainty grids to match the state space grid, all having 100 points. This has been running overnight…&lt;/p&gt;
&lt;p&gt;Anyway, it looks clever and I which I had a better idea of how he was setting up the model, and what discretizations and assumptions were being made.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/04/18/notes.html"/>
   <updated>2014-04-18T00:00:00+00:00</updated>
   <id>/04/18/notes</id>
   <content type="html">&lt;h2 id=&quot;eml&quot;&gt;EML&lt;/h2&gt;
&lt;p&gt;I’ve added a interface for custom units. You don’t have to mess around with &lt;code&gt;additionalMetadata&lt;/code&gt; as it should be handled automatically. Just define the unit itself, like this &lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&quot;sourceCode coffee&quot;&gt;&lt;code class=&quot;sourceCode coffee&quot;&gt;  create_custom_unit&lt;span class=&quot;kw&quot;&gt;(&lt;/span&gt;id &lt;span class=&quot;kw&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;metersSquaredPerHectare&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;,&lt;/span&gt;
                     parentSI &lt;span class=&quot;kw&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;dimensionless&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;,&lt;/span&gt;
                     unitType &lt;span class=&quot;kw&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;dimensionless&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;,&lt;/span&gt;
                     multiplierToSI &lt;span class=&quot;kw&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;0.0001&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;,&lt;/span&gt;
                     description &lt;span class=&quot;kw&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Square meters per hectare&amp;quot;&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then use the &lt;code&gt;id&lt;/code&gt; you give as the unit type in your &lt;code&gt;unit.defs&lt;/code&gt;. &lt;code&gt;create_custom_unit&lt;/code&gt; updates a &lt;code&gt;custom_units&lt;/code&gt; list in the &lt;code&gt;EMLConfig&lt;/code&gt; environment, which the &lt;code&gt;eml&lt;/code&gt; or &lt;code&gt;write_eml&lt;/code&gt; functions detect and use to write in the additional metadata. &lt;a href=&quot;#fn2&quot; class=&quot;footnoteRef&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If the custom unit is not defined and the session is interactive, EML will prompt for essential fields in the unit definition. Other optional fields are also described in the documentation.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Note that I just use “dimensionless” as an illustration here, a the better choice might perhaps be to define an area/area &lt;code&gt;unitType&lt;/code&gt; as &lt;span class=&quot;citation&quot; data-cites=&quot;mbjones&quot;&gt;@mbjones&lt;/span&gt; discusses in #12. (We’ll add support for OBOE semantics eventually, but that’s much more complex)&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;This isn’t ideal, since the use of environments means the function has “side effects” and you could inadvertently include unit definitions you defined for some other reason in your EML file (if you didn’t do &lt;code&gt;eml_reset_config()&lt;/code&gt; or start a fresh R session). That wouldn’t break anything technical, but would seem a bit strange to define units you didn’t use. The environment mechanism can be avoided by explicitly passing list of custom_units to the &lt;code&gt;eml&lt;/code&gt; or &lt;code&gt;eml_write&lt;/code&gt; functions.&lt;a href=&quot;#fnref2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/04/17/notes.html"/>
   <updated>2014-04-17T00:00:00+00:00</updated>
   <id>/04/17/notes</id>
   <content type="html">&lt;h2 id=&quot;units-in-eml&quot;&gt;Units in EML&lt;/h2&gt;
&lt;p&gt;Overview of how units are determined in the EML package:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;library&lt;/span&gt;(EML)
dat &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;data.set&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;river =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;factor&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;SAC&amp;quot;&lt;/span&gt;,  &lt;span class=&quot;st&quot;&gt;&amp;quot;SAC&amp;quot;&lt;/span&gt;,   &lt;span class=&quot;st&quot;&gt;&amp;quot;AM&amp;quot;&lt;/span&gt;)),
               &lt;span class=&quot;dt&quot;&gt;spp   =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Oncorhynchus tshawytscha&amp;quot;&lt;/span&gt;,  &lt;span class=&quot;st&quot;&gt;&amp;quot;Oncorhynchus tshawytscha&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;Oncorhynchus kisutch&amp;quot;&lt;/span&gt;),
               &lt;span class=&quot;dt&quot;&gt;stg   =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;ordered&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;smolt&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;parr&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;smolt&amp;quot;&lt;/span&gt;), &lt;span class=&quot;dt&quot;&gt;levels=&lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;parr&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;smolt&amp;quot;&lt;/span&gt;)), &lt;span class=&quot;co&quot;&gt;# levels indicates increasing level, eg. parr &amp;lt; smolt&lt;/span&gt;
               &lt;span class=&quot;dt&quot;&gt;ct    =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(293L,    410L,    210L),
               &lt;span class=&quot;dt&quot;&gt;day   =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;as.Date&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;2013-09-01&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;2013-09-1&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;2013-09-02&amp;quot;&lt;/span&gt;)),

               &lt;span class=&quot;dt&quot;&gt;stringsAsFactors =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;FALSE&lt;/span&gt;,

               &lt;span class=&quot;dt&quot;&gt;col.defs =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;River site used for collection&amp;quot;&lt;/span&gt;,
                            &lt;span class=&quot;st&quot;&gt;&amp;quot;Species scientific name&amp;quot;&lt;/span&gt;,
                            &lt;span class=&quot;st&quot;&gt;&amp;quot;Life Stage&amp;quot;&lt;/span&gt;,
                            &lt;span class=&quot;st&quot;&gt;&amp;quot;count of live fish in traps&amp;quot;&lt;/span&gt;,
                            &lt;span class=&quot;st&quot;&gt;&amp;quot;day traps were sampled (usually in morning thereof)&amp;quot;&lt;/span&gt;),

               &lt;span class=&quot;dt&quot;&gt;unit.defs =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;SAC =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;The Sacramento River&amp;quot;&lt;/span&gt;,                         &lt;span class=&quot;co&quot;&gt;# Factor, levels defined explicitly&lt;/span&gt;
                                  &lt;span class=&quot;dt&quot;&gt;AM =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;The American River&amp;quot;&lt;/span&gt;),
                                &lt;span class=&quot;st&quot;&gt;&amp;quot;Scientific name&amp;quot;&lt;/span&gt;,                                      &lt;span class=&quot;co&quot;&gt;# Character string (levels not defined)&lt;/span&gt;
                                &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;parr =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;third life stage&amp;quot;&lt;/span&gt;,                            &lt;span class=&quot;co&quot;&gt;# Ordered factor&lt;/span&gt;
                                  &lt;span class=&quot;dt&quot;&gt;smolt =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;fourth life stage&amp;quot;&lt;/span&gt;),
                                &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;unit =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;number&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;precision =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;bounds =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;ot&quot;&gt;Inf&lt;/span&gt;)),  &lt;span class=&quot;co&quot;&gt;# Integer&lt;/span&gt;
                                &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;format =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;YYYY-MM-DD&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;precision =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;)))               &lt;span class=&quot;co&quot;&gt;# Date&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The EML package provides a variety of interfaces to transform this into EML format, depending on the level of granularity desired. At the highest level, as user can directly call &lt;code&gt;eml_write&lt;/code&gt; (aliased as &lt;code&gt;write.eml&lt;/code&gt; to mimic other write file conventions),&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&lt;span class=&quot;kw&quot;&gt;eml_write&lt;/span&gt;(dat, &lt;span class=&quot;st&quot;&gt;&amp;quot;example.xml&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;contact =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Carl Boettiger &amp;lt;cboettig@ropensci.org&amp;quot;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This call works simply by calling the lower level functions. If &lt;code&gt;dat&lt;/code&gt; is already the S4 object representation for &lt;code&gt;eml&lt;/code&gt; or &lt;code&gt;dataset&lt;/code&gt; object, they are dealt with directly. Otherwise, the function simply passes its arguments to the &lt;code&gt;eml&lt;/code&gt; constructor function. The main difference between the &lt;code&gt;eml&lt;/code&gt; constructor and &lt;code&gt;write_eml&lt;/code&gt; is that the &lt;code&gt;eml&lt;/code&gt; function returns an &lt;code&gt;eml&lt;/code&gt; S4 object, while the &lt;code&gt;write_eml&lt;/code&gt; function takes the additional step of transforming the S4 &lt;code&gt;eml&lt;/code&gt; structure into XML and writing it to the desired file.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;eml&lt;/code&gt; function, also available to the user, has more optional named arguments than &lt;code&gt;write_eml&lt;/code&gt;, though these can all be given to the higher level &lt;code&gt;write_eml&lt;/code&gt; as well since they are passed through the &lt;code&gt;...&lt;/code&gt; mechanism. (Its default arguments illustrate calls to some of the lower-level constructors, such as &lt;code&gt;eml_coverage&lt;/code&gt;, and will automatically try and read &lt;code&gt;creator&lt;/code&gt; and &lt;code&gt;contact&lt;/code&gt; from the configuration environment if they are not provided)&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&amp;gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;eml
function (&lt;span class=&quot;dt&quot;&gt;dat =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;title =&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;metadata&amp;quot;&lt;/span&gt;,
          &lt;span class=&quot;dt&quot;&gt;creator =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;get&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;defaultCreator&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;envir =&lt;/span&gt; EMLConfig),
          &lt;span class=&quot;dt&quot;&gt;contact =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;get&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;defaultContact&amp;quot;&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;envir =&lt;/span&gt; EMLConfig),
          &lt;span class=&quot;dt&quot;&gt;coverage =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;eml_coverage&lt;/span&gt;(&lt;span class=&quot;dt&quot;&gt;scientific_names =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;,
                                  &lt;span class=&quot;dt&quot;&gt;dates =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;,
                                  &lt;span class=&quot;dt&quot;&gt;geographic_description =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;,
                                  &lt;span class=&quot;dt&quot;&gt;NSEWbox =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;),
          &lt;span class=&quot;dt&quot;&gt;methods =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;new&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;methods&amp;quot;&lt;/span&gt;),
          ...,
          &lt;span class=&quot;dt&quot;&gt;additionalMetadata =&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;c&lt;/span&gt;(&lt;span class=&quot;kw&quot;&gt;new&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;additionalMetadata&amp;quot;&lt;/span&gt;)),
          &lt;span class=&quot;dt&quot;&gt;citation =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;,
          &lt;span class=&quot;dt&quot;&gt;software =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;,
          &lt;span class=&quot;dt&quot;&gt;protocol =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because all other arguments are optional, it is sufficient to call this function with the &lt;code&gt;dat&lt;/code&gt; argument alone. The &lt;code&gt;data&lt;/code&gt; object is allowed to be &lt;code&gt;NULL&lt;/code&gt; if at least one of the other top-level alternatives to a &lt;code&gt;dataset&lt;/code&gt; is provided: &lt;code&gt;citation&lt;/code&gt;, &lt;code&gt;software&lt;/code&gt; or &lt;code&gt;protocol&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;my_eml &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;eml&lt;/span&gt;(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function follows the same logic as before: The function first constructs a unique identifier for the EML &lt;code&gt;packageId&lt;/code&gt;. If &lt;code&gt;dat&lt;/code&gt; is already an S4 &lt;code&gt;dataset&lt;/code&gt; or &lt;code&gt;dataTable&lt;/code&gt; object it is added immediately to a &lt;code&gt;new(&amp;quot;eml&amp;quot;&lt;/code&gt; object; otherwise a &lt;code&gt;dataTable&lt;/code&gt; object is constructed with the next helper function, &lt;code&gt;eml_dataTable&lt;/code&gt;. (These constructors prefaced with (&lt;code&gt;eml_&lt;/code&gt;) are always just thin wrappers around the direct construction of these S4 objects with &lt;code&gt;new(&amp;quot;classname&amp;quot;, ...)&lt;/code&gt;, and exist just to simplify certain things wich are usually and frequently automated, such as creating unique ide elements.)&lt;/p&gt;
&lt;p&gt;So far our &lt;code&gt;data.set&lt;/code&gt; object &lt;code&gt;dat&lt;/code&gt; is just passed unchanged from &lt;code&gt;write_eml&lt;/code&gt; to &lt;code&gt;eml&lt;/code&gt; to &lt;code&gt;eml_dataTable&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;my_dataTable &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kw&quot;&gt;eml_dataTable&lt;/span&gt;(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This constructor is the first to peer inside the &lt;code&gt;dat&lt;/code&gt; object, extracting metadata for the &lt;code&gt;attributeList&lt;/code&gt; elements. (The &lt;code&gt;dat&lt;/code&gt; object is also passed to the &lt;code&gt;eml_physical&lt;/code&gt; constructor, which will use the actual &lt;code&gt;data.frame&lt;/code&gt; to write the csv file.)&lt;/p&gt;
&lt;p&gt;The metadata extraction is performed in two steps. First, the helper function &lt;code&gt;detect_class&lt;/code&gt; extracts a list of the necessary metadata from the &lt;code&gt;data.set&lt;/code&gt;. Then this list is coerced into an EML &lt;code&gt;attributeList&lt;/code&gt; (&lt;strong&gt;Note&lt;/strong&gt; in this review it becomes clear that the coercion is not a flexible and robust way to handle this, so this task is now performed by &lt;code&gt;eml_attributeList&lt;/code&gt; and in turn, &lt;code&gt;eml_attribute&lt;/code&gt;, following the same logic as above.) Currently, &lt;code&gt;detect_class&lt;/code&gt; takes the legacy format of having a &lt;code&gt;data.frame&lt;/code&gt; and a list of &lt;code&gt;meta&lt;/code&gt; objects, structured as column name, col definitions, and unit defintions (each as character vectors, like in a &lt;code&gt;data.set&lt;/code&gt;. Here is where things get tricky. &lt;code&gt;detect_class&lt;/code&gt; uses the declared class of the column to decide how to interpret the column and unit metadata, using the following mapping:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;numeric or integer : ratio
ordered factor: ordinal/enumeratedDomain
factor : nominal/enumeratedDomain
POSIXlt, POSIXct, Date : dateTime
character : nominal/textDomain&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like to would be best for &lt;code&gt;eml_attribute&lt;/code&gt; to handle this mapping itself, particulary since the different conventions bifurcate at different spots (e.g. we must know if a &lt;code&gt;nominal&lt;/code&gt; is enumerated or text). This could also allow for finer handling of optional unit information, such as the bounds or precision.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/04/11/notes.html"/>
   <updated>2014-04-11T00:00:00+00:00</updated>
   <id>/04/11/notes</id>
   <content type="html">&lt;h2 id=&quot;labnotebook&quot;&gt;labnotebook&lt;/h2&gt;
&lt;p&gt;Updates to configuration to be more portable and lightweight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Drop APIs for Mendeley and Github in favor of just pulling from atom/rss feeds. Doesn’t require a platform-specific API or authentication, and has proven more robust, if sometimes less flexible and fancy.&lt;/li&gt;
&lt;li&gt;Drop git-modified and sha timestamps. It’s nice having these on the pages and included in the rdfa, but the information is accessible from the git repository and from Github. The overhead time in computing these values and successfully including them wasn’t worth it.&lt;/li&gt;
&lt;li&gt;However, add a sha for the vita page using a simple filter.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wishing I could just embed svgs into md output as XML, instead of as binary. See full description of issue: &lt;a href=&quot;https://github.com/yihui/knitr/issues/754&quot;&gt;yihui/knitr#754&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Installed vim-pandoc’s dev versions, &lt;a href=&quot;https://github.com/vim-pandoc/vim-pantondoc&quot;&gt;vim-pantondoc&lt;/a&gt; and &lt;a href=&quot;https://github.com/vim-pandoc/vim-pandoc-syntax&quot;&gt;vim-pandoc-syntax&lt;/a&gt; via pathogen; seem to be working rather nicely.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reproducible-paper-example&quot;&gt;reproducible paper example&lt;/h2&gt;
&lt;p&gt;from richfitz mwpennel and co: A manuscript with examples executed by makefile (or R source script) and checked and built by travis: &lt;a href=&quot;https://github.com/richfitz/wood/&quot;&gt;richfitz/wood&lt;/a&gt;. Wow. Particularly nice jobs in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;handling downloading of raw data from original (Dryad-based) sources. Makefile seems to handle doing this in a cache-friendly way that is lighter on Travis network.&lt;/li&gt;
&lt;li&gt;providing the cleaned, process data as seperate data files in clearly marked &lt;code&gt;output&lt;/code&gt; directory. Still, the output would be more modular if done in some more generic format than R’s binary serialization as rds files. They do also provide data on Dryad, which is presumably the processed data in a more sensible format, though their data DOI, &lt;a href=&quot;http://doi.org/10.5061/dryad.v7m14&quot;&gt;10.5061/dryad.v7m14&lt;/a&gt;, is still embargoed in advance of publication at the time of writing.&lt;/li&gt;
&lt;li&gt;Travis checks not only that code runs, but that it downloads and that the manuscript and figures can be complied (by LaTeX), with supplement knitted and compiled as html.&lt;/li&gt;
&lt;li&gt;in a very nice touch, Travis pushes html and pdf results back to Github on gh-pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some very minor nitpicks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manuscript isn’t strictly a literate (knitr) document, though figures are being produced on the fly from code by make. May be a sensible choice (though misses a few opportunities for dynamic inline values).&lt;/li&gt;
&lt;li&gt;license not declared, but already coming soon &lt;a href=&quot;https://github.com/richfitz/wood/issues/8&quot;&gt;richfitz/wood#8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;discussion of citation of code: &lt;a href=&quot;https://github.com/richfitz/wood/issues/11&quot;&gt;richfitz/wood#11&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;trouble in executing locally, see &lt;a href=&quot;https://github.com/richfitz/wood/issues/12&quot;&gt;richfitz/wood#12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rfishbase&quot;&gt;rfishbase&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;added travis and minor updates to test suite. &lt;a href=&quot;https://github.com/ropensci/rfishbase/issues/21&quot;&gt;ropensci/rfishbase#21&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Uncertainty Notes</title>
	 <link href="/2014/04/10/multiple-uncertainty-notes.html"/>
   <updated>2014-04-10T00:00:00+00:00</updated>
   <id>/04/10/multiple-uncertainty-notes</id>
   <content type="html">&lt;h2 id=&quot;multiple-uncertainty&quot;&gt;multiple uncertainty&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Set up Paul Fackler’s &lt;a href=&quot;https://sites.google.com/site/mdpsolve/download&quot;&gt;MDPSOLVE&lt;/a&gt; and run Paul’s code. Installing user-contributed extensions in matlab seems mostly a matter adding the unzipped source directories to the path, e.g. as so:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;m&quot;&gt;&lt;code&gt;currentdir = cd
cd /home/cboettig/.matlab/MDPSOLVE
addpath(genpath(cd))

cd /home/cboettig/.matlab/plot2svg
addpath(genpath(cd))

cd(currentdir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(I still find matlab/octave’s syntax of not distinguishing character strings from variable names from function names kind of terrifying, but I guess that’s the price of working with economists.)&lt;/p&gt;
&lt;p&gt;Code runs, with svg output using the user-contributed svg function &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/7401-scalable-vector-graphics-svg-export-of-figures&quot;&gt;plot2svg&lt;/a&gt; since Matlab doesn’t seem to include such basic functionality (though works fine on octave).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/SethiEtAl05_Figure3.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;ropensci&quot;&gt;ropensci&lt;/h2&gt;
&lt;p&gt;Morning mostly spent wrapping up proposal steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pandoc-latex formatting stuff (xelatex, Times New Roman 12pt, doublespacing, makefile configuration, latex-template configuration, margins, yaml header metadata).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reworking introduction into seperate sections&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;feedback on further issues through issue tracker.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Further evidence that anything can happen with regards to transitions in spatial dynamics: &lt;a href=&quot;http://dx.doi.org/10.1103/PhysRevE.89.022701&quot;&gt;Gowda et al, Phys Rev E&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Uncertainty Notes</title>
	 <link href="/2014/04/09/multiple-uncertainty-notes.html"/>
   <updated>2014-04-09T00:00:00+00:00</updated>
   <id>/04/09/multiple-uncertainty-notes</id>
   <content type="html">&lt;h2 id=&quot;attempt-to-replicate-figure-3-but-with-one-noise-at-a-time&quot;&gt;Attempt to replicate Figure 3, but with one noise at a time&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Code: &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/95a79c15659248ccc635f231b24a5db849b01e73/inst/matlab/one_noise_at_a_time.m&quot;&gt;one_noise_at_a_time.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Uniform Noise&lt;/li&gt;
&lt;li&gt;Large noise means devation of 0.5, as in Sethi&lt;/li&gt;
&lt;li&gt;Other noise terms are 0 instead of 0.1&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/one_at_a_time.svg&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;figure-3&quot;&gt;Figure 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Code: &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/ac84a3623786099e827e47bc1ef0c3c094438080/inst/matlab/carl_fig3.m&quot;&gt;carl_fig3.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Uniform Noise&lt;/li&gt;
&lt;li&gt;Large noise means devation of 0.5, as in Sethi&lt;/li&gt;
&lt;li&gt;Small noise means 0.1, as in Sethi&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/carl_fig3.svg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Now fixed&lt;/strong&gt;. See:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Updated code version of &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/be2597935e166f1311f7d43df91750d07a69f7cc/inst/matlab/carl_fig3.m&quot;&gt;carl_fig3.m&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/carl_figure3.svg&quot; /&gt;
&lt;/figure&gt;
&lt;hr /&gt;
&lt;p&gt;One noise at a time, lognormal noise, &lt;code&gt;sigma = 0.3&lt;/code&gt;. (&lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/9739f930c5c6c78cdf41956ce6c38a5d5b2870dc/inst/matlab/lognormal_one_at_a_time.m&quot;&gt;code&lt;/a&gt;)&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/lognormal_one_at_a_time.svg&quot; /&gt;
&lt;/figure&gt;
</content>
 </entry>
 
 <entry>
   <title>Multiple Uncertainty Notes</title>
	 <link href="/2014/04/08/multiple-uncertainty-notes.html"/>
   <updated>2014-04-08T00:00:00+00:00</updated>
   <id>/04/08/multiple-uncertainty-notes</id>
   <content type="html">&lt;h2 id=&quot;growth-noise-only&quot;&gt;Growth Noise Only&lt;/h2&gt;
&lt;p&gt;Run:&lt;/p&gt;
&lt;pre class=&quot;sourceCode bash&quot;&gt;&lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;kw&quot;&gt;matlab&lt;/span&gt; -nodesktop &lt;span class=&quot;kw&quot;&gt;&amp;lt;&lt;/span&gt; testing.m &lt;span class=&quot;kw&quot;&gt;&amp;gt;&lt;/span&gt; testing.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results from running &lt;a href=&quot;http://github.com/cboettig/multiple_uncertainty/tree/master/inst/matlab/testing.m&quot;&gt;testing.m&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;log normal noise&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sigma_g = 0.2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Other noise set to zero.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Coarse grid &lt;code&gt;0:5:150&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(See linked code all parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/carl.svg&quot; alt=&quot;Growth noise only&quot; /&gt;&lt;figcaption&gt;Growth noise only&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;measurement-noise-only&quot;&gt;measurement Noise only&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Code: &lt;a href=&quot;http://github.com/cboettig/multiple_uncertainty/tree/master/inst/matlab/measurement_uncertainty.m&quot;&gt;measurement_uncertainty.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sigma_m = 0.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Other noise set to zero.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Fine grid &lt;code&gt;0:1:150&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(See linked code all parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/unif_0.5_fine_measurement.svg&quot; alt=&quot;measurement noise only&quot; /&gt;&lt;figcaption&gt;measurement noise only&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;implementation-noise-only&quot;&gt;Implementation Noise only&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Code: &lt;a href=&quot;http://github.com/cboettig/multiple_uncertainty/tree/master/inst/matlab/implementation_uncertainty.m&quot;&gt;implementation_uncertainty.m&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sigma_i = 0.5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Other noise set to zero.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Fine grid &lt;code&gt;0:1:150&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(See linked code all parameters)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://cboettig.github.com/multiple_uncertainty/unif_0.5_fine_implementation.svg&quot; alt=&quot;Implementation noise only&quot; /&gt;&lt;figcaption&gt;Implementation noise only&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Thinking about the best way to embed SVGs in the notebook, see &lt;a href=&quot;https://github.com/ropensci/docs/issues/21&quot;&gt;docs/21&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Thoughts on date metadata, see &lt;a href=&quot;https://github.com/ropensci/docs/issues/20&quot;&gt;docs/20&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/04/07/notes.html"/>
   <updated>2014-04-07T00:00:00+00:00</updated>
   <id>/04/07/notes</id>
   <content type="html">&lt;h2 id=&quot;ropensci&quot;&gt;ropensci&lt;/h2&gt;
&lt;p&gt;Working on proposal. Discussion of section 2: other players in the domain (or rather, the many community interfaces with rOpenSci).&lt;/p&gt;
&lt;h2 id=&quot;other&quot;&gt;other&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Discussion with Hadley re: API testing and &lt;code&gt;test_if&lt;/code&gt; functionality, see &lt;a href=&quot;https://github.com/hadley/testthat/issues/141#issuecomment-39737034&quot;&gt;testthat/141&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Related with Hadley discussion on handling tests and keys for API packages, &lt;a href=&quot;https://github.com/hadley/httr/issues/93&quot;&gt;httr/93&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://software-carpentry.org/blog/2014/04/novice-r-discussion-summary.html&quot;&gt;Software Carpentry developing R lessons&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;rfishbase&quot;&gt;rfishbase&lt;/h2&gt;
&lt;p&gt;Add FAO areas (from 04/05), see &lt;a href=&quot;https://github.com/ropensci/rfishbase/issues/20&quot;&gt;rfishbase/20&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;eml&quot;&gt;EML&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Discussion of unit handling, feedback from Karen. &lt;a href=&quot;https://github.com/ropensci/EML/issues/12#issuecomment-39697171&quot;&gt;EML/12&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Need to update creation of filenames for csv files following Matt’s advice: &lt;a href=&quot;https://github.com/ropensci/EML/issues/106&quot;&gt;EML/106&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No luck with &lt;code&gt;XMLSchema::readSchema&lt;/code&gt; parsing the measurement unit schema definitions in &lt;a href=&quot;https://raw.githubusercontent.com/cboettig/eml/master/stmml.xsd&quot;&gt;STMML&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;&amp;gt;&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;XMLSchema::&lt;span class=&quot;kw&quot;&gt;readSchema&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;../stmml.xsd&amp;quot;&lt;/span&gt;)
Error in &lt;span class=&quot;kw&quot;&gt;FUN&lt;/span&gt;(X[[1L]], ...) :
&lt;span class=&quot;st&quot;&gt;  &lt;/span&gt;formal argument &lt;span class=&quot;st&quot;&gt;&amp;quot;localElements&amp;quot;&lt;/span&gt; matched by multiple actual arguments&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;miscreading&quot;&gt;misc/reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;teary: discuss interviews. send copy of materials for archive.&lt;/li&gt;
&lt;li&gt;review request EcoLet&lt;/li&gt;
&lt;li&gt;&lt;p&gt;follow-up emails from Wisconsin visit. Follow-up reading:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.rdcep.org/people&quot;&gt;RDCEP&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here’s the &lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2370657&quot;&gt;RDCEP robust control paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here are two papers on the value of experimentation, &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4616.2007.00016.x/abstract;jsessionid=29F390887AD667C409ACF504ECB50B34.f02t01?deniedAccessCustomisedMessage=&amp;amp;userIsAuthenticate&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://economics.yale.edu/sites/default/files/files/Workshops-Seminars/Macroeconomics/hansen-080926.pdf&quot;&gt;2&lt;/a&gt;. Key references are the two papers by V. Wieland cited there. Steve and Buz discussed one of these in &lt;a href=&quot;http://www.pnas.org/content/104/39/15206.full.pdf+html&quot;&gt;“Panceas” paper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here’s a link to the BDW Brookings paper which has an interesting discussion by &lt;a href=&quot;http://www.brookings.edu/~/media/projects/bpea/spring%202003/2003a_bpea_brock.pdf&quot;&gt;Sargent and Sims&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0304407605002277&quot;&gt;This one&lt;/a&gt; proposes an approach to reporting to policy makers that respects uncertainties&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here is one we did for the World Bank to deal with &lt;a href=&quot;http://elibrary.worldbank.org/doi/abs/10.1093/wber/15.2.229?journalCode=wber&quot;&gt;uncertainties in growth policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since optimal control and even robust control can lead to controls that are complicated functions of the state vector, there’s a literature in applications to central banking that stresses “simple rules”, here’s a &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0304393207000591&quot;&gt;recent example&lt;/a&gt; from our shop that has lots of references,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, on EWS’s, there’s a general multivariate mathematical approach to variance-convariance matrix approaches to EWS’s in the Mathematical Appendix to this paper by &lt;a href=&quot;http://www.pnas.org/content/106/3/826.short&quot;&gt;Biggs et al.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Wisconsin Visit</title>
	 <link href="/2014/04/03/Wisconsin-visit.html"/>
   <updated>2014-04-03T00:00:00+00:00</updated>
   <id>/04/03/Wisconsin-visit</id>
   <content type="html">&lt;p&gt;Carl Boettiger Schedule 3-4 April&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thursday&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Person/activity&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Time&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Location&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Arrive&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Dane County Airport&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Tony&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;4:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;pickup at hotel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Jesse Miller&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;4:30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 448&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Kyle Webert&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;5:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 457&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Cristina Herren&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;5:30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 456&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Dinner with Tony, Karen Strier&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;6:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;leave from Birge Hall&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Person/activity&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Time&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Location&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Breakfast, Meghan Fitzgerald and Fan Huan&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;7:30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;pickup at hotel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Cecile Ane, Lam Si Tung Ho&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;9:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 341&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Steve Carpenter and CFL folk&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10:45&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;CFL 226A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Jeremy Ash&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 338&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Break, walk over with Jacob&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2:30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Birge 460&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;prep for seminar&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Nolan Hall&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;seminar&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3:30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Nolan Hall&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;reception&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;6:00&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Tony’s house&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
a few notes

- Tony students: Kyle (Mytvan), Megan (spider evolution/behavior), Fan (genomics)
- Jesse: plant comm ecology Ph.D.
- Cristina: (Mytvan)
- Jeremy: LTER (plant) community change 
- Karen: anthropology primates 

--&gt;
&lt;p&gt;Misc notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See email from Buz for robust control reading. email to Karen.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Ropenhack Notes</title>
	 <link href="/2014/03/31/ropenhack-notes.html"/>
   <updated>2014-03-31T00:00:00+00:00</updated>
   <id>/03/31/ropenhack-notes</id>
   <content type="html">&lt;h2 id=&quot;breakfast&quot;&gt;Breakfast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;discussion of what makes an API good or a total pain&lt;/li&gt;
&lt;li&gt;discussion of extending data.set class with metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;morning-session&quot;&gt;Morning Session&lt;/h2&gt;
&lt;p&gt;Chatting with Hadley:&lt;/p&gt;
&lt;h3 id=&quot;best-practices-for-r-client-packages-to-rest-apis&quot;&gt;Best practices for R client packages to REST APIs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(When) Do we return response call metadata?&lt;/li&gt;
&lt;li&gt;Very modular function construction&lt;/li&gt;
&lt;li&gt;Error handling, &lt;code&gt;assertthat&lt;/code&gt; checks&lt;/li&gt;
&lt;li&gt;Explicit parsing, as seperate function. e.g.&lt;/li&gt;
&lt;li&gt;Better alternatives in place of &lt;code&gt;require&lt;/code&gt; when loading SUGGESTS lists, which still pollutes the namespace; see examples in &lt;code&gt;httr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ve begun writing &lt;a href=&quot;https://github.com/ropensci/dependencies&quot;&gt;dependencies&lt;/a&gt; package to address some of the issues in managing version dependencies of R packages.&lt;/p&gt;
&lt;h2 id=&quot;afternoon-session&quot;&gt;Afternoon Session&lt;/h2&gt;
&lt;p&gt;Working with Martin and Gavin on &lt;a href=&quot;https://github.com/ropensci/docs&quot;&gt;docs&lt;/a&gt; workflow.&lt;/p&gt;
&lt;p&gt;One challenge of this setup is passing secure data over travis, such as the keys necessary to execute the &lt;code&gt;git push&lt;/code&gt;. Travis has a mechanism for encrypting and transmitting these keys.&lt;/p&gt;
&lt;p&gt;Otherwise, working in simplifying and sanitizing the separation between ruby and knitr interfaces in providing a dynamic coding static blogging platform.&lt;/p&gt;
&lt;p&gt;Also various interactions and video interview for event documentation.&lt;/p&gt;
&lt;h2 id=&quot;evening&quot;&gt;Evening&lt;/h2&gt;
&lt;p&gt;Excellent social; discussions with Josh Bloom re BIDS etc&lt;/p&gt;
&lt;h2 id=&quot;wednesday&quot;&gt;Wednesday&lt;/h2&gt;
&lt;p&gt;Mostly hacking on &lt;a href=&quot;https://github.com/ropensci/docs&quot;&gt;docs&lt;/a&gt; workflow, see commit log etc.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Review Of Progress On Packages</title>
	 <link href="/2014/03/24/review-of-progress-on-packages.html"/>
   <updated>2014-03-24T00:00:00+00:00</updated>
   <id>/03/24/review-of-progress-on-packages</id>
   <content type="html">&lt;p&gt;A lot of the past week has been focused on some catching up and tidying up in my active projects in metadata manipulation and publishing tools, &lt;code&gt;EML&lt;/code&gt;, &lt;code&gt;RNeXML&lt;/code&gt;, and &lt;code&gt;rfigshare&lt;/code&gt;. The full story of the week and half unfolds on github, but summarizing progress here.&lt;/p&gt;
&lt;h2 id=&quot;rfigshare&quot;&gt;rfigshare&lt;/h2&gt;
&lt;p&gt;Changes to &lt;code&gt;httr&lt;/code&gt; interface orginally broke &lt;code&gt;rfigshare&lt;/code&gt;, though working with &lt;span class=&quot;citation&quot; data-cites=&quot;hadley&quot;&gt;@hadley&lt;/span&gt; the original mechanism was restored through &lt;code&gt;sign_oauth1.0()&lt;/code&gt;. The changes provided a much nicer mechanism for handling &lt;code&gt;oauth&lt;/code&gt; authentication without having to specify tokens directly, and resulted in a relatively straightforward overhaul of the authentication mechanism in the package, while still preserving the package API. During this process I also tackled a bunch of outstanding issues, such as: pretty-printing results, fixing behavior of search and adding authors, getting embeddable image URLs and flushing out the test suite and activiting full travis integration of the tests. See package &lt;a href=&quot;https://github.com/ropensci/rfigshare/blob/master/NEWS&quot;&gt;NEWS&lt;/a&gt; for a more complete list (v0.2-9 and v0.3). This version is now on CRAN.&lt;/p&gt;
&lt;p&gt;In the process I learned a bit more about &lt;code&gt;httr&lt;/code&gt;’s oauth handling.&lt;/p&gt;
&lt;h2 id=&quot;rnexml&quot;&gt;RNeXML&lt;/h2&gt;
&lt;p&gt;Main focus has been moving manuscript text foward, flushing out a more complete description of package functions, organization, and use cases. In particular this has focused on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Outlining the tiered levels of metadata manipulation (from the elementrary &lt;code&gt;get_metadata()&lt;/code&gt; to the XPath and SPARQL queries) and metadata generation (from &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;creator&lt;/code&gt; to automatic taxonomic reference links using taxize to building on existing and custom ontologies; and the related example of extending the schema itself using metadata (simmap).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finalizing and writing out this simmap example was the other goal, illustrating extending the package.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lost too much time on some of the fine points of travis integration with package dependencies on specific users and branches of Github, on omegahat, and cases where apt-get cannot get background dependencies automatically.&lt;/p&gt;
&lt;h2 id=&quot;eml&quot;&gt;EML&lt;/h2&gt;
&lt;p&gt;Still need to continue mansucript writing and extending functionality; meanwhile minor changes to reflect updates to rfigshare and to travis integration.&lt;/p&gt;
&lt;h2 id=&quot;rfishbase&quot;&gt;rfishbase&lt;/h2&gt;
&lt;p&gt;Some momentum on rfishbase2.0 thanks to involvement from &lt;span class=&quot;citation&quot; data-cites=&quot;tpoi&quot;&gt;@tpoi&lt;/span&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Software Sustainability Issues In R</title>
	 <link href="/2014/03/20/software-sustainability-issues-in-R.html"/>
   <updated>2014-03-20T00:00:00+00:00</updated>
   <id>/03/20/software-sustainability-issues-in-R</id>
   <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Editorial note:&lt;/strong&gt; The following is slightly edited text from my post to R-devel discussing this issue, which I first drafted here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There seems to be some question of how frequently changes to software packages result in irreproducible results. I am sure that research using functions like &lt;code&gt;glm&lt;/code&gt; and other functions that are shipped with base R are quite reliable; and after all they already benefit from being versioned with R releases (as Jeroen has argued).&lt;/p&gt;
&lt;p&gt;In my field of ecology and evolution, the situation is quite different. Packages are frequently developed by scientists without any background in programming and become widely used, such as &lt;a href=&quot;http://cran.r-project.org/web/packages/geiger/&quot;&gt;geiger&lt;/a&gt;, with 463 papers citing it and probably many more using it that do not cite it (both because it is sometimes used only as a dependency of another package or just because our community isn’t great at citing packages). The package has changed substantially over the time it has been on CRAN and many functions that would once run based on older versions could no longer run on newer ones. It’s dependencies, notably the phylogenetics package ape, has changed continually over that interval with both bug fixes and substantial changes to the basic data structure. The ape package has 1,276 citations (again a lower bound). I suspect that correctly identifying the right version of the software used in any of these thousands of papers would prove difficult and for a large fraction the results would simply not execute successfully. It would be much harder to track down cases where the bug fixes would have any impact on the result. I have certainly seen both problems in the hundreds of Sweave/knitr files I have produced over the years that use these packages.&lt;/p&gt;
&lt;p&gt;Even work that simply relies on a package that has been archived becomes a substantial challenge to reproducibility by other scientists even when an expert familiar with the packages (e.g. the original author) would not have a problem, as the informatics team at the Evolutionary Synthesis center recently concluded in an exercise trying to reproduce several papers including my own that used a package that had been archived (odesolve, whose replacement, deSolve, does not use quite the same function call for the same &lt;code&gt;lsoda&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;New methods are being published all the time, and I think it is excellent that in ecology and evolution it is increasingly standard to publish R packages implementing those methods, as a scan of any table of contents in “methods in Ecology and Evolution”, for instance, will quickly show. But unlike &lt;code&gt;glm&lt;/code&gt;, these methods have a long way to go before they are fully tested and debugged, and reproducing any work based on them requires a close eye to the versions (particularly when unit tests and even detailed changelogs are not common). The methods are invariably built by “user-developers”, researchers developing the code for their own needs, and thus these packages can themselves fall afoul of changes as they depend and build upon work of other nascent ecology and evolution packages.&lt;/p&gt;
&lt;p&gt;Detailed reproducibility studies of published work in this area are still hard to come by, not least because the actual code used by the researchers is seldom published (other than when it is published as it’s own R package). But incompatibilities between successive versions of the 100s of packages in our domain, along with the interdependencies of those packages might provide some window into the difficulties of computational reproducibility. I suspect changes in these fast-moving packages are far more culprit than differences in compilers and operating systems.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/03/18/notes.html"/>
   <updated>2014-03-18T00:00:00+00:00</updated>
   <id>/03/18/notes</id>
   <content type="html">&lt;h2 id=&quot;ropensci&quot;&gt;ropensci&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ensuring interoperability in R / ropensci projects. See pre-hackathon discussion: &lt;a href=&quot;https://github.com/ropensci/hackathon/issues/20&quot;&gt;ropensci/hackathon/issues/#20&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rfigshare&lt;/code&gt; to use new oauth interface of &lt;code&gt;httr&lt;/code&gt;, see &lt;a href=&quot;https://github.com/ropensci/rfigshare/issues/72&quot;&gt;rfigshare/#72&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ugh. More fun with travis integration on &lt;code&gt;RNeXML&lt;/code&gt; &lt;a href=&quot;https://github.com/ropensci/RNeXML/issues/61&quot;&gt;RNeXML/#61&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;pdg-control-policycosts&quot;&gt;pdg-control / policycosts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Started tidying up this repo’s manuscript folder a bit.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Merge Paul’s various changes on tex and Word documents into Rmd&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Whoops. Couldn’r resist a wee rant on &lt;a href=&quot;https://plus.google.com/112929796403983408632/posts/EDTGvMVTTAa&quot;&gt;G+, 2014-03-02&lt;/a&gt; on this #PLOSfail discussion.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/03/11/notes.html"/>
   <updated>2014-03-11T00:00:00+00:00</updated>
   <id>/03/11/notes</id>
   <content type="html">&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NAP report on “Abrupt impacts of climate change: anticipating suprises.”
&lt;ul&gt;
&lt;li&gt;Nice overview of major climate tipping points. Interesting that the 2011 IPCC report put to rest concerns about a sudden transition of the thermohaline circulation within this century, despite good evidence that it is a bistable dynamic. Likewise appears to dismiss concerns of a tipping point related to the release of permafrost methane, thus putting aside the two most well-known and well-studied examples. Meanwhile no shortage of other concerns, from species extinctions and other “climate impacts” to sea-level rise, ice sheet melting, acidification, ocean oxygen depletion and other examples. (See table 4.1)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Box 4.2 covers early warning signals, and a citation to my Nature piece (with my name misspelled) echoes our skepticism of generic warning signals and overall handles these issues rather well. (pg 153)&lt;/li&gt;
&lt;li&gt;Recommends a global Abrupt Change Early Warning System (ACEWS) with a rather nice discussion of some existing monitoring efforts one would build on, and highlighting the need for modeling and synthesis as well.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two very interesting papers by @&lt;a href=&quot;https://github.com/jameshowison&quot;&gt;jameshowison&lt;/a&gt; &lt;a href=&quot;https://github.com/mozillascience/code-research-object/issues/1#issuecomment-37339160&quot;&gt;discuss&lt;/a&gt; issues of practices and incentives in software publication. In particular, &lt;a href=&quot;http://doi.org/10.1145/1958824.1958904&quot;&gt;10.1145/1958824.1958904&lt;/a&gt;, &lt;a href=&quot;http://james.howison.name/pubs/HowisonHerbsleb2011SciSoftIncentives.pdf&quot;&gt;pdf&lt;/a&gt; is an excellent survey of both some common and different challenges in high-energy physics vs. structural biology vs. bioinformatics (would have been great to see a field like ecology included in which the software used is potentially more heterogenous and less central still…) Likewise, his second paper (&lt;a href=&quot;http://james.howison.name/pubs/IncentivesAndIntegration-p459-howison.pdf&quot;&gt;pdf&lt;/a&gt;), which focuses on the evolution and versions of BLAST, includes an interesting discussion with particular messages for funders on how research software incentives differ from the open source software movement, and what possible incentives might ameliorate these challenges.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Piece from Matt Pennell on punctuated equilibrium and reply critiquing it. Sounds like the two agree more than they let on, with confusion in the terminology lying not so much in its creation as in its subsequent use in the literature.
&lt;ul&gt;
&lt;li&gt;Pennell, M. W., Harmon, L. J. &amp;amp; Uyeda, J. C. 2014 Is there room for punctuated equilibrium in macroevolution? Trends Ecol. Evol. 29, 23-32. (doi:10.1016/j.tree.2013.07.004)&lt;/li&gt;
&lt;li&gt;Lieberman, B. S. &amp;amp; Eldredge, N. 2014 What is punctuated equilibrium? What is macroevolution? A response to Pennell et al . Trends Ecol. Evol. , 1-2. (doi:10.1016/j.tree.2014.02.005)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;somewhat less clarity in the debate on generic early warning signals in medical contexts:
&lt;ul&gt;
&lt;li&gt;Bos, E. H. &amp;amp; De Jonge, P. 2014 “Critical slowing down in depression” is a great idea that still needs empirical proof. Proc. Natl. Acad. Sci. U. S. A. 111, 2014. (doi:10.1073/pnas.1323672111)&lt;/li&gt;
&lt;li&gt;Wichers, M., Borsboom, D., Tuerlinckx, F., Kuppens, P., Viechtbauer, W., van de Leemput, I. a., Kendler, K. S. &amp;amp; Scheffer, M. 2014 Reply to Bos and De Jonge: Between-subject data do provide first empirical support for critical slowing down in depression. Proc. Natl. Acad. Sci. 111, E879-E879. (doi:10.1073/pnas.1323835111)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Melissa lab group meeting on statistical estimation from arial survey data on sea otters. Very neat stuff.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Going over ropensci reporting materials&lt;/li&gt;
&lt;li&gt;Going over NSF reporting requirements (see subsequent post).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;meeting-with-steve&quot;&gt;Meeting with Steve&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Go over derivation – had dropped a &lt;span class=&quot;math&quot;&gt;\(\sigma^2\)&lt;/span&gt; term in the denominator so wasn’t getting the expected Gamma-like integral.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consider random-walk prior / functional rather than linear forcing&lt;/li&gt;
&lt;li&gt;Explore implementation via Kalman filtering&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/03/10/notes.html"/>
   <updated>2014-03-10T00:00:00+00:00</updated>
   <id>/03/10/notes</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;RNeXML merge travis integration with support for rrdf. See &lt;a href=&quot;https://github.com/ropensci/RNeXML/pull/57&quot;&gt;pull #57&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reply to Perry re nonparametric approaches&lt;/li&gt;
&lt;li&gt;10am Andrew Skype meeting re: his manuscript&lt;/li&gt;
&lt;li&gt;rOpenSci discussion re: other services, formats.&lt;/li&gt;
&lt;li&gt;Reading &lt;a href=&quot;http://doi.org/10.1002/2014GL059205&quot; title=&quot;Statistical Significance of Climate Sensitivity Predictors Obtained by Data Mining&quot;&gt;Caldwell et al, Geophysical Letters&lt;/a&gt;. Good example of naiive data mining leading to spurious correlations from non-independent models. (/ht @&lt;a href=&quot;http://twitter.com/FreshwaterEcology&quot;&gt;FreshwaterEcology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Looking over NSF’s Data Infrastructure Building Blocks, &lt;a href=&quot;http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=504776&amp;amp;WT.mc_id=USNSF_39&amp;amp;WT.mc_ev=click&quot;&gt;DIBB&lt;/a&gt; call and previously funded work.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remote control for Ubuntu via android? Considering:&lt;/li&gt;
&lt;li&gt;xbmc with xbmc-remote: Looks promising if a bit heavy; connection unsuccessful.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;WiFi Mouse successful connection via autoconnect and thethered wireless, works pretty smothly with limited setup. Does need root running &lt;code&gt;mouseserver&lt;/code&gt;, installed from an open source &lt;code&gt;.deb&lt;/code&gt; file.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;2pm rOpenSci conference call&lt;/li&gt;
&lt;li&gt;Calculations, see below&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;bayesian-early-warning&quot;&gt;Bayesian Early Warning&lt;/h2&gt;
&lt;p&gt;Continuing calculation from &lt;a href=&quot;http://carlboettiger.info/2013/12/10/notes.html&quot;&gt;2013/12/10&lt;/a&gt;, which integrates out &lt;span class=&quot;math&quot;&gt;\(\theta\)&lt;/span&gt;. Next steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrate out &lt;span class=&quot;math&quot;&gt;\(\sigma\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Grid-search &lt;span class=&quot;math&quot;&gt;\(\alpha\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Implement numerically&lt;/li&gt;
&lt;li&gt;Implement approach for time-heterogeneous case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplifying and combing terms from where we left off: the integral of some Gaussian of variance &lt;span class=&quot;math&quot;&gt;\(\nu\)&lt;/span&gt; evaluates to &lt;span class=&quot;math&quot;&gt;\(\sqrt{2\pi \nu}\)&lt;/span&gt; which we solved for previously, giving us:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[f(A_t) \left( 2 \pi V_t \right)^{-(T-1)/2} \left( 2 \pi V_t \right)^{1/2}  \left(B^2(T-1) \right)^{-1/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ = f(A_t) \left( 2 \pi V_t \right)^{-(T-2)/2}   \left(B^2(T-1) \right)^{-1/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting in for &lt;span class=&quot;math&quot;&gt;\(V\)&lt;/span&gt; to have explicitly in terms of sigma, (&lt;strong&gt;note&lt;/strong&gt;: Correcting the &lt;span class=&quot;math&quot;&gt;\(f(A_t)\)&lt;/span&gt; term, which was missing the denominator in the earlier version of &lt;a href=&quot;http://carlboettiger.info/2013/12/10/notes.html&quot;&gt;2013/12/10&lt;/a&gt;, now fixed).&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ = \exp\left(\frac{}{}\right)  \sigma^{-(T-2)} \left( \frac{\pi}{\alpha}\left(1- e^{-\alpha t}\right) \right)^{-(T-2)/2}   \left(B^2(T-1) \right)^{-1/2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and is then is integrable in terms of &lt;span class=&quot;math&quot;&gt;\(\sigma\)&lt;/span&gt; by recognizing this as a Gamma integral,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ =  f(A_t)  \sigma^{-(T-2)} \left( \frac{\pi}{\alpha}\left(1- e^{-\alpha t}\right) \right)^{-(T-2)/2}   \left(B^2(T-1) \right)^{-1/2}\]&lt;/span&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/03/07/notes.html"/>
   <updated>2014-03-07T00:00:00+00:00</updated>
   <id>/03/07/notes</id>
   <content type="html">&lt;h2 id=&quot;thursday&quot;&gt;Thursday&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reading through Andrew’s manuscript&lt;/li&gt;
&lt;li&gt;Working on RNeXML checks / travis continuous integration. Completing documentation, updating unit tests to reflect recent API changes.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Difficulty getting &lt;code&gt;taxize&lt;/code&gt; dependency to pass travis check. Needed these lines in travis:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;sourceCode yaml&quot;&gt;&lt;code class=&quot;sourceCode yaml&quot;&gt;&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; sudo apt-get install gdal-bin libgdal1 libgdal1-dev netcdf-bin libproj-dev 
&lt;span class=&quot;kw&quot;&gt;-&lt;/span&gt; ./travis-tool.sh install_aptget gdal-bin&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Interview with Nature Careers journalist Amanda, discussing open notebook and open science developments&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;friday&quot;&gt;Friday&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Finish up RNeXML checks / travis integration. Yay, passing check now.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Integrate unit test suite into &lt;code&gt;R CMD CHECK&lt;/code&gt; / travis checks. Updated a few unit tests and bugfixes for simmap, everything passing now.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Difficulty getting rrdf on travis, see &lt;a href=&quot;https://github.com/craigcitro/r-travis/issues/104#issuecomment-37088542&quot;&gt;r-travis/issues/104&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Interview with Science journalist Chris on open science and lab notebook technology.&lt;/li&gt;
&lt;li&gt;Interview with Sci American Susan journalist on tipping points, early warning signals.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See NAP report on &lt;a href=&quot;http://www.nap.edu/catalog.php?record_id=18373&quot;&gt;Abrupt Impacts of Climate Change: Anticipating Suprises&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Cran And Package Dependencies</title>
	 <link href="/2014/02/19/CRAN-and-package-dependencies.html"/>
   <updated>2014-02-19T00:00:00+00:00</updated>
   <id>/02/19/CRAN-and-package-dependencies</id>
   <content type="html">&lt;h2 id=&quot;section&quot;&gt;2014-02-19&lt;/h2&gt;
&lt;p&gt;R allows you to specify &lt;code&gt;&amp;gt;=&lt;/code&gt; in a package dependency, not &lt;code&gt;==&lt;/code&gt;. The Comprehensive R Archive Network, CRAN, makes no requirement that package authors do not break their previous function API. For instance, if version 0.1 of the package had a function&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;foo &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x, &lt;span class=&quot;dt&quot;&gt;y =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;){
  ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then one could add new functionality in say, version 0.2 without breaking the existing API with a definition that looked like:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;foo &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x, &lt;span class=&quot;dt&quot;&gt;y =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dt&quot;&gt;z =&lt;/span&gt; &lt;span class=&quot;ot&quot;&gt;NULL&lt;/span&gt;){
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since any earlier function call such as &lt;code&gt;foo(5)&lt;/code&gt; or &lt;code&gt;foo(3,4)&lt;/code&gt; would give the same results. However, changes like this break earlier calls:&lt;/p&gt;
&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code class=&quot;sourceCode r&quot;&gt;foo &amp;lt;-&lt;span class=&quot;st&quot;&gt; &lt;/span&gt;function(x, z, &lt;span class=&quot;dt&quot;&gt;y =&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since the order of the arguments have changed, since &lt;code&gt;z&lt;/code&gt; is not an optional argument, and since the default to &lt;code&gt;y&lt;/code&gt; has changed. Any such changes break the replicability of earlier code. Now this would not be a problem if I could specify a particular version in the dependency, e.g. &lt;code&gt;Depends: foobar == 0.1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Current CRAN guidelines state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A package or ‘R’ can appear more than once in the ‘Depends’ field, for example to give upper and lower bounds on acceptable versions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any use of this will throw a NOTE, not sure if CRAN would object. If the upper bound is not the most recent version on CRAN however, presumably CRAN will not take the package. Check seems to check against what is installed on the local system (which I believe does not support having multiple versions installed at once).&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;section-1&quot;&gt;2014-03-17&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Some much more detailed thinking on this from Jeroem Ooms in this piece on the &lt;a href=&quot;http://arxiv.org/abs/1303.2140&quot;&gt;arxiv&lt;/a&gt;, including the ‘standard’ solution of linux distributions (versioned releases a la debian stable, unstable, testing), or more explicit version management by package, a la NPM.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sometimes interesting follow-up discussion on &lt;a href=&quot;https://stat.ethz.ch/pipermail/r-devel/2014-March/068548.html&quot;&gt;r-devel&lt;/a&gt;. Some acknowledgement of the issues, some good counterpoints, and some confusion.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also see our discussion of these issues on &lt;a href=&quot;https://github.com/ropensci/hackathon/issues/19&quot;&gt;hackathon/issues/19&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;section-2&quot;&gt;2014-03-24&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Interesting approach based on git versioning all CRAN packages to work around the troubles with archive. &lt;a href=&quot;https://github.com/metacran/tools&quot; class=&quot;uri&quot;&gt;https://github.com/metacran/tools&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Real World Challenges In Reproducible Research Code</title>
	 <link href="/2014/02/18/real-world-challenges-in-reproducible-research-code.html"/>
   <updated>2014-02-18T00:00:00+00:00</updated>
   <id>/02/18/real-world-challenges-in-reproducible-research-code</id>
   <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Editorial Note:&lt;/strong&gt; Scratchpad in replying to issues raised by the NESCent reproducibility exercise that included my prosecutor’s fallacy paper. See more developed version of this disucssion in the package issues log:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/pull/1&quot;&gt;#1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/issues/2&quot;&gt;#2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cboettig/prosecutors-fallacy/pull/3&quot;&gt;#3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;hlapp&quot;&gt;@hlapp&lt;/span&gt; Thanks for the bug reports; I think there are quite a few good illustrations of the challenges in sharing code here go beyond &lt;a href=&quot;&quot;&gt;“Publish your code, it’s good enough”&lt;/a&gt;, despite the importance of that message. If you haven’t seen it, you might be interested in joining the thread SWC started on that topic.&lt;/p&gt;
&lt;p&gt;I should probably flush these out into further issues, but (not surprisingly) the problems go deeper than these issues.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the &lt;code&gt;odesolve&lt;/code&gt; package has since been removed from CRAN, as it is obsoleted by the deSolve package (see &lt;a href=&quot;http://cran.r-project.org/web/packages/odesolve/index.html&quot;&gt;page&lt;/a&gt;). Unfortunately, the function API changes slightly in the lsoda function in deSolve, so this fix requires (a little easy) manual intervention.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As I think you figured out, this package / paper grew out of my earlier work on the &lt;a href=&quot;https://github.com/cboettig/earlywarning&quot;&gt;earlywarning&lt;/a&gt; package, and at the beginning was a branch of that package. I’ve fixed the deSolve issue in earlywarning, but forgot to do so here. I made it a separate package once it was clear this line of investigation led to a completely different paper.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;This package should depend on &lt;code&gt;earlywarning&lt;/code&gt; package, not duplicate it.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This creates an added challenge in versioning, since when all the functions are together, it is sufficient to go back to the SHA at the time of publication to get the functions in their original form, while leaving them into a separate package we would have to capture two SHAs. R’s dependency mechanisms don’t handle this as well, particularly when relying on github.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Presumably it would depend on the version of &lt;code&gt;earlywarning&lt;/code&gt; in the master branch, which should ideally maintain a backwards-compatible API and have tests to catch when it doesn’t&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I should be better at tagging the SHA at the time of publication, and possibly relying on branches to distinguish different versions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For instance, I started exploring further directions such as &lt;code&gt;bayeswarning.R&lt;/code&gt; on the &lt;code&gt;prosecutor&lt;/code&gt; branch of the &lt;code&gt;earlywarning&lt;/code&gt; package. The DESCRIPTION file was never recompiled on that branch, hence the install error in #1 . (The install also gives a further warning since &lt;code&gt;devtools&lt;/code&gt; has since changed the argument from &lt;code&gt;branch&lt;/code&gt; to &lt;code&gt;ref&lt;/code&gt;, since it now supports giving an arbitrary SHA instead of just a branch name). The &lt;code&gt;bayeswarning.R&lt;/code&gt; functions should have never been added to that branch (hey, it was just the branch that was currently active and I was exploring quickly), and should now be deleted, as they represent a different exploration. I’m happy to see I actually did create a branch for that. &lt;a href=&quot;https://github.com/cboettig/earlywarning/tree/bayesian/R&quot; class=&quot;uri&quot;&gt;https://github.com/cboettig/earlywarning/tree/bayesian/R&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obviously there is a desire to maintain the original code base used in the paper, as well as to provide downstream updates, but a simple version SHA is not be sufficient. (I could also be better at tagging the SHA at time of publication, etc).&lt;/p&gt;
&lt;p&gt;I’m not sure the best way to reflect that provenance, but it introduces challenges either way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While there are example scripts with example outputs a plenty, there’s no actual unit tests in either &lt;code&gt;earlywarning&lt;/code&gt; or &lt;code&gt;prosectors-fallacy&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I doubt either &lt;code&gt;earlywarning&lt;/code&gt; or &lt;code&gt;prosectors-fallacy&lt;/code&gt; pass &lt;code&gt;R CMD check&lt;/code&gt;, the basic requirement for a CRAN package, at this time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;travis integration could clearly help, though the burden of keeping every bit of software one has ever written up-to-date when dependencies change as fast and as dramatically as they do in R (CRAN makes no requirements not to break your API when updating a package, and asserting a particular version of a package dependency, instead of &amp;gt;=, is not&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/02/14/notes.html"/>
   <updated>2014-02-14T00:00:00+00:00</updated>
   <id>/02/14/notes</id>
   <content type="html">&lt;h2 id=&quot;pomdp&quot;&gt;pomdp&lt;/h2&gt;
&lt;p&gt;Write Iadine, send example code (Reed example in multiple_uncertainty)&lt;/p&gt;
&lt;h2 id=&quot;multiple-uncertainty&quot;&gt;multiple-uncertainty&lt;/h2&gt;
&lt;p&gt;Check in with Jim on how results compare to Sethi. See commit log for &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/commits/96e426c5656b367895137f5ac3bf2ed432f9609c&quot;&gt;multiple_uncertainty&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;open-science&quot;&gt;open-science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h1&gt;PublishPerish14 continues&lt;/h1&gt;&lt;/li&gt;
&lt;li&gt;Comments on software publishing https://github.com/mozillascience/code-research-object/issues/2#issuecomment-35109160&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Drake, J. M. 2014 Tail probabilities of extinction time in a large number of experimental populations. Ecology , 140206083444001. (doi:10.1890/13-1107.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neat result. thinking how this relates to predictions of Mangel and Tier 1993; probably still a significant deviation?&lt;/p&gt;
&lt;p&gt;Other papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ho and Ane paper out now: Ho, L. &amp;amp; Ané, C. 2014 A linear-time algorithm for Gaussian and non-Gaussian trait evolution models. Syst. Biol. , 1-38.&lt;/li&gt;
&lt;li&gt;More on climate change vectors: Burrows, M. T. et al. 2014 Geographical limits to species-range shifts are suggested by climate velocity. Nature (doi:10.1038/nature12976)&lt;/li&gt;
&lt;li&gt;Essington, T. &amp;amp; Munch, S. 2014 Trade-offs between supportive and provisioning ecosystem services of forage species in marine food webs. Ecol. Appl. (doi:10.1890/13-1403.1)&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Scratch Notes While Outlining Talk</title>
	 <link href="/2014/02/10/scratch-notes-while-outlining-talk.html"/>
   <updated>2014-02-10T00:00:00+00:00</updated>
   <id>/02/10/scratch-notes-while-outlining-talk</id>
   <content type="html">&lt;p&gt;Scratch notes for my seminar&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Part I: Tipping Points: Early Warning signals and decision theory


Global change threats. So what do we do?

This is a pendulum. easy to predict.
Not a pendulum. Oscillations not easy to predict. Crashes

Tipping points



No equation for $f_i(s,h)$

Everything matters.

Data is good, getting better.

Crushing our data under parameters.  Stop throwing out the data.


Early Warning signals


Optimal Control problem -- Model the descision process! Action space!

Model choice sucks.

Nonparametrics work.


Why an informatics approach?

What is Ecoinformatics?


 Theory      |     Data
-------------|-------------
 Algorithms  |  Informatics




50 years ago Ecology Golden (Folk) Age / Peter, Paul &amp;amp; Mary: theory had little need of good algorithms
Today the models, data, and statistics are more numerous and more complex. An algorithmic and computational component is now essential

Today&amp;#39;s data needs similar infrastructure

Bioinformatics, ATGC, vertically integrated.

No one need be told we are awash in data today. Yet it is not the data we have but the questions we face which demand this approach.

Let us recall how utterly desperate the warning signal idea is: an extrapolation, not based on averages but on the noise itself, and under the very assumption that everything is about to change suddenly and extrapolation should be impossible.

That we seek to manage a fishery not knowing the dynamics not only for one species, but for the whole ecosystem that feeds and eats it, a variable and changing climate, and socio-economic winds every bit as daunting. Across scales of space, time and ecology.


(sequentially add...)
Fishbase.  Treebase.  GBIF

rOpenSci Story

- Shared challenge ~&amp;gt; collaboration
- collaboration ~&amp;gt; network, developer collective rOpenSci
- Building bridges, teaching tools. Rapid growth. Sloan


- next challenges: from vertically integrated to metadata-driven. Better leverage your own data.
(out of scope? or some data management needed?  differentiate from future work proposal)


(I think we can safely define vertically integrated and metadata driven database without &amp;quot;giving away&amp;quot; all future research proposals...)

Risk in doing this: Prosecutor&amp;#39;s fallacy.

A mandate to try.  Just as policy must be made on the best available science, science must be made on the best available data.





Please don&amp;#39;t show this, don&amp;#39;t think this:

   Math     |     Ecology
------------|------------------
 Statistics | Computer Science


What does it do?

Access / Discovery. (Data Mining?)
Manipulation
Management

Modular: Reusable, Interoperable components

archiving. metadata.


-----------------------------------------------------------------------

Who has big data already? Who needs you?
The pitch to those not using external data
The pitch to experimental scientist

---------------------------------------------


- Fix figures.  Myers -&amp;gt; Maynard-Smith


------------------------------------------------------------------

What is Ecoinformatics?
=======================


An analogy to computation
-------------------------

Ecoinformatics:data :: computation : models &amp;amp; theory.

We don&amp;#39;t need a computer to do ecology.  But it sure helps.

Statistics.  Visualization.


On the transition to ecoinformatics: another analogy
----------------------------------------------------

Having a digital representation of our data, rather than a handwritten
one in a paper notebook, has forever changed how we do science. That is
the promise ecoinformatics.

Digital data could be backed up, shared, and manipulated with ease.
Most importantly, it provided a relatively seemless integration into
all kinds of tools, while minimizing the potential to introduce human
error. Despite the improvement, archiving, sharing, and manipulating
digital data isn&amp;#39;t always effortless.  Adapting, formatting and
manipulating data for the analyses is anything but easy or foolproof.
Ecoinformatics seeks to address each of these challenges: archiving,
sharing, manipulation, inter-operability, automation, and the reduction
of human error.

How do we do that? What does it look like?


Platform.

We&amp;#39;re familiar with platform for statistics. We&amp;#39;re familiar with platform for data in other contexts: Google Maps. Search. Airline flights.

Yeah, but what does it look like for ecology?

- Vertically integrated databases.  (literally - same column)
- Metadata-driven databases.&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Nceas Workflows Day3</title>
	 <link href="/2014/02/07/nceas-workflows-day3.html"/>
   <updated>2014-02-07T00:00:00+00:00</updated>
   <id>/02/07/nceas-workflows-day3</id>
   <content type="html">&lt;h2 id=&quot;day-2-work&quot;&gt;(Day 2 work)&lt;/h2&gt;
&lt;p&gt;Hack-a-thon; no notebook entry but all progress appears in group Github repo: &lt;a href=&quot;https://github.com/NCEAS/commdyn/&quot;&gt;github.com/NCEAS/commdyn/&lt;/a&gt;. 78 Commits in 2 days, nice work team.&lt;/p&gt;
&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;of challenges and opportunities in open data and ecological workflows&lt;/p&gt;
&lt;p&gt;Synthesis and Reporting back&lt;/p&gt;
&lt;h2 id=&quot;group-1.5-plan&quot;&gt;Group 1.5 Plan&lt;/h2&gt;
&lt;h3 id=&quot;product-1&quot;&gt;Product 1:&lt;/h3&gt;
&lt;p&gt;Paper: Visualizing global change in long term community ecology studies: the strawmen?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lead: Sydney Jones&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Long-term ecological research (LTER) sites have been collecting and publishing raw community ecology data for over 30 years from across the US and across a wide array of taxa.&lt;/p&gt;
&lt;h3 id=&quot;product-2&quot;&gt;Product 2:&lt;/h3&gt;
&lt;p&gt;Paper: How global trends in community diversity vary by metric&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leads: Ben&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;product-3&quot;&gt;Product 3:&lt;/h3&gt;
&lt;p&gt;Data publications: Each processed dataset will be republished to the KNB along with the code required to generate that data from the original raw data, and with appropriate metadata describing both the new data table and the code itself.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Private datasets will be privately/securely published&lt;/li&gt;
&lt;li&gt;Processed data will be appropriately linked back to the KNB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Collaborative git environment share our work. Still impenetrable to an outsider. Why?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Documentation of what we have done&lt;/li&gt;
&lt;li&gt;Modularity and interoperability&lt;/li&gt;
&lt;li&gt;Abstraction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Incentives to do this better&lt;/p&gt;
&lt;p&gt;Publishing of cleaned data and workflow&lt;/p&gt;
&lt;p&gt;Impressed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pair-programming&lt;/li&gt;
&lt;li&gt;divide-and-conquer&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Workflows Working Group Day 1</title>
	 <link href="/2014/02/05/workflows-working-group-day-1.html"/>
   <updated>2014-02-05T00:00:00+00:00</updated>
   <id>/02/05/workflows-working-group-day-1</id>
   <content type="html">&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Develop ideas for larger synthesis across systems&lt;/li&gt;
&lt;li&gt;Introduce scientists to the concept of workflow systems&lt;/li&gt;
&lt;li&gt;Introduce scientists to open data and methods sharing&lt;/li&gt;
&lt;li&gt;Gather requirements for collaboration tools that would allow scientists to use them and share data and workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;participants&quot;&gt;Participants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ilkay Altintas, UC San Diego,&lt;/li&gt;
&lt;li&gt;Carl Boettiger, UC Davis,&lt;/li&gt;
&lt;li&gt;Erica Christensen, Utah State,&lt;/li&gt;
&lt;li&gt;Elsa Cleland, UCSD,&lt;/li&gt;
&lt;li&gt;Scott Collins, U New Mexico,&lt;/li&gt;
&lt;li&gt;Sam Fey, Dartmouth,&lt;/li&gt;
&lt;li&gt;Corinna Gries, U Wisconsin,&lt;/li&gt;
&lt;li&gt;Lauren Hallett, Berkeley,&lt;/li&gt;
&lt;li&gt;Stan Harpole, Iowa State,&lt;/li&gt;
&lt;li&gt;Dave Harris, UC Davis,&lt;/li&gt;
&lt;li&gt;Matt Jones, UC Santa Barbara,&lt;/li&gt;
&lt;li&gt;Sydney Jones, U New Mexico,&lt;/li&gt;
&lt;li&gt;Julie Ripplinger, Arizona State University,&lt;/li&gt;
&lt;li&gt;Andrew Rypel, WDNR,&lt;/li&gt;
&lt;li&gt;David Seekell, UVa,&lt;/li&gt;
&lt;li&gt;Elizabeth Siddon, U. Alaska Fairbanks,&lt;/li&gt;
&lt;li&gt;Eric Sokol, Virginia Tech,&lt;/li&gt;
&lt;li&gt;Lizzie Wolkovitch, UCSD/Harvard,&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;agenda&quot;&gt;Agenda&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday February 5, 2014&lt;/li&gt;
&lt;li&gt;8:30 am - 9:00 am Welcome and introductions Corinna&lt;/li&gt;
&lt;li&gt;9:00 am - 9:20 am Project overview Corinna, Scott, Matt&lt;/li&gt;
&lt;li&gt;9:20 am - 10:00 am Intro to community metrics Scott&lt;/li&gt;
&lt;li&gt;10:00 am - 10:30 am Coffee Break&lt;/li&gt;
&lt;li&gt;10:30 am - 11:00 am Automating community metrics workflows - data and workflow repositories Matt&lt;/li&gt;
&lt;li&gt;11:00 am - 12:00 pm Hands-on Kepler exercise ‘making workflows portable’ Matt&lt;/li&gt;
&lt;li&gt;12:00 pm - 1:00 pm Lunch on your own&lt;/li&gt;
&lt;li&gt;1:00 pm - 2:00 pm Round-the-room brainstorm community metrics Scott&lt;/li&gt;
&lt;li&gt;2:00 pm - 2:30 pm Prioritize community metrics Scott&lt;/li&gt;
&lt;li&gt;2:30 pm - 3:00 pm form working groups plenary&lt;/li&gt;
&lt;li&gt;3:00 pm - 3:30 pm Afternoon Break&lt;/li&gt;
&lt;li&gt;3:30 pm - 5:30 pm Community metrics implementation hackathons (R) working groups&lt;/li&gt;
&lt;li&gt;Thursday February 6, 2014&lt;/li&gt;
&lt;li&gt;8:30 am - 10:00 am Community metrics implementation hackathons (R) working groups&lt;/li&gt;
&lt;li&gt;10:00 am - 10:30 am Coffee Break&lt;/li&gt;
&lt;li&gt;10:30 am - 12:00 pm Community metrics implementation hackathons (Kepler) working groups&lt;/li&gt;
&lt;li&gt;12:00 pm - 1:00 pm Lunch on your own&lt;/li&gt;
&lt;li&gt;1:00 pm - 2:00 pm Discussion hackathon outcomes plenary&lt;/li&gt;
&lt;li&gt;2:00 pm - 3:00 pm Executing and sharing community metrics workflows working groups&lt;/li&gt;
&lt;li&gt;3:00 pm - 3:30 pm Afternoon Break&lt;/li&gt;
&lt;li&gt;3:30 pm - 4:30 pm Desiderata for sharing community metrics working groups&lt;/li&gt;
&lt;li&gt;4:30 pm - 5:30 pm Next steps - requirements for sharing workflows plenary&lt;/li&gt;
&lt;li&gt;Friday February 7, 2014&lt;/li&gt;
&lt;li&gt;8:30 am - 10:00 am Next steps - prioritized list of datasets, analyses, data operations, visualizations plenary&lt;/li&gt;
&lt;li&gt;10:00 am - 10:30 am Coffee Break&lt;/li&gt;
&lt;li&gt;10:30 am - 12:00 pm Next steps - research questions to be answered with diverse data and identified community metrics plenary&lt;/li&gt;
&lt;li&gt;12:00 pm Adjourn&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;See most notes on the NCEAS etherpads&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Weekly Review</title>
	 <link href="/2014/01/31/weekly-review.html"/>
   <updated>2014-01-31T00:00:00+00:00</updated>
   <id>/01/31/weekly-review</id>
   <content type="html">&lt;h1 id=&quot;eml&quot;&gt;EML&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;bug fixes and replies to various input from community on EML package&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thoughts on understanding and using EML in research, including Dryad. See https://github.com/ropensci/EML/issues/82#issuecomment-33558301&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;multiple-uncertainty&quot;&gt;multiple-uncertainty&lt;/h1&gt;
&lt;h4 id=&quot;jim-and-mike-meeting&quot;&gt;Jim and Mike meeting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Why the steps in the optimal solution of the deterministic case?&lt;/li&gt;
&lt;li&gt;Irregularities in value function in first iteration: sometimes not exact ties; actually preferring to fish at almost 0 rather than 0.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;address range constraints on &lt;code&gt;h&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;next-steps&quot;&gt;Next steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Puzzle out irregular first iteration. Role of state equation,&lt;/li&gt;
&lt;li&gt;Interpolation-based inference of optimum, rather than grid&lt;/li&gt;
&lt;li&gt;When can we optimize over the &lt;span class=&quot;math&quot;&gt;\(X_{t+1}\)&lt;/span&gt; that maximizes the value and just back out the corresponding harvest?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the stochastic case, multiple harvests could all realize the &lt;span class=&quot;math&quot;&gt;\(X_{t+1}\)&lt;/span&gt; that maximizes the value, though each with different probability. I do not believe that it holds in general that the h for which it is most likely to get the optimal &lt;span class=&quot;math&quot;&gt;\(X_{t+1}\)&lt;/span&gt; is necessarily the h which maximizes the value. Would you agree? (recall we want to maximize the expectation, so we need to integrate over all outcomes of the harvest, not consider only the most likely outcome). I see no alternative to solving for h directly then?&lt;/p&gt;
&lt;h1 id=&quot;nceas-workshop-prep&quot;&gt;NCEAS workshop prep&lt;/h1&gt;
&lt;p&gt;The agenda for the workshop, participant list, goals and intended outcomes can be found at https://projects.ecoinformatics.org/ecoinfo/projects/commdyn/wiki/First_Community_Workshop and is attached. In preparation for the workshop we would like for you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If at all possible, identify and bring a dataset that includes community data for at least ten points in time at somewhat even intervals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LTER searches; Bestelmeyer maybe&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bring ideas for metrics that are meaningful measures of community change over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Early warning signals? Climate change vectors?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop ecological research questions that may be answered with the metrics of community change that will be developed/used during the workshop. Ideally, these synthesis questions span several ecological systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is this list in reverse sequence? e.g. should develop questions, then metrics, then identify datasets?&lt;/p&gt;
&lt;p&gt;Desired Outcomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Repository design requirements for sharing workflows&lt;/li&gt;
&lt;li&gt;list of community metrics&lt;/li&gt;
&lt;li&gt;list of data sets&lt;/li&gt;
&lt;li&gt;list of data transformation operations to get from raw data to analyses&lt;/li&gt;
&lt;li&gt;list of visualizations&lt;/li&gt;
&lt;li&gt;research collaborations: Ideas for ecological questions that can be answered with these data and metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;reading&quot;&gt;Reading&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Nature&lt;/em&gt; on troubled times for the monitoring buoys used by the TAO early warning system for El Nino events &lt;a href=&quot;http://doi.org/10.1038/505585a&quot;&gt;10.1038/505585a&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nice pieces on reproducibility in Nature &lt;a href=&quot;http://doi.org/10.1038/505612a&quot;&gt;10.1038/505612a&lt;/a&gt; and Science &lt;a href=&quot;http://doi.org/10.1126/science.1250475&quot;&gt;10.1126/science.1250475&lt;/a&gt; recently.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/01/31/notes.html"/>
   <updated>2014-01-31T00:00:00+00:00</updated>
   <id>/01/31/notes</id>
   <content type="html">&lt;p&gt;Scratch notes on uncertainty.&lt;/p&gt;
&lt;p&gt;Function definition: &lt;a href=&quot;https://github.com/cboettig/multiple_uncertainty/blob/c6e464cb35cbdd9b852c8d63f8d35446152c1be1/inst/matlab/multiple_uncertainty.m&quot;&gt;multiple_uncertainty.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Settings:&lt;/p&gt;
&lt;pre class=&quot;sourceCode matlab&quot;&gt;&lt;code class=&quot;sourceCode matlab&quot;&gt;f=@(x, h) max( (x-h) * (&lt;span class=&quot;fl&quot;&gt;1&lt;/span&gt; - (x-h) ./ &lt;span class=&quot;fl&quot;&gt;100&lt;/span&gt;) + (x-h), &lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;);
x_grid = [&lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;fl&quot;&gt;5&lt;/span&gt;:&lt;span class=&quot;fl&quot;&gt;100&lt;/span&gt;];
h_grid = x_grid; &lt;span class=&quot;co&quot;&gt;% Must be same dimensions as x_grid, or L91 errors...  &lt;/span&gt;
Tmax = &lt;span class=&quot;fl&quot;&gt;5&lt;/span&gt;;
sigma_g = &lt;span class=&quot;fl&quot;&gt;0.1&lt;/span&gt;;
sigma_m = &lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;.;
sigma_i = &lt;span class=&quot;fl&quot;&gt;0.0&lt;/span&gt;;
delta = &lt;span class=&quot;fl&quot;&gt;0.05&lt;/span&gt;;
pdf = @(p,mu,s) lognpdf(p ./ mu, &lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;, s);
&lt;span class=&quot;co&quot;&gt;%pdf = @(p,mu,s) unifpdf(p, mu .* (1 - s), mu .* (1 + s)); &lt;/span&gt;

[D, V, M, I, P, Ep, F, f_matrix] =  multiple_uncertainty(f, x_grid, h_grid, Tmax, sigma_g, sigma_m, sigma_i, delta, pdf);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Log normal noise gives the anticipated Reed solution:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm8.staticflickr.com/7325/12242548754_7a4b40376b.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Same settings but with uniform noise has a step:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm8.staticflickr.com/7292/12242606504_5815e2d618.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;This step goes away under any one of the following tweaks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use log-normal noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Avoids the finite support problem that makes uniform noise look deterministic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use delta = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Results in exact ties, but future isn’t discounted, so we choose the smallest h (default behavior of &lt;code&gt;min()&lt;/code&gt; function for ties).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Increase the noise size: &lt;code&gt;sigma_g = 0.5&lt;/code&gt; doesn’t remove the step. Smaller, non-zero noise &lt;code&gt;sigma_g = 0.01&lt;/code&gt;, or &lt;code&gt;sigma_g = 0.05&lt;/code&gt; results in solutions with no escapement (under uniform noise; not a problem for log-normal noise).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use a finer mesh, which breaks this 1 step into lots of little steps.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deterministic system (obviously independent of noise structure, since pdf isn’t used) has losts steps when delta &lt;span class=&quot;math&quot;&gt;\(&amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm3.staticflickr.com/2814/12242822336_672932882e.jpg&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;This is resolved by using the interpolation method to find the values off-grid.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Startup Culture And Platforms For The Academy</title>
	 <link href="/2014/01/23/Startup-Culture-and-Platforms-for-the-Academy.html"/>
   <updated>2014-01-23T00:00:00+00:00</updated>
   <id>/01/23/Startup-Culture-and-Platforms-for-the-Academy</id>
   <content type="html">&lt;h2 id=&quot;scientific-research-moving-beyond-bubbles-to-platforms.&quot;&gt;Scientific Research: Moving beyond bubbles to platforms.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.economist.com/news/special-report/21593580-cheap-and-ubiquitous-building-blocks-digital-products-and-services-have-caused&quot;&gt;The Economist&lt;/a&gt; contrasts the current start-up culture to that of the dotcom bubble. Their special report argues that while the dotcom bubble was characterized by heavy investment in a single idea with a “Build it and they will come” attitude, the current generation of startups is characterized by tinkering and rapid experimentation. They argue that this is possible thanks to the cheaply available &lt;em&gt;“platforms”&lt;/em&gt; of open source software, cloud computing, APIs, and social media dissemination. And they argue that this time, it is no bubble. That it will eat the world. That these lessons apply far beyond tech startups, to businesses in general and even governments. The special report is well worth reading in full.&lt;/p&gt;
&lt;h2 id=&quot;so-how-about-science&quot;&gt;So how about science?&lt;/h2&gt;
&lt;p&gt;After all, experimentation is what science is all about. Yet the platforms that run that world are largely absent from scientific research. Research looks more like the dotcom model, where each research team must make an immense investment in infrastructure and data gathering up front. Must hire expertise in-house for each step of the process, from experimental design to data collection to analysis to writing. Such large teams resemble more the companies of previous decades with their own development, marketing, and research departments, then the agile start-ups currently building the future. The resulting products are monolithic, “build it and they will come”.&lt;/p&gt;
&lt;p&gt;I believe that this is the cornerstone of the open science and reproducible research movement.&lt;/p&gt;
&lt;h2 id=&quot;update-2014-07-24&quot;&gt;Update (2014-07-24)&lt;/h2&gt;
&lt;p&gt;This seems to be exactly what Martin Fenner is talking about in “&lt;a href=&quot;http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/&quot;&gt;Roads, not Stagecoaches&lt;/a&gt;” – the value of transformative infrastructure over individual software contributions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Asn Conference</title>
	 <link href="/2014/01/15/ASN-Conference.html"/>
   <updated>2014-01-15T00:00:00+00:00</updated>
   <id>/01/15/ASN-Conference</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; posting this considerably after the fact as I did not have a chance to make a proper entry during the conference.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://pbs.twimg.com/media/BVCAYS7CQAA198y.jpg&quot; alt=&quot;ASN’s wordle of conference abstracts&quot; /&gt;&lt;figcaption&gt;ASN’s wordle of conference abstracts&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://t.co/P3pJ66g5jA&quot;&gt;Full program&lt;/a&gt;, abstracts, etc.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;President’s debate: are ecological communities saturated in species diversity? Watch the whole debate on &lt;strong&gt;&lt;a href=&quot;http://t.co/8NOGWwQaRD&quot;&gt;youtube&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.com/search?q=%23ASN2014&amp;amp;src=typd&amp;amp;f=realtime&quot;&gt;tweet log, maybe?&lt;/a&gt;. Or from the &lt;a href=&quot;http://comments.amnat.org/2014/02/tweets-of-asilomar-asn2014.html&quot;&gt;editor’s desk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;https://pbs.twimg.com/media/Bd-97FbCIAAYZC4.jpg&quot; alt=&quot;In beautiful Asilomar, image credit @svincenzi&quot; /&gt;&lt;figcaption&gt;In beautiful Asilomar, image credit &lt;span class=&quot;citation&quot; data-cites=&quot;svincenzi&quot;&gt;@svincenzi&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Some highlights (such as on the top of my head 2 months later anyway, certainly others I’ve forgotten to mention. too many good talks in a short window).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;President’s debate.&lt;/li&gt;
&lt;li&gt;Sebastian Schreiber’s talk on resource allocation in stochastic environments&lt;/li&gt;
&lt;li&gt;Lizzie Wolkovich’s talk on nonstationary climate and disparate impacts on phenology&lt;/li&gt;
&lt;li&gt;Robin Snyder’s talk with beautiful reaction-diffusion approximation in ecology and evolutionary dynamics of marine mass spawners with correlated fates.&lt;/li&gt;
&lt;li&gt;Luke Mahler’s talk on adaptive radiations&lt;/li&gt;
&lt;li&gt;Dick Golmuckwicz’s talk, but also his stories over dinner; particularly about proving his own critque of a claim in Caswell’s book wrong. Also some of his earlier work on functional / non-parametric approaches.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Time with the Mangel lab in Asilomar, of course.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;http://farm4.staticflickr.com/3790/13109212075_340afd4a56_b.jpg&quot; alt=&quot;some of my notes from the president’s debate, apologies for screenshot.&quot; /&gt;&lt;figcaption&gt;some of my notes from the president’s debate, apologies for screenshot.&lt;/figcaption&gt;
&lt;/figure&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/01/09/notes.html"/>
   <updated>2014-01-09T00:00:00+00:00</updated>
   <id>/01/09/notes</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;fishbase API discussions continue&lt;/li&gt;
&lt;li&gt;&lt;p&gt;some discussion / work on rfigshare continues (As Karthik writes comprehensive test suite, yay).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Possible ontological term for simulated data? &lt;a href=&quot;http://www.ontobee.org/browser/rdf.php?o=ERO&amp;amp;iri=http://purl.obolibrary.org/obo/ERO_0100304&quot;&gt;modeling and simulation operation&lt;/a&gt; looking for example using this term now…&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://t.co/e6eiEzRW7M&quot;&gt;Request for term on OBI&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Moore &lt;a href=&quot;http://t.co/MOKfUAZvGo&quot;&gt;DDD investigators&lt;/a&gt;, &lt;a href=&quot;http://www.moore.org/docs/default-source/Grantee-Resources/ddd-investigator-competition-solicitation-for-applications.pdf?sfvrsn=0&quot;&gt;full call&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;.&lt;span class=&quot;citation&quot; data-cites=&quot;NOAA&quot;&gt;@NOAA&lt;/span&gt; gives a glimpse of what the future of scipub could be w/ web native format for this peer-reviewed report: http://t.co/NCireU39DY&lt;/p&gt;
&lt;h2 id=&quot;asn-talk&quot;&gt;ASN talk&lt;/h2&gt;
&lt;p&gt;Working on ASN talk (primarily introduction), again.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;grand challenges of global change, consequences for ecosystems and the services they provide, economies, and biodiversity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To manage these things, we need some understanding of how they behave, and how they respond to our management options. We need a model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When data is unavailable, a mechanistic model is the best thing. When mechanisms are unavailable, the data is the best thing…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Classical modeling approaches alone won’t work. (But they should still help guide what we do!)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This amount of complexity, this amount of data, would require too many parameters. we never estimate all of them. uncertainty is too large.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stop trying to shove all the data into a handful of parameters. or an army of parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;management needs non-parametric approaches for decision making.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--

How to explain cyberinfrastructure?  


Cyberinfrastructure is a hard sell, because like any infrastructure: when it works we take it for granted unless it breaks.  Before it exists we don&#39;t realize we need it; or recognize that it is the implementation and not the idea that requires brilliance.   

The other problem is nerd jargon. The problem with nerd jargon is that as a nerd, you get so incredibly excited by these words that you can&#39;t help yourself.  To us each one is this shiny new box of awesomeness.  To appreciate this, visit an Apple store on the launch of their latest product.  In a grey or black rectangle, it&#39;s just a lump of melted sand. To the buyers, it is the pinnacle of thought, design and engineering. --&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/01/07/notes.html"/>
   <updated>2014-01-07T00:00:00+00:00</updated>
   <id>/01/07/notes</id>
   <content type="html">&lt;h2 id=&quot;coding&quot;&gt;Coding&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fishbase API discussions&lt;/li&gt;
&lt;li&gt;rfishbase updates pushed to CRAN&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merges and minor bug fixes for rfigshare&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RNeXML: After some cooperative sleuth-work, we successfully resolved issue &lt;a href=&quot;https://github.com/ropensci/RNeXML/issues/23&quot;&gt;#23&lt;/a&gt;, uncovering a bug (missing feature really) in xmllint.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;as usual, detailed activity on github.&lt;/p&gt;
&lt;h2 id=&quot;reading&quot;&gt;Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The sudden collapse of pollinator communities - Jelle Lever - 2014 - Ecology Letters - Wiley Online Library http://goo.gl/ZKuKsj&lt;/li&gt;
&lt;li&gt;Github has &lt;a href=&quot;https://t.co/Wpf27aqIYk&quot;&gt;traffic stats&lt;/a&gt; again.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hmm, interesting reading mentioned on the SWC thread:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pgbovine.net/two-cultures-of-computing.htm&quot;&gt;Philip Gao on “Two Cultures of Computing”&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yet I think the answer is simple. The user culture (GUI culture) is about what computers can do. The programmer-culture (unix culture) is about what they can’t do – yet. Unix is like a box of legos – you can reassemble the blocks into just about anything you can imagine. The students are pointing to pre-built, permanently glued together finished products. Yeah, they look more like what is on the cover of the lego box than the pile of bricks inside. But that’s not why kids buy Lego.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes</title>
	 <link href="/2014/01/06/notes.html"/>
   <updated>2014-01-06T00:00:00+00:00</updated>
   <id>/01/06/notes</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;fishbase API discussions&lt;/li&gt;
&lt;li&gt;knitcitations (push previous updates to CRAN)&lt;/li&gt;
&lt;li&gt;data extraction from sardine collapse story (see EML in data/), &lt;a href=&quot;http://t.co/CxATffouSZ&quot;&gt;news article&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://t.co/ajcAILczcp&quot;&gt;My thoughts&lt;/a&gt; on SWC teaching web-native publication workflow&lt;/li&gt;
&lt;li&gt;reading, ToCs&lt;/li&gt;
&lt;li&gt;speaker invitation replies&lt;/li&gt;
&lt;li&gt;review request replies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;reading&quot;&gt;Reading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Are rapid transitions btwn invasive and native species caused by alt stable states, and does it matter?” http://feedly.com/k/1gbEIWT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Phylogenetic trait-based analyses of ecological networks.” http://feedly.com/k/1gbEvD6&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nice editorial on “core elements of a TPB paper.” Would be interested to see other eds outline what best… http://goo.gl/NVxWlr&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discussing replication with Dan in light of data from recent paper, http://t.co/S1bipN8fHe See Github discussion.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Open Science Literature Highlights</title>
	 <link href="/2014/01/02/open-science-literature-highlights.html"/>
   <updated>2014-01-02T00:00:00+00:00</updated>
   <id>/01/02/open-science-literature-highlights</id>
   <content type="html">&lt;p&gt;A student recently asked me for some recommendations for an article on an open science topic for a journal club. Since I haven’t jotted these down in one place before, though I might copy my reply here for my future reference, or in case anyone else is interested in my list.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;There’s a variety of sub-topics one might focus on; though in my opinion there’s no good, consise overview of all of the topics in one place.&lt;/p&gt;
&lt;h3 id=&quot;open-data&quot;&gt;open data:&lt;/h3&gt;
&lt;p&gt;Probably my first pick for a general discussion. The perspective in Science from 2011 is probably one of my favorites, but as the title suggests is more balanced than controversial. Nice also that it focuses on Ecology in particular and touches on some of the major initiatives in our field:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reichman, O. J., Jones, M. B. &amp;amp; Schildhauer, M. P. 2011 Challenges and Opportunities of Open Data in Ecology. Science 331, 692-693. (doi:&lt;a href=&quot;http://doi.org/10.1126/science.1197962&quot;&gt;10.1126/science.1197962&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also from the ecology perspective but with a slightly more provocative stance is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hampton, S. E., Tewksbury, J. J. &amp;amp; Strasser, C. 2012 Ecological data in the Information Age. Front. Ecol. Environ. 10, 59-59. (doi:&lt;a href=&quot;http://doi.org/10.1890/1540-9295-10.2.59&quot;&gt;10.1890/1540-9295-10.2.59&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This set of interviews from Nature’s Careers column this summer might make a more interesting context for discussion (e.g. practical concerns rather than the idealistic):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Van Noorden, R. 2013 Data-sharing: Everything on display. Nature 500, 243-245. (doi:&lt;a href=&quot;http://doi.org/10.1038/nj7461-243a&quot;&gt;10.1038/nj7461-243a&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;open-code&quot;&gt;open code:&lt;/h3&gt;
&lt;p&gt;In my opinion this is the upcoming big issue. Of course open source has a long history in academia, but publishers and funders have paid a lot more attention to open data and open access (Creation of repositories like Dryad; required data deposition by many leading journals; funder’s support of data synthesis initiatives, etc). Interest in the potential for open code and concerns about reliability of increasingly software-driven results seem to be growing, this article does a good job providing concrete examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ince, D. C., Hatton, L. &amp;amp; Graham-Cumming, J. 2012 The case for open computer programs. Nature 482, 485-488. (doi:&lt;a href=&quot;http://doi.org/10.1038/nature10836&quot;&gt;10.1038/nature10836&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;a more recent piece in Science focuses on Ecology and some of the social challenges involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Joppa, L. N., McInerny, G., Harper, R., Salido, L., Takeda, K., O’Hara, K., Gavaghan, D. &amp;amp; Emmott, S. 2013 Troubling Trends in Scientific Software Use. Science 340, 814-815. (doi:[10.1126/science.1231535])(http://doi.org/10.1126/science.1231535))&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;reproducible-research&quot;&gt;reproducible research:&lt;/h3&gt;
&lt;h4 id=&quot;the-computational-side&quot;&gt;The computational side&lt;/h4&gt;
&lt;p&gt;The computational side is more straight forward; this piece in Science discusses how the Biostatistics Journal optionally checks for code reproducibility and rewards it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Peng, R. D. 2011 Reproducible Research in Computational Science. Science 334, 1226-1227. (doi:&lt;a href=&quot;http://doi.org/10.1126/science.1213847&quot;&gt;10.1126/science.1213847&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;experimental-side&quot;&gt;experimental side&lt;/h4&gt;
&lt;p&gt;On the experimental side there’s been a lot of attention too, mostly on the medical issues. This piece actually focuses on risks and challenges in experimental replication:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bissell, M. 2013 Reproducibility: The risks of the replication drive. Nature 503, 333-334. (doi:&lt;a href=&quot;http://doi.org/10.1038/503333a&quot;&gt;10.1038/503333a&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;altmetrics&quot;&gt;Altmetrics:&lt;/h3&gt;
&lt;p&gt;Since open science usually emphasizes valuing things not ‘traditionally’ valued (code, data, open/reproducible papers), measuring academic impact often becomes part of the discussion. This comment piece from Heather gives an excellent overview:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Piwowar, H. A. 2013 Altmetrics: Value all research products. Nature 493, 159-159. (doi:&lt;a href=&quot;http://doi.org/10.1038/493159a&quot;&gt;10.1038/493159a&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;open-access&quot;&gt;open access:&lt;/h3&gt;
&lt;p&gt;Perhaps the original “open science”, not sure there is much interesting to discuss here, more of an economic issue at this point than a scientific one (in my opinion anyhow). A few other interesting things being done by publishers, e.g. as mentioned in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pincock, S. 2013 Publishing: Open to possibilities. Nature 495, 539-541. (doi:&lt;a href=&quot;http://doi.org/10.1038/nj7442-539a&quot;&gt;10.1038/nj7442-539a&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;open-peer-review&quot;&gt;open peer review:&lt;/h3&gt;
&lt;p&gt;Some journals have been doing this for some time; for instance this piece reflects on the experience of one of the Nature journals that has been doing it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pulverer, B. 2010 Transparency showcases strength of peer review. Nature 468, 29-31. (doi:&lt;a href=&quot;http://doi.org/10.1038/468029a&quot;&gt;10.1038/468029a&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;open-citations&quot;&gt;open citations:&lt;/h3&gt;
&lt;p&gt;A topic that doesn’t get as much attention, but with some interesting ideas, e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shotton, D. 2013 Publishing: Open citations. Nature 502, 295-297. (doi:&lt;a href=&quot;http://doi.org/10.1038/502295a&quot;&gt;10.1038/502295a&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 
